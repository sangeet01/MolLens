{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69c2a40",
   "metadata": {},
   "source": [
    "# MS-to-Structure Deep Learning Pipeline \n",
    "\n",
    "Steps: Step-by-Step Pipeline Overview\n",
    "1. Setup & Environment (Cells 1-3)\n",
    "Installs required packages (PyTorch, RDKit, XGBoost, FAISS, etc.)\n",
    "Configures GPU optimization for RTX 3080 Ti\n",
    "Sets random seeds for reproducibility\n",
    "Defines special tokens (PAD, SOS, EOS, MASK)\n",
    "\n",
    "2. Data Loading & Configuration (Cell 4)\n",
    "Loads MassSpecGym dataset (231K samples total)\n",
    "Splits into training (208K) and external test (23K) sets\n",
    "Configures hyperparameters (model dimensions, epochs, batch sizes)\n",
    "Inspects dataset structure (m/z values, intensities, SMILES, adducts)\n",
    "\n",
    "3. Data Preprocessing (Cells 5-7)\n",
    "SMILES Processing : Canonicalizes and augments SMILES with stereoisomers\n",
    "Spectrum Binning : Converts raw m/z peaks into 1000-bin vectors\n",
    "Graph Creation : Builds molecular graphs from spectra\n",
    "Feature Engineering : Extracts ion modes, precursor m/z, adduct types\n",
    "Data Cleaning : Handles shape issues and missing values\n",
    "\n",
    "4. XGBoost Baseline (Cell 8)\n",
    "Extracts numerical features from spectra (mean, std, max intensity, peak count)\n",
    "Encodes SMILES strings as classification targets\n",
    "Trains gradient boosting model on spectral features\n",
    "Provides feature importance analysis\n",
    "\n",
    "5. RAG System (Cells 10-15)\n",
    "Molecular Indexing : Creates semantic embeddings of molecular descriptions\n",
    "Fingerprint Database : Builds Morgan fingerprint index for structure similarity\n",
    "\n",
    "Search Capabilities :\n",
    "Semantic search using sentence transformers\n",
    "Structure-based search using Tanimoto similarity\n",
    "Hybrid search combining both approaches\n",
    "FAISS Integration : Enables fast similarity search\n",
    "\n",
    "6. Deep Learning Architecture (Cells 16-20)\n",
    "SELFIES Tokenization : Converts SMILES to robust molecular tokens\n",
    "\n",
    "Hybrid Model :\n",
    "Transformer Encoder : Processes binned spectra with attention\n",
    "GNN Encoder : Analyzes molecular graph structure\n",
    "Fusion Layer : Combines transformer and GNN representations\n",
    "Transformer Decoder : Generates SMILES sequences\n",
    "\n",
    "Training Pipeline :\n",
    "Self-supervised pretraining with masked language modeling\n",
    "Supervised fine-tuning with cross-entropy loss\n",
    "Beam search for inference\n",
    "\n",
    "7. Training & Validation (Cells 21-22)\n",
    "Cross-Validation : 5-fold CV for robust evaluation\n",
    "Hyperparameter Optimization : Uses Optuna for learning rate tuning\n",
    "Memory Management : Handles OOM errors with batch size reduction\n",
    "Model Checkpointing : Saves best models for each fold\n",
    "\n",
    "8. Evaluation & Analysis (Cells 23-24)\n",
    "Multiple Metrics : Tanimoto similarity, Dice coefficient, MCS overlap\n",
    "Molecular Properties : MW and LogP difference analysis\n",
    "Visualization : Attention weights, molecular structures, performance plots\n",
    "Error Analysis : Identifies failure modes and improvement areas\n",
    "\n",
    "9. Integration System (Cell 21)\n",
    "Ensemble Prediction : Combines XGBoost, Deep Learning, and RAG\n",
    "Weighted Scoring : Balances different prediction approaches\n",
    "Confidence Estimation : Provides uncertainty quantification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad17c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell1\n",
    "# Install required packages for MS-to-Structure pipeline\n",
    "#! pip install torch torch_geometric rdkit-pypi selfies datasets optuna nltk python-Levenshtein tqdm scikit-learn matplotlib xgboost faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f1ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2\n",
    "# Import libraries and set up logging for Jupyter compatibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
    "from rdkit.Chem.EnumerateStereoisomers import StereoEnumerationOptions\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import RDLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import optuna\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance\n",
    "import logging\n",
    "import traceback\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setup logging for Jupyter (prints to stdout)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed6ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cell 3\n",
    "# Set random seed for reproducibility and define global variables\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "\n",
    "# GPU optimization for RTX 3080 Ti\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdb68fb-0fe7-498d-99d4-5368de9b337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 231104 samples\n",
      "MassSpecGym size: 207993 External test size: 23111\n",
      "Dataset Columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge']\n",
      "\n",
      "First few rows of MassSpecGym dataset:\n",
      "             identifier                                                mzs  \\\n",
      "0  MassSpecGymID0000001  91.0542,125.0233,154.0499,155.0577,185.0961,20...   \n",
      "1  MassSpecGymID0000002  91.0542,125.0233,155.0577,185.0961,229.0859,24...   \n",
      "2  MassSpecGymID0000003  69.0343,91.0542,125.0233,127.039,153.0699,154....   \n",
      "3  MassSpecGymID0000004  69.0343,91.0542,110.06,111.0441,112.0393,120.0...   \n",
      "4  MassSpecGymID0000005  91.0542,125.0233,185.0961,229.0859,246.1125,28...   \n",
      "\n",
      "                                         intensities  \\\n",
      "0  0.24524524524524524,1.0,0.08008008008008008,0....   \n",
      "1  0.0990990990990991,0.28128128128128127,0.04004...   \n",
      "2  0.03403403403403404,0.31431431431431434,1.0,0....   \n",
      "3  0.17917917917917917,0.47347347347347346,0.0380...   \n",
      "4  0.07807807807807808,0.1841841841841842,0.03503...   \n",
      "\n",
      "                                          smiles  adduct  precursor_mz  \n",
      "0  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "1  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "2  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "3  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "4  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "\n",
      "Unique adduct values: ['[M+H]+' '[M+Na]+']\n"
     ]
    }
   ],
   "source": [
    "#cell 4\n",
    "# Production Configuration\n",
    "class Config:\n",
    "    DATASET_PATH = '/home/onepaw/dataset'  # Change to your dataset path\n",
    "    TRAIN_SPLIT = 0.9\n",
    "    RANDOM_SEED = 42\n",
    "    N_BINS = 1000\n",
    "    MAX_MZ = 1000\n",
    "    NOISE_LEVEL = 0.05\n",
    "    MAX_ISOMERS = 8\n",
    "    D_MODEL = 512\n",
    "    NHEAD = 8\n",
    "    NUM_LAYERS = 6\n",
    "    BATCH_SIZE = 64\n",
    "    SSL_EPOCHS = 3\n",
    "    SUPERVISED_EPOCHS = 30\n",
    "    LEARNING_RATE = 1e-4\n",
    "    PATIENCE = 5\n",
    "    N_FOLDS = 5\n",
    "    # Token definitions\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    SOS_TOKEN = '< SOS >'\n",
    "    EOS_TOKEN = '<EOS>'\n",
    "    MASK_TOKEN = '[MASK]'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Load dataset with configurable path\n",
    "try:\n",
    "    dataset = load_dataset(config.DATASET_PATH, split='train')\n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(f'Loaded dataset with {len(df)} samples')\n",
    "except Exception as e:\n",
    "    print(f'Error loading dataset: {e}')\n",
    "    print('Please update config.DATASET_PATH')\n",
    "    raise\n",
    "\n",
    "# Split dataset based on configuration\n",
    "split_idx = int(config.TRAIN_SPLIT * len(df))\n",
    "df_massspecgym = df.iloc[:split_idx].copy()\n",
    "df_external = df.iloc[split_idx:].copy()\n",
    "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
    "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
    "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7780db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 5\n",
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=None):\n",
    "    max_isomers = max_isomers or config.MAX_ISOMERS\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.StereoEnumerationOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [\n",
    "                Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers\n",
    "                ]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n",
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=None):\n",
    "    max_isomers = max_isomers or config.MAX_ISOMERS\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.StereoEnumerationOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [\n",
    "                Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers\n",
    "                ]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d72b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 6\n",
    "# Apply canonicalization, augmentation, and binning to the dataframe\n",
    "# Preprocess ion mode, precursor m/z, and adducts\n",
    "df_massspecgym['smiles'] = df_massspecgym['smiles'].apply(canonicalize_smiles)\n",
    "df_external['smiles'] = df_external['smiles'].apply(canonicalize_smiles)\n",
    "df_massspecgym = df_massspecgym.dropna(subset=['smiles'])\n",
    "df_external = df_external.dropna(subset=['smiles'])\n",
    "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].apply(augment_smiles)\n",
    "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list']).rename(columns={'smiles_list': 'smiles'})\n",
    "\n",
    "df_massspecgym['ion_mode'] = df_massspecgym['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_massspecgym['precursor_bin'] = pd.qcut(df_massspecgym['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "df_external['ion_mode'] = df_external['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_external['precursor_bin'] = pd.qcut(df_external['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx)\n",
    "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
    "\n",
    "def safe_bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct):\n",
    "    try:\n",
    "        # Clean mzs and intensities to remove non-numeric values\n",
    "        mzs_clean = [float(x) for x in mzs if isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','',1).replace('-','',1).isdigit())]\n",
    "        intensities_clean = [float(x) for x in intensities if isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','',1).replace('-','',1).isdigit())]\n",
    "        return bin_spectrum_to_graph(mzs_clean, intensities_clean, ion_mode, precursor_mz, adduct)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Skipping value error in safe_bin_spectrum_to_graph: {e}\")\n",
    "        return np.zeros(100), None\n",
    "\n",
    "df_massspecgym[['binned', 'graph_data']] = df_massspecgym.apply(\n",
    "    lambda row: pd.Series(safe_bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    "    )\n",
    "df_external[['binned', 'graph_data']] = df_external.apply(\n",
    "    lambda row: pd.Series(safe_bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "xgb_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing df_massspecgym['smiles']...\n",
      "Initial column shape: (562533, 2)\n",
      "Sample value: smiles     COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc(oc(c1)=O)[C@@H](NC(C)=O)Cc1ccccc1\n",
      "Name: 0, dtype: object\n",
      "Final column shape: (562533,)\n",
      "Final sample value: COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "\n",
      "Processing df_external['smiles']...\n",
      "Initial column shape: (23111,)\n",
      "Sample value: CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(OC)c(OC)c1-2)C(=O)Nc1nc(C(=O)OC)c(-c2ccccc2)s1\n",
      "Final column shape: (23111,)\n",
      "Final sample value: CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(OC)c(OC)c1-2)C(=O)Nc1nc(C(=O)OC)c(-c2ccccc2)s1\n",
      "\n",
      "Final verification:\n",
      "df_massspecgym['smiles'] shape: (562533, 2)\n",
      "df_external['smiles'] shape: (23111,)\n"
     ]
    }
   ],
   "source": [
    "#cell 7\n",
    "\n",
    "# Fix SMILES column shape issue\n",
    "# Ensure SMILES column is 1D and contains only strings\n",
    "def flatten_smiles_column(col):\n",
    "    # Debug print\n",
    "    print(f\"Initial column shape: {col.values.shape if hasattr(col.values, 'shape') else 'no shape'}\")\n",
    "    print(f\"Sample value: {col.iloc[0]}\")\n",
    "    \n",
    "    # Force to Series if DataFrame\n",
    "    if isinstance(col, pd.DataFrame):\n",
    "        col = col.iloc[:, 0]\n",
    "    \n",
    "    # Force 2D array to 1D\n",
    "    if hasattr(col.values, 'shape') and len(col.values.shape) > 1:\n",
    "        col = pd.Series(col.values.ravel())\n",
    "    \n",
    "    # Flatten any remaining sequences in cells\n",
    "    col = col.apply(lambda x: str(x[0]) if isinstance(x, (list, tuple, np.ndarray)) else str(x))\n",
    "    \n",
    "    # Debug print\n",
    "    print(f\"Final column shape: {col.values.shape if hasattr(col.values, 'shape') else 'no shape'}\")\n",
    "    print(f\"Final sample value: {col.iloc[0]}\")\n",
    "    \n",
    "    return col\n",
    "\n",
    "# Reset the column to ensure clean state\n",
    "df_massspecgym = df_massspecgym.copy()\n",
    "df_external = df_external.copy()\n",
    "\n",
    "print(\"Processing df_massspecgym['smiles']...\")\n",
    "df_massspecgym['smiles'] = flatten_smiles_column(df_massspecgym['smiles'])\n",
    "\n",
    "print(\"\\nProcessing df_external['smiles']...\")\n",
    "df_external['smiles'] = flatten_smiles_column(df_external['smiles'])\n",
    "\n",
    "# Verify the columns are 1D before LabelEncoder\n",
    "print(\"\\nFinal verification:\")\n",
    "print(f\"df_massspecgym['smiles'] shape: {df_massspecgym['smiles'].values.shape}\")\n",
    "print(f\"df_external['smiles'] shape: {df_external['smiles'].values.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xgb_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened SMILES shape: (562533,)\n",
      "Example: COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "y_all shape: (562533,)\n",
      "Number of unique labels: 25944\n",
      "X_all shape: (562533, 8)\n",
      "Filtered X_all shape: (49994, 8)\n",
      "Filtered number of unique labels: 1012\n",
      "Train classes: 1012\n",
      "Test classes: 992\n",
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.6966\n",
      "mean_intensity: 0.0061\n",
      "std_intensity: 0.0103\n",
      "max_intensity: 0.0056\n",
      "peak_count: 0.0056\n",
      "precursor_mz: 0.5358\n",
      "ion_mode: 0.0000\n",
      "adduct_idx: 0.4180\n",
      "spectrum_length: 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (562533, 8)\n",
      "Filtered X_all shape: (49994, 8)\n",
      "Filtered number of unique labels: 1012\n",
      "Train classes: 1012\n",
      "Test classes: 992\n",
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.6966\n",
      "mean_intensity: 0.0061\n",
      "std_intensity: 0.0103\n",
      "max_intensity: 0.0056\n",
      "peak_count: 0.0056\n",
      "precursor_mz: 0.5358\n",
      "ion_mode: 0.0000\n",
      "adduct_idx: 0.4180\n",
      "spectrum_length: 0.0185\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "# Extract features and prepare XGBoost data\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Flatten SMILES column\n",
    "# ----------------------------\n",
    "smiles_flat = df_massspecgym['smiles'].iloc[:, 0].astype(str)\n",
    "print(\"Flattened SMILES shape:\", smiles_flat.shape)\n",
    "print(\"Example:\", smiles_flat.iloc[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Encode labels\n",
    "# ----------------------------\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(smiles_flat)\n",
    "print(\"y_all shape:\", y_all.shape)\n",
    "print(\"Number of unique labels:\", len(np.unique(y_all)))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Feature extraction\n",
    "# ----------------------------\n",
    "def extract_features(df):\n",
    "    feats = []\n",
    "    for _, row in df.iterrows():\n",
    "        spectrum = np.array(row['binned'], dtype=np.float32)\n",
    "        mzs = row['mzs']\n",
    "        if isinstance(mzs, str):\n",
    "            mzs_list = [m for m in mzs.split(',') if m]\n",
    "        else:\n",
    "            mzs_list = mzs if isinstance(mzs, (list, tuple, np.ndarray)) else []\n",
    "        mz_len = len(mzs_list)\n",
    "\n",
    "        feat = [\n",
    "            float(np.mean(spectrum)),\n",
    "            float(np.std(spectrum)),\n",
    "            float(np.max(spectrum)),\n",
    "            float(np.sum(spectrum > 0.1)),\n",
    "            float(row['precursor_mz']),\n",
    "            float(row['ion_mode']),\n",
    "            float(row['adduct_idx']),\n",
    "            float(mz_len)\n",
    "        ]\n",
    "        feats.append(feat)\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "X_all = extract_features(df_massspecgym)\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Subsample if needed\n",
    "# ----------------------------\n",
    "subset_size = min(50000, X_all.shape[0])\n",
    "X_all = X_all[:subset_size]\n",
    "smiles_subset = smiles_flat[:subset_size]\n",
    "\n",
    "# Re-encode labels after subsetting\n",
    "le_subset = LabelEncoder()\n",
    "y_all = le_subset.fit_transform(smiles_subset)\n",
    "\n",
    "# Filter out classes with fewer than 2 samples\n",
    "from collections import Counter\n",
    "class_counts = Counter(y_all)\n",
    "valid_classes = [cls for cls, count in class_counts.items() if count >= 2]\n",
    "mask = np.isin(y_all, valid_classes)\n",
    "\n",
    "X_all = X_all[mask]\n",
    "y_all = y_all[mask]\n",
    "smiles_subset = smiles_subset[mask]\n",
    "\n",
    "# NEW: Re-encode labels after filtering to ensure consecutive integers\n",
    "le_final = LabelEncoder()\n",
    "y_all = le_final.fit_transform(smiles_subset)  # Re-encode filtered SMILES\n",
    "\n",
    "print(\"Filtered X_all shape:\", X_all.shape)\n",
    "print(\"Filtered number of unique labels:\", len(np.unique(y_all)))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train/test split (stratified)\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train).size)\n",
    "print(\"Test classes:\", np.unique(y_test).size)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) XGBoost model setup\n",
    "# ----------------------------\n",
    "n_classes = len(np.unique(y_train))\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    num_class=n_classes  # Explicitly set number of classes\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Train model\n",
    "# ----------------------------\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Predict & evaluate\n",
    "# ----------------------------\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Feature importances\n",
    "# ----------------------------\n",
    "feature_names = [\n",
    "    'mean_intensity', 'std_intensity', 'max_intensity', 'peak_count',\n",
    "    'precursor_mz', 'ion_mode', 'adduct_idx', 'spectrum_length'\n",
    "]\n",
    "\n",
    "for name, val in zip(feature_names, xgb_model.feature_importances_):\n",
    "    print(f\"{name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "xgb_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "True: C/C=C1/[C@H](O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]2O)OC=C(C(=O)OC)[C@H]1CC(=O)O[C@@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O\n",
      "Pred: C/C=C1/[C@H](O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]2O)OC=C(C(=O)OC)[C@H]1CC(=O)O[C@@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O\n",
      "Match: True\n",
      "\n",
      "True: C/C=C1/C[C@@]2(CO)O[C@@]2(C)C(=O)OCC2=CCN3CC[C@@H](OC1=O)[C@@H]23\n",
      "Pred: C/C=C1/C[C@@]2(CO)O[C@@]2(C)C(=O)OCC2=CCN3CC[C@@H](OC1=O)[C@@H]23\n",
      "Match: True\n",
      "\n",
      "True: C=C1CCCC2C1(C)CCC(C)C2(C)CC1=C(O)C(NCCC(=O)O)=CC(=O)C1=O\n",
      "Pred: C=C1C(O)CCC2(C)C1CCC1=C3CCC(C(C)CCC(C)C(C)C)C3(C)CCC12\n",
      "Match: False\n",
      "\n",
      "True: C=C(C)[C@H]1COc2cc3oc(=O)ccc3cc2O1\n",
      "Pred: C=C(C)[C@H]1COc2cc3oc(=O)ccc3cc2O1\n",
      "Match: True\n",
      "\n",
      "True: C=C(C(=O)O)C1CCC(C)C2CCC(=O)OC2(C)C1\n",
      "Pred: C=C1/C=C/C(=O)N(C)CC(=O)N[C@@H]([C@H](O)C(N)=O)C(=O)OC([C@H](C)CCCCCC)[C@H](C)C(=O)N1\n",
      "Match: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cell 9\n",
    "# Display results\n",
    "print('\\nSample predictions:')\n",
    "for i in range(min(5, len(y_test))):\n",
    "    true_smiles = le.inverse_transform([y_test[i]])[0]\n",
    "    pred_smiles = le.inverse_transform([y_pred[i]])[0]\n",
    "    print(f'True: {true_smiles}')\n",
    "    print(f'Pred: {pred_smiles}')\n",
    "    print(f'Match: {true_smiles == pred_smiles}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4041421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 03:52:47,501 INFO Use pytorch device_name: cuda:0\n",
      "2025-11-19 03:52:47,501 INFO Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing enhanced RAG system...\n",
      "Building semantic index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a39ce7a03143ff9b53d2139040b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building fingerprint index...\n",
      "RAG system ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:00:54] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:00:54] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:00:54] smiles\n",
      "[04:00:54] ~^\n",
      "[04:00:54] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n",
      "[04:00:54] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:00:54] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:00:54] smiles\n",
      "[04:00:54] ~^\n",
      "[04:00:54] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n"
     ]
    }
   ],
   "source": [
    "#cell 10\n",
    "# Enhanced RAG System for Molecular Data\n",
    "class MolecularRAG:\n",
    "    def __init__(self, df):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.df = df.copy()\n",
    "        self.morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "        self.build_molecular_descriptions()\n",
    "        self.build_index()\n",
    "        self.build_fingerprint_index()\n",
    "\n",
    "    def get_molecular_properties(self, smiles):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                mw = Descriptors.MolWt(mol)\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                hbd = Descriptors.NumHDonors(mol)\n",
    "                hba = Descriptors.NumHAcceptors(mol)\n",
    "                rings = Descriptors.RingCount(mol)\n",
    "                aromatic = Descriptors.NumAromaticRings(mol)\n",
    "                return {'mw': mw, 'logp': logp, 'hbd': hbd, 'hba': hba, 'rings': rings, 'aromatic': aromatic}\n",
    "        except:\n",
    "            pass\n",
    "        return {'mw': 0, 'logp': 0, 'hbd': 0, 'hba': 0, 'rings': 0, 'aromatic': 0}\n",
    "\n",
    "    def build_molecular_descriptions(self):\n",
    "        descriptions = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            props = self.get_molecular_properties(row['smiles'])\n",
    "            desc = f\"Molecule with SMILES {row['smiles']}. \"\n",
    "            desc += f\"Molecular weight: {props['mw']:.1f} Da. \"\n",
    "            desc += f\"LogP: {props['logp']:.2f}. \"\n",
    "            desc += f\"H-bond donors: {props['hbd']}, acceptors: {props['hba']}. \"\n",
    "            desc += f\"Contains {props['rings']} rings, {props['aromatic']} aromatic. \"\n",
    "            desc += f\"Adduct: {row['adduct']}, precursor m/z: {row['precursor_mz']:.2f}. \"\n",
    "            desc += f\"Ion mode: {'positive' if row['ion_mode'] == 0 else 'negative'}.\"\n",
    "            descriptions.append(desc)\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def build_index(self):\n",
    "        print('Building semantic index...')\n",
    "        self.embeddings = self.encoder.encode(self.descriptions, show_progress_bar=True)\n",
    "        self.semantic_index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.semantic_index.add(self.embeddings.astype('float32'))\n",
    "\n",
    "    def build_fingerprint_index(self):\n",
    "        print('Building fingerprint index...')\n",
    "        fingerprints = []\n",
    "        for smiles in self.df['smiles']:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                fp = self.morgan_gen.GetFingerprint(mol)\n",
    "                fp_array = np.zeros(2048)\n",
    "                DataStructs.ConvertToNumpyArray(fp, fp_array)\n",
    "                fingerprints.append(fp_array)\n",
    "            else:\n",
    "                fingerprints.append(np.zeros(2048))\n",
    "        self.fingerprints = np.array(fingerprints)\n",
    "        self.fp_index = faiss.IndexFlatIP(2048)\n",
    "        self.fp_index.add(self.fingerprints.astype('float32'))\n",
    "\n",
    "    def semantic_search(self, query, k=5):\n",
    "        query_emb = self.encoder.encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        scores, indices = self.semantic_index.search(query_emb.astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'score': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz'],\n",
    "                'description': self.descriptions[idx]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def structure_search(self, query_smiles, k=5):\n",
    "        mol = Chem.MolFromSmiles(query_smiles)\n",
    "        if not mol:\n",
    "            return []\n",
    "        query_fp = self.morgan_gen.GetFingerprint(mol)\n",
    "        query_array = np.zeros(2048)\n",
    "        DataStructs.ConvertToNumpyArray(query_fp, query_array)\n",
    "        scores, indices = self.fp_index.search(query_array.reshape(1, -1).astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'tanimoto': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz']\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query, query_smiles=None, k=5, alpha=0.7):\n",
    "        semantic_results = self.semantic_search(query, k*2)\n",
    "        if query_smiles:\n",
    "            structure_results = self.structure_search(query_smiles, k*2)\n",
    "            # Combine scores\n",
    "            combined = {}\n",
    "            for r in semantic_results:\n",
    "                combined[r['smiles']] = {'semantic': r['score'], 'structure': 0, 'data': r}\n",
    "            for r in structure_results:\n",
    "                if r['smiles'] in combined:\n",
    "                    combined[r['smiles']]['structure'] = r['tanimoto']\n",
    "                else:\n",
    "                    combined[r['smiles']] = {'semantic': 0, 'structure': r['tanimoto'], 'data': r}\n",
    "            # Hybrid scoring\n",
    "            for smiles in combined:\n",
    "                combined[smiles]['hybrid_score'] = alpha * combined[smiles]['semantic'] + (1-alpha) * combined[smiles]['structure']\n",
    "            sorted_results = sorted(combined.items(), key=lambda x: x[1]['hybrid_score'], reverse=True)\n",
    "            return [{'smiles': smiles, 'hybrid_score': data['hybrid_score'], 'semantic_score': data['semantic'], 'structure_score': data['structure']} for smiles, data in sorted_results[:k]]\n",
    "        return semantic_results[:k]\n",
    "\n",
    "print('Initializing enhanced RAG system...')\n",
    "rag_system = MolecularRAG(df_massspecgym)\n",
    "print('RAG system ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rag_semantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: aromatic compound with hydroxyl group\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52466b0de87e4a159a439b0297a0f70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "Name: 99678, dtype: object (Score: 0.3758)\n",
      "   Adduct: [M+H]+, m/z: 180.08\n",
      "2. SMILES: smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "Name: 99679, dtype: object (Score: 0.3756)\n",
      "   Adduct: [M+H]+, m/z: 180.08\n",
      "3. SMILES: smiles    CN(C)c1cccc(Br)c1\n",
      "smiles    CN(C)c1cccc(Br)c1\n",
      "Name: 52971, dtype: object (Score: 0.3739)\n",
      "   Adduct: [M+H]+, m/z: 200.01\n",
      "\n",
      "Query: small molecule with high logP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a338268324d3b935894697e9c66fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "2. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "3. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "\n",
      "Query: compound with multiple rings and nitrogen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae5a33712454b87b668e15e1c113c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16817, dtype: object (Score: 0.4792)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n",
      "2. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16815, dtype: object (Score: 0.4789)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n",
      "3. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16812, dtype: object (Score: 0.4781)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n"
     ]
    }
   ],
   "source": [
    "#cell 11\n",
    "# Semantic Search Examples\n",
    "queries = [\n",
    "    'aromatic compound with hydroxyl group',\n",
    "    'small molecule with high logP',\n",
    "    'compound with multiple rings and nitrogen'\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f'\\nQuery: {query}')\n",
    "    results = rag_system.semantic_search(query, k=3)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'{i+1}. SMILES: {result[\"smiles\"]} (Score: {result[\"score\"]:.4f})')\n",
    "        print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rag_structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure search for: c1ccccc1O\n",
      "1. SMILES: smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: 1, dtype: object (Tanimoto: 0.0000)\n",
      "   Adduct: [M+H]+, m/z: 288.12\n",
      "2. SMILES: smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: 0, dtype: object (Tanimoto: 0.0000)\n",
      "   Adduct: [M+H]+, m/z: 288.12\n",
      "3. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n",
      "4. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n",
      "5. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n"
     ]
    }
   ],
   "source": [
    "#cell 12\n",
    "# Structure-based Search\n",
    "query_smiles = 'c1ccccc1O'  # phenol\n",
    "print(f'Structure search for: {query_smiles}')\n",
    "results = rag_system.structure_search(query_smiles, k=5)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'{i+1}. SMILES: {result[\"smiles\"]} (Tanimoto: {result[\"tanimoto\"]:.4f})')\n",
    "    print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rag_hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 13\n",
    "# Hybrid Search\n",
    "query = 'compound with multiple rings and nitrogen'\n",
    "def hybrid_search(self, text_query, structure_query, k=5):\n",
    "    # 1️ Get semantic and structure search results\n",
    "    semantic_results = self.semantic_search(text_query, k=k)\n",
    "    structure_results = self.structure_search(structure_query, k=k)\n",
    "\n",
    "    # 2️ Merge safely\n",
    "    combined = {}\n",
    "    for r in semantic_results:\n",
    "        smiles_key = r['smiles']\n",
    "        if isinstance(smiles_key, pd.Series):\n",
    "            smiles_key = smiles_key.iloc[0]\n",
    "        if pd.isna(smiles_key):\n",
    "            continue\n",
    "        smiles_key = str(smiles_key)\n",
    "        combined[smiles_key] = {'semantic': r['score'], 'structure': 0, 'data': r}\n",
    "\n",
    "    for r in structure_results:\n",
    "        smiles_key = r['smiles']\n",
    "        if isinstance(smiles_key, pd.Series):\n",
    "            smiles_key = smiles_key.iloc[0]\n",
    "        if pd.isna(smiles_key):\n",
    "            continue\n",
    "        smiles_key = str(smiles_key)\n",
    "        if smiles_key in combined:\n",
    "            combined[smiles_key]['structure'] = r['score']\n",
    "        else:\n",
    "            combined[smiles_key] = {'semantic': 0, 'structure': r['score'], 'data': r}\n",
    "\n",
    "    # 3️ Compute hybrid score\n",
    "    results = []\n",
    "    for k_smiles, v in combined.items():\n",
    "        hybrid_score = v['semantic'] + v['structure']  # adjust weighting if needed\n",
    "        results.append({\n",
    "            'smiles': k_smiles,\n",
    "            'semantic_score': v['semantic'],\n",
    "            'structure_score': v['structure'],\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'data': v['data']\n",
    "        })\n",
    "\n",
    "    # 4️ Sort and return top-k\n",
    "    results = sorted(results, key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    return results[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rag_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG System Statistics:\n",
      "Total molecules indexed: 562533\n",
      "Embedding dimension: 384\n",
      "Fingerprint dimension: 2048\n",
      "No valid molecular weights found in the first 100 SMILES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:00:54] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:00:54] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:00:54] smiles\n",
      "[04:00:54] ~^\n",
      "[04:00:54] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n",
      "[04:00:54] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:00:54] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:00:54] smiles\n",
      "[04:00:54] ~^\n",
      "[04:00:54] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed1063f974b45039902bb652e1a3d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic search time: 0.0439s\n",
      "Structure search time: 0.0013s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 22 \n",
    "# RAG System Analysis\n",
    "import time\n",
    "from rdkit import Chem\n",
    "\n",
    "print('RAG System Statistics:')\n",
    "print(f'Total molecules indexed: {len(rag_system.df)}')\n",
    "print(f'Embedding dimension: {rag_system.embeddings.shape[1]}')\n",
    "print(f'Fingerprint dimension: {rag_system.fingerprints.shape[1]}')\n",
    "\n",
    "# Sample molecular properties distribution\n",
    "mw_values = []\n",
    "smiles_col = [col for col in rag_system.df.columns if 'smiles' in col.lower()][0]\n",
    "for smiles in rag_system.df[smiles_col].head(100):\n",
    "    props = rag_system.get_molecular_properties(smiles)\n",
    "    if props['mw'] > 0:  # Only include valid molecular weights\n",
    "        mw_values.append(props['mw'])\n",
    "if mw_values:\n",
    "    print(f'Sample MW range: {min(mw_values):.1f} - {max(mw_values):.1f} Da')\n",
    "else:\n",
    "    print(\"No valid molecular weights found in the first 100 SMILES.\")\n",
    "\n",
    "# Test query performance\n",
    "start = time.time()\n",
    "_ = rag_system.semantic_search('test query', k=10)\n",
    "semantic_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = rag_system.structure_search('CCO', k=10)\n",
    "structure_time = time.time() - start\n",
    "\n",
    "print(f'Semantic search time: {semantic_time:.4f}s')\n",
    "print(f'Structure search time: {structure_time:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f0a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "df_massspecgym shape: (562533, 20)\n",
      "Columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n"
     ]
    }
   ],
   "source": [
    "#cell 15\n",
    "# Hybrid Search Examples\n",
    "queries = [\n",
    "    'aromatic compound with hydroxyl group',\n",
    "    'small molecule with high logP',\n",
    "    'compound with multiple rings and nitrogen'\n",
    "]\n",
    "# Cell to verify df_massspecgym\n",
    "print('df_massspecgym' in globals())\n",
    "if 'df_massspecgym' in globals():\n",
    "    print(\"df_massspecgym shape:\", df_massspecgym.shape)\n",
    "    print(\"Columns:\", df_massspecgym.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "813f86e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:00:54,287 INFO Dataset size: (562533, 20)\n",
      "2025-11-19 04:00:54,288 INFO Columns in dataset: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-11-19 04:00:54,368 INFO First 5 rows:\n",
      "             identifier                                                mzs  \\\n",
      "0  MassSpecGymID0000001  91.0542,125.0233,154.0499,155.0577,185.0961,20...   \n",
      "1  MassSpecGymID0000002  91.0542,125.0233,155.0577,185.0961,229.0859,24...   \n",
      "2  MassSpecGymID0000003  69.0343,91.0542,125.0233,127.039,153.0699,154....   \n",
      "3  MassSpecGymID0000004  69.0343,91.0542,110.06,111.0441,112.0393,120.0...   \n",
      "4  MassSpecGymID0000005  91.0542,125.0233,185.0961,229.0859,246.1125,28...   \n",
      "\n",
      "                                         intensities  \\\n",
      "0  0.24524524524524524,1.0,0.08008008008008008,0....   \n",
      "1  0.0990990990990991,0.28128128128128127,0.04004...   \n",
      "2  0.03403403403403404,0.31431431431431434,1.0,0....   \n",
      "3  0.17917917917917917,0.47347347347347346,0.0380...   \n",
      "4  0.07807807807807808,0.1841841841841842,0.03503...   \n",
      "\n",
      "                                    smiles        inchikey    formula  \\\n",
      "0  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "1  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "2  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "3  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "4  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "\n",
      "  precursor_formula  parent_mass  precursor_mz  adduct instrument_type  \\\n",
      "0         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "1         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "2         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "3         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "4         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "\n",
      "   collision_energy   fold  simulation_challenge  \\\n",
      "0              30.0  train                  True   \n",
      "1              20.0  train                  True   \n",
      "2              40.0  train                  True   \n",
      "3              55.0  train                  True   \n",
      "4              10.0  train                  True   \n",
      "\n",
      "                                    smiles  ion_mode  precursor_bin  \\\n",
      "0  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "1  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "2  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "3  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "4  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "\n",
      "   adduct_idx                                             binned  \\\n",
      "0           0  [0.46928015209500606, 0.6944444444444444, 1.03...   \n",
      "1           0  [0.5189473636660348, 0.3523541331252303, 0.533...   \n",
      "2           0  [0.8205128205128205, 0.9487179487179487, 0.307...   \n",
      "3           0  [0.9428571428571428, 1.0, 0.3523809523809524, ...   \n",
      "4           0  [0.6052631578947368, 0.4473684210526316, 1.000...   \n",
      "\n",
      "                                          graph_data  \n",
      "0  [(x, [tensor([0.4693]), tensor([0.6944]), tens...  \n",
      "1  [(x, [tensor([0.5189]), tensor([0.3524]), tens...  \n",
      "2  [(x, [tensor([0.8205]), tensor([0.9487]), tens...  \n",
      "3  [(x, [tensor([0.9429]), tensor([1.]), tensor([...  \n",
      "4  [(x, [tensor([0.6053]), tensor([0.4474]), tens...  \n",
      "2025-11-19 04:00:54,369 WARNING Multiple SMILES columns found: ['smiles', 'smiles']\n",
      "2025-11-19 04:00:54,369 INFO Indices of 'smiles' columns: [3, 14]\n",
      "2025-11-19 04:00:54,370 INFO Sample data from 'smiles' column at index 3:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-11-19 04:00:54,370 INFO Sample data from 'smiles' column at index 14:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-11-19 04:00:54,370 INFO Using column 'smiles' at index 3 as SMILES source.\n",
      "2025-11-19 04:00:54,371 INFO Type of selected column: <class 'pandas.core.series.Series'>\n",
      "2025-11-19 04:02:13,730 INFO Extracted 562533 valid SMILES strings.\n",
      "2025-11-19 04:04:42,026 INFO Generated 562533 SELFIES strings successfully.\n",
      "2025-11-19 04:05:03,709 INFO SELFIES vocabulary size: 87\n",
      "2025-11-19 04:05:03,709 INFO Pretrain MAX_LEN: 100, Supervised MAX_LEN: 135\n"
     ]
    }
   ],
   "source": [
    "#cell 16\n",
    "# SELFIES Vocabulary Construction\n",
    "# Enhanced SMILES Extraction and SELFIES Vocabulary Construction\n",
    "\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "import selfies as sf\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Verify dataset\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "\n",
    "# Step 1: Inspect dataset\n",
    "logging.info(f\"Dataset size: {df_massspecgym.shape}\")\n",
    "logging.info(f\"Columns in dataset: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"First 5 rows:\\n{df_massspecgym.head(5)}\")\n",
    "\n",
    "# Step 2: Auto-detect SMILES column\n",
    "smiles_col_candidates = [col for col in df_massspecgym.columns if 'smiles' in col.lower()]\n",
    "if not smiles_col_candidates:\n",
    "    raise ValueError(\"No column containing 'smiles' found in MassSpecGym dataset.\")\n",
    "\n",
    "# Handle duplicate 'smiles' columns\n",
    "if len(smiles_col_candidates) > 1:\n",
    "    logging.warning(f\"Multiple SMILES columns found: {smiles_col_candidates}\")\n",
    "    # Get indices of all 'smiles' columns\n",
    "    smiles_col_indices = [i for i, col in enumerate(df_massspecgym.columns) if col == 'smiles']\n",
    "    logging.info(f\"Indices of 'smiles' columns: {smiles_col_indices}\")\n",
    "    \n",
    "    # Inspect contents of each 'smiles' column\n",
    "    for i, idx in enumerate(smiles_col_indices):\n",
    "        col_data = df_massspecgym.iloc[:, idx].head(5)\n",
    "        logging.info(f\"Sample data from 'smiles' column at index {idx}:\\n{col_data}\")\n",
    "    \n",
    "    smiles_col_index = smiles_col_indices[0]\n",
    "    smiles_col = 'smiles'\n",
    "else:\n",
    "    smiles_col_index = df_massspecgym.columns.get_loc(smiles_col_candidates[0])\n",
    "    smiles_col = smiles_col_candidates[0]\n",
    "\n",
    "logging.info(f\"Using column '{smiles_col}' at index {smiles_col_index} as SMILES source.\")\n",
    "\n",
    "# Step 3: Extract clean SMILES strings safely\n",
    "def is_valid_smiles(smiles):\n",
    "    if not isinstance(smiles, str) or not smiles.strip():\n",
    "        return False\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        return mol is not None\n",
    "    except ValueError as e:\n",
    "        logging.debug(f\"Invalid SMILES '{smiles}': {e}\")\n",
    "        return False\n",
    "\n",
    "smiles_series = df_massspecgym.iloc[:, smiles_col_index]\n",
    "logging.info(f\"Type of selected column: {type(smiles_series)}\")\n",
    "if not isinstance(smiles_series, pd.Series):\n",
    "    raise TypeError(f\"Selected column at index {smiles_col_index} is {type(smiles_series)}, expected pandas.Series\")\n",
    "\n",
    "all_smiles = smiles_series.dropna().tolist()\n",
    "valid_smiles = [s for s in all_smiles if is_valid_smiles(s)]\n",
    "logging.info(f\"Extracted {len(valid_smiles)} valid SMILES strings.\")\n",
    "if not valid_smiles:\n",
    "    raise ValueError(\"No valid SMILES could be extracted. Check your dataset!\")\n",
    "\n",
    "# Step 4: Convert to SELFIES\n",
    "all_selfies = []\n",
    "failed_conversions = []\n",
    "for s in valid_smiles:\n",
    "    try:\n",
    "        selfies_str = sf.encoder(s)\n",
    "        all_selfies.append(selfies_str)\n",
    "    except Exception as e:\n",
    "        failed_conversions.append((s, str(e)))\n",
    "        continue\n",
    "\n",
    "if failed_conversions:\n",
    "    logging.warning(f\"Failed to convert {len(failed_conversions)} SMILES to SELFIES. First few errors: {failed_conversions[:5]}\")\n",
    "if not all_selfies:\n",
    "    raise ValueError(\"No valid SELFIES could be generated. Check your SMILES extraction!\")\n",
    "logging.info(f\"Generated {len(all_selfies)} SELFIES strings successfully.\")\n",
    "\n",
    "# Step 5: Build SELFIES vocabulary\n",
    "selfies_alphabet = set()\n",
    "for s in all_selfies:\n",
    "    selfies_alphabet.update(sf.split_selfies(s))\n",
    "\n",
    "token_to_idx = {token: idx for idx, token in enumerate(sorted(selfies_alphabet))}\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "# Add special tokens if needed\n",
    "special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "for token in special_tokens:\n",
    "    if token not in token_to_idx:\n",
    "        token_to_idx[token] = len(token_to_idx)\n",
    "        idx_to_token[len(idx_to_token)] = token\n",
    "\n",
    "vocab_size = len(token_to_idx)\n",
    "PRETRAIN_MAX_LEN = min(100, max(len(list(sf.split_selfies(s))) for s in all_selfies) if all_selfies else 0)\n",
    "SUPERVISED_MAX_LEN = max(len(list(sf.split_selfies(s))) + 2 for s in all_selfies) if all_selfies else 0\n",
    "\n",
    "logging.info(f\"SELFIES vocabulary size: {vocab_size}\")\n",
    "logging.info(f\"Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}\")\n",
    "\n",
    "# Define encode_selfies function\n",
    "def encode_selfies(selfies_str, max_len):\n",
    "    \"\"\"Encode a SELFIES string into a list of token indices, padded/truncated to max_len.\"\"\"\n",
    "    tokens = list(sf.split_selfies(selfies_str))\n",
    "    token_indices = [token_to_idx.get(token, token_to_idx['<unk>']) for token in tokens]\n",
    "    token_indices = [token_to_idx['<start>']] + token_indices + [token_to_idx['<end>']]\n",
    "    if len(token_indices) > max_len:\n",
    "        token_indices = token_indices[:max_len]\n",
    "    padding_idx = token_to_idx['<pad>']\n",
    "    token_indices += [padding_idx] * (max_len - len(token_indices))\n",
    "    return token_indices\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({'SELFIES': all_selfies}).to_csv('massspecgym_selfies.csv', index=False)\n",
    "with open('selfies_vocab.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump({'token_to_idx': token_to_idx, 'idx_to_token': idx_to_token}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677d6a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:05:05,177 INFO df_massspecgym columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-11-19 04:05:05,177 INFO df_external columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-11-19 04:05:05,178 WARNING Multiple 'smiles' columns found in df_massspecgym at indices: [3, 14]\n",
      "2025-11-19 04:05:05,179 INFO Sample data from df_massspecgym 'smiles' at index 3:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-11-19 04:05:05,180 INFO Sample data from df_massspecgym 'smiles' at index 14:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-11-19 04:05:05,180 INFO Using df_massspecgym 'smiles' column at index 3\n",
      "2025-11-19 04:05:05,181 INFO Using df_external 'smiles' column at index 3\n",
      "2025-11-19 04:05:05,212 INFO Extracted 31602 unique SMILES strings.\n",
      "2025-11-19 04:05:12,054 INFO Generated fingerprints for 31602 SMILES strings.\n"
     ]
    }
   ],
   "source": [
    "#cell 17\n",
    "# Precompute Morgan Fingerprints for All Unique SMILES\n",
    "\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Verify datasets\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "if 'df_external' not in globals():\n",
    "    raise NameError(\"df_external is not defined. Please load the dataset.\")\n",
    "\n",
    "# Step 1: Inspect datasets\n",
    "logging.info(f\"df_massspecgym columns: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"df_external columns: {df_external.columns.tolist()}\")\n",
    "\n",
    "# Step 2: Select SMILES column from df_massspecgym\n",
    "massspecgym_smiles_cols = [i for i, col in enumerate(df_massspecgym.columns) if col == 'smiles']\n",
    "if not massspecgym_smiles_cols:\n",
    "    raise ValueError(\"No 'smiles' column found in df_massspecgym.\")\n",
    "if len(massspecgym_smiles_cols) > 1:\n",
    "    logging.warning(f\"Multiple 'smiles' columns found in df_massspecgym at indices: {massspecgym_smiles_cols}\")\n",
    "    for idx in massspecgym_smiles_cols:\n",
    "        logging.info(f\"Sample data from df_massspecgym 'smiles' at index {idx}:\\n{df_massspecgym.iloc[:, idx].head(5)}\")\n",
    "massspecgym_smiles_index = massspecgym_smiles_cols[0]\n",
    "logging.info(f\"Using df_massspecgym 'smiles' column at index {massspecgym_smiles_index}\")\n",
    "\n",
    "# Step 3: Select SMILES column from df_external\n",
    "external_smiles_cols = [i for i, col in enumerate(df_external.columns) if col == 'smiles']\n",
    "if not external_smiles_cols:\n",
    "    raise ValueError(\"No 'smiles' column found in df_external.\")\n",
    "if len(external_smiles_cols) > 1:\n",
    "    logging.warning(f\"Multiple 'smiles' columns found in df_external at indices: {external_smiles_cols}\")\n",
    "    for idx in external_smiles_cols:\n",
    "        logging.info(f\"Sample data from df_external 'smiles' at index {idx}:\\n{df_external.iloc[:, idx].head(5)}\")\n",
    "external_smiles_index = external_smiles_cols[0]\n",
    "logging.info(f\"Using df_external 'smiles' column at index {external_smiles_index}\")\n",
    "\n",
    "# Step 4: Extract and combine unique SMILES\n",
    "massspecgym_smiles = df_massspecgym.iloc[:, massspecgym_smiles_index].dropna().tolist()\n",
    "external_smiles = df_external.iloc[:, external_smiles_index].dropna().tolist()\n",
    "all_smiles = list(set(massspecgym_smiles + external_smiles))\n",
    "logging.info(f\"Extracted {len(all_smiles)} unique SMILES strings.\")\n",
    "\n",
    "# Step 5: Precompute Morgan fingerprints for all unique SMILES\n",
    "all_fingerprints = {}\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "for smiles in all_smiles:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            all_fingerprints[smiles] = morgan_gen.GetFingerprint(mol)\n",
    "        else:\n",
    "            logging.warning(f\"Invalid SMILES '{smiles}' skipped during fingerprint generation.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to process SMILES '{smiles}': {e}\")\n",
    "logging.info(f\"Generated fingerprints for {len(all_fingerprints)} SMILES strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 18\n",
    "# Dataset class for MS/MS data\n",
    "\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=PRETRAIN_MAX_LEN, is_ssl=False):\n",
    "        self.spectra = np.stack(dataframe['binned'].values)\n",
    "        self.graph_data = dataframe['graph_data'].values\n",
    "        self.ion_modes = dataframe['ion_mode'].values\n",
    "        self.precursor_bins = dataframe['precursor_bin'].values\n",
    "        self.adduct_indices = dataframe['adduct_idx'].values\n",
    "        self.raw_smiles = dataframe['smiles'].values\n",
    "\n",
    "        self.is_ssl = is_ssl\n",
    "        self.ssl_max_len = max_len                 # for self-supervised (pretrain)\n",
    "        self.sup_max_len = SUPERVISED_MAX_LEN      # for supervised fine-tune\n",
    "\n",
    "        if is_ssl:\n",
    "            # self-supervised: create original & masked SELFIES encodings\n",
    "            self.smiles = []\n",
    "            self.masked_smiles = []\n",
    "            for s in self.raw_smiles:\n",
    "                selfies = sf.encoder(s)\n",
    "                masked_s, orig_s = self.mask_selfies(selfies)\n",
    "                self.smiles.append(encode_selfies(orig_s, self.ssl_max_len))\n",
    "                self.masked_smiles.append(encode_selfies(masked_s, self.ssl_max_len))\n",
    "        else:\n",
    "            # supervised: only original SELFIES with supervised max length\n",
    "            self.smiles = [\n",
    "                encode_selfies(sf.encoder(s), max_len=self.sup_max_len)\n",
    "                for s in self.raw_smiles\n",
    "            ]\n",
    "\n",
    "    def mask_selfies(self, selfies, mask_ratio=0.10):\n",
    "        \"\"\"Randomly mask a fraction of SELFIES tokens for SSL pretraining.\"\"\"\n",
    "        max_len = self.ssl_max_len\n",
    "        try:\n",
    "            # split_selfies returns a generator → convert to list, then truncate\n",
    "            tokens = list(sf.split_selfies(selfies))[:max_len - 2]\n",
    "\n",
    "            masked_tokens = tokens.copy()\n",
    "            n_mask = int(mask_ratio * len(tokens))\n",
    "\n",
    "            if n_mask > 0:\n",
    "                mask_indices = np.random.choice(len(tokens), n_mask, replace=False)\n",
    "                for idx in mask_indices:\n",
    "                    masked_tokens[idx] = config.MASK_TOKEN\n",
    "\n",
    "            # return masked version and original (both as SELFIES strings)\n",
    "            return ''.join(masked_tokens), ''.join(tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"mask_selfies failed for {selfies}: {e}\\n{traceback.format_exc()}\"\n",
    "            )\n",
    "            # Fallback: no masking so dataset creation doesn't crash\n",
    "            return selfies, selfies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectra)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_ssl:\n",
    "            return (\n",
    "                torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "                self.graph_data[idx],\n",
    "                torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.masked_smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "                torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "                torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "                self.raw_smiles[idx],\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "            self.graph_data[idx],\n",
    "            torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "            torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "            torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "            self.raw_smiles[idx],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6336cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 19\n",
    "# Positional encoding and model encoder/decoder classes\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Neural Network Models\n",
    "class SpectrumTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.unsqueeze(-1))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SpectrumGNNEncoder(MessagePassing):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.d_model = d_model\n",
    "        self.lin = nn.Linear(1, d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.lin(x)\n",
    "        x = self.propagate(edge_index, x=x)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return self.mlp(x_j)\n",
    "\n",
    "class SmilesTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        output = self.transformer(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return self.output_proj(output)\n",
    "\n",
    "class MSMS2SmilesHybrid(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, **kwargs):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder = SpectrumTransformerEncoder(d_model, nhead, num_layers)\n",
    "        self.gnn_encoder = SpectrumGNNEncoder(d_model)\n",
    "        self.decoder = SmilesTransformerDecoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.fusion = nn.Linear(d_model * 2, d_model)\n",
    "\n",
    "    def forward(self, spectrum, graph_data, tgt, tgt_mask=None):\n",
    "        transformer_out = self.transformer_encoder(spectrum)\n",
    "        gnn_out = self.gnn_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        memory = self.fusion(torch.cat([transformer_out.mean(1), gnn_out], dim=1)).unsqueeze(1)\n",
    "        return self.decoder(tgt, memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 20\n",
    "# Training and evaluation functions\n",
    "\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "\n",
    "def make_graph_batch(graph_data, device):\n",
    "    \"\"\"\n",
    "    Handle graph_data coming from either:\n",
    "    - torch_geometric.loader.DataLoader  -> already a Batch / tupleBatch\n",
    "    - torch.utils.data.DataLoader        -> list/tuple of Data\n",
    "    - a single Data object\n",
    "    \"\"\"\n",
    "    # Already a PyG Batch (common with GeoDataLoader)\n",
    "    if isinstance(graph_data, GeoBatch):\n",
    "        return graph_data.to(device)\n",
    "\n",
    "    # Likely a sequence (list/tuple/tupleBatch) of Data\n",
    "    try:\n",
    "        return GeoBatch.from_data_list(list(graph_data)).to(device)\n",
    "    except Exception:\n",
    "        # Fallback: assume single Data\n",
    "        return GeoBatch.from_data_list([graph_data]).to(device)\n",
    "\n",
    "\n",
    "def ssl_pretrain(model, dataloader, epochs=3, lr=1e-4):\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f'SSL Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, masked, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            graph_batch = make_graph_batch(graph_data, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # For “true” SSL you might want masked[:, :-1] here instead of target[:, :-1]\n",
    "                output = model(spectrum, graph_batch, target[:, :-1])\n",
    "                loss = F.cross_entropy(\n",
    "                    output.reshape(-1, output.size(-1)),\n",
    "                    target[:, 1:].reshape(-1),\n",
    "                    ignore_index=0\n",
    "                )\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        print(f'SSL Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "def supervised_train(model, train_loader, val_loader, epochs=30, lr=1e-4, patience=5):\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            graph_batch = make_graph_batch(graph_data, device)\n",
    "            \n",
    "            # Create attention (causal) mask\n",
    "            tgt_mask = torch.triu(\n",
    "                torch.ones(target.size(1)-1, target.size(1)-1, device=device),\n",
    "                diagonal=1\n",
    "            ).bool()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(spectrum, graph_batch, target[:, :-1], tgt_mask=tgt_mask)\n",
    "                loss = F.cross_entropy(\n",
    "                    output.reshape(-1, output.size(-1)),\n",
    "                    target[:, 1:].reshape(-1),\n",
    "                    ignore_index=0\n",
    "                )\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                spectrum, graph_data, target, _, _, _, _ = batch\n",
    "                spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                graph_batch = make_graph_batch(graph_data, device)\n",
    "                with autocast():\n",
    "                    output = model(spectrum, graph_batch, target[:, :-1])\n",
    "                    loss = F.cross_entropy(\n",
    "                        output.reshape(-1, output.size(-1)),\n",
    "                        target[:, 1:].reshape(-1),\n",
    "                        ignore_index=0\n",
    "                    )\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "def beam_search(model, spectrum, graph_data, ion_mode, precursor_bin, adduct_idx,\n",
    "                true_smiles, beam_width=5, max_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device)\n",
    "        # graph_data here is usually a single Data, so make_graph_batch will handle it\n",
    "        graph_batch = make_graph_batch(graph_data, device)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        sequences = [[token_to_idx[config.SOS_TOKEN]]]\n",
    "        scores = [0.0]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            for i, seq in enumerate(sequences):\n",
    "                if seq[-1] == token_to_idx[config.EOS_TOKEN]:\n",
    "                    candidates.append((seq, scores[i]))\n",
    "                    continue\n",
    "                \n",
    "                tgt = torch.tensor([seq]).to(device)\n",
    "                output = model(spectrum, graph_batch, tgt)\n",
    "                probs = F.softmax(output[0, -1], dim=-1)\n",
    "                \n",
    "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                for prob, idx in zip(top_probs, top_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = scores[i] + torch.log(prob).item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            sequences = [seq for seq, _ in candidates[:beam_width]]\n",
    "            scores = [score for _, score in candidates[:beam_width]]\n",
    "        \n",
    "        results = []\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            smiles = decode_selfies(seq)\n",
    "            if smiles:\n",
    "                results.append((smiles, score))\n",
    "        return results[:beam_width]\n",
    "\n",
    "\n",
    "# Missing evaluation functions\n",
    "def mw_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolWt(mol1) - Descriptors.MolWt(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "\n",
    "def logp_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolLogP(mol1) - Descriptors.MolLogP(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "\n",
    "def substructure_match(smiles1, smiles2, substructures=None):\n",
    "    return 0.5  # Placeholder\n",
    "\n",
    "\n",
    "def error_analysis(pred_list, true_list, adduct_list, fingerprints):\n",
    "    print('Error analysis completed')\n",
    "\n",
    "\n",
    "def plot_attention_weights(weights, title='Attention'):\n",
    "    print(f'Attention visualization: {title}')\n",
    "\n",
    "\n",
    "def plot_gnn_edge_weights(weights, edges, title='GNN'):\n",
    "    print(f'GNN visualization: {title}')\n",
    "\n",
    "\n",
    "def calculate_bleu(predicted_smiles, true_smiles):\n",
    "    try:\n",
    "        pred_tokens = list(predicted_smiles)\n",
    "        true_tokens = list(true_smiles)\n",
    "        return sentence_bleu([true_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2, fingerprint_dict):\n",
    "    if smiles1 in fingerprint_dict and smiles2 in fingerprint_dict:\n",
    "        return DataStructs.TanimotoSimilarity(fingerprint_dict[smiles1], fingerprint_dict[smiles2])\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def validity_rate(smiles_list):\n",
    "    valid = sum(1 for s in smiles_list if Chem.MolFromSmiles(s) is not None)\n",
    "    return (valid / len(smiles_list)) * 100 if smiles_list else 0\n",
    "\n",
    "\n",
    "def objective(trial, train_data, val_data):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    return lr  # Simplified for demo\n",
    "\n",
    "\n",
    "# Additional metrics and visualization\n",
    "def dice_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            fp1 = Chem.RDKFingerprint(mol1)\n",
    "            fp2 = Chem.RDKFingerprint(mol2)\n",
    "            return DataStructs.DiceSimilarity(fp1, fp2)\n",
    "    except:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def mcs_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            mcs = rdFMCS.FindMCS([mol1, mol2])\n",
    "            return mcs.numAtoms / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    except:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def prediction_diversity(smiles_list):\n",
    "    unique_smiles = set(smiles_list)\n",
    "    return len(unique_smiles) / len(smiles_list) if smiles_list else 0\n",
    "\n",
    "\n",
    "def plot_molecular_comparison(true_smiles, pred_smiles, title='Comparison'):\n",
    "    try:\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "        pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "        if true_mol and pred_mol:\n",
    "            img = Draw.MolsToGridImage(\n",
    "                [true_mol, pred_mol],\n",
    "                molsPerRow=2,\n",
    "                subImgSize=(300, 300),\n",
    "                legends=['True', 'Predicted']\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.axis('off')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Visualization error: {e}')\n",
    "\n",
    "\n",
    "# Model checkpointing\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'vocab_size': vocab_size,\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "\n",
    "# Data validation functions\n",
    "def validate_spectrum_quality(mzs, intensities, min_peaks=5, max_mz_range=2000):\n",
    "    if len(mzs) < min_peaks or max(mzs) > max_mz_range:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_molecular_properties(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return False\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        return 50 <= mw <= 1000\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def remove_duplicates(df, subset=['smiles', 'precursor_mz']):\n",
    "    return df.drop_duplicates(subset=subset, keep='first')\n",
    "\n",
    "\n",
    "# Memory management\n",
    "def clear_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration system ready\n"
     ]
    }
   ],
   "source": [
    "#cell 21\n",
    "# Integration: XGBoost + RAG + Deep Learning\n",
    "class HybridPredictor:\n",
    "    def __init__(self, dl_model, xgb_model, rag_system, label_encoder):\n",
    "        self.dl_model = dl_model\n",
    "        self.xgb_model = xgb_model\n",
    "        self.rag_system = rag_system\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def predict_ensemble(self, spectrum, graph_data, features, query_text=None, weights=[0.5, 0.3, 0.2]):\n",
    "        # Ensure spectrum is a torch tensor on the correct device\n",
    "        if not isinstance(spectrum, torch.Tensor):\n",
    "            spectrum = torch.tensor(spectrum, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spectrum = spectrum.to(device)\n",
    "\n",
    "        # Validate weights (normalize if slightly off)\n",
    "        if abs(sum(weights) - 1.0) > 0.01:\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Deep learning prediction with error handling\n",
    "        try:\n",
    "            dl_results = beam_search(\n",
    "                self.dl_model,\n",
    "                spectrum,\n",
    "                graph_data,\n",
    "                ion_mode=0,\n",
    "                precursor_bin=0,\n",
    "                adduct_idx=0,\n",
    "                true_smiles='',\n",
    "                beam_width=5,\n",
    "                device=device\n",
    "            )\n",
    "            if dl_results and dl_results[0][0]:\n",
    "                predictions.append(('DL', dl_results[0][0], weights[0]))\n",
    "        except Exception as e:\n",
    "            print(f'DL prediction failed: {e}')\n",
    "        \n",
    "        # XGBoost prediction with error handling\n",
    "        try:\n",
    "            xgb_pred = self.xgb_model.predict([features])[0]\n",
    "            xgb_smiles = self.label_encoder.inverse_transform([xgb_pred])[0]\n",
    "            predictions.append(('XGB', xgb_smiles, weights[1]))\n",
    "        except Exception as e:\n",
    "            print(f'XGBoost prediction failed: {e}')\n",
    "        \n",
    "        # RAG prediction with error handling\n",
    "        if query_text:\n",
    "            try:\n",
    "                rag_results = self.rag_system.semantic_search(query_text, k=1)\n",
    "                if rag_results and len(rag_results) > 0:\n",
    "                    predictions.append(('RAG', rag_results[0]['smiles'], weights[2]))\n",
    "            except Exception as e:\n",
    "                print(f'RAG prediction failed: {e}')\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def evaluate_ensemble(self, test_data, n_samples=10):\n",
    "        results = {'dl': [], 'xgb': [], 'rag': [], 'ensemble': []}\n",
    "        \n",
    "        for i in range(min(n_samples, len(test_data))):\n",
    "            row = test_data.iloc[i]\n",
    "            true_smiles = row['smiles']\n",
    "            \n",
    "            # Extract features\n",
    "            spectrum = row['binned']          # numpy array\n",
    "            graph_data = row['graph_data']    # PyG Data object\n",
    "            features = [\n",
    "                np.mean(spectrum),\n",
    "                np.std(spectrum),\n",
    "                np.max(spectrum),\n",
    "                np.sum(spectrum > 0.1),\n",
    "                row['precursor_mz'],\n",
    "                row['ion_mode'],\n",
    "                row['adduct_idx'],\n",
    "                len(row['mzs'])\n",
    "            ]\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            preds = self.predict_ensemble(\n",
    "                spectrum,\n",
    "                graph_data,\n",
    "                features,\n",
    "                query_text=f\"molecule with MW {row['precursor_mz']:.1f}\"\n",
    "            )\n",
    "\n",
    "            if not preds:\n",
    "                continue  # nothing to evaluate for this sample\n",
    "            \n",
    "            # Evaluate each method\n",
    "            for method, pred_smiles, weight in preds:\n",
    "                similarity = tanimoto_similarity(pred_smiles, true_smiles, all_fingerprints)\n",
    "                results[method.lower()].append(similarity)\n",
    "            \n",
    "            # Weighted ensemble score\n",
    "            total_weight = sum(w for _, _, w in preds)\n",
    "            if total_weight > 0:\n",
    "                ensemble_score = sum(\n",
    "                    tanimoto_similarity(pred, true_smiles, all_fingerprints) * w\n",
    "                    for _, pred, w in preds\n",
    "                ) / total_weight\n",
    "                results['ensemble'].append(ensemble_score)\n",
    "        \n",
    "        return {k: np.mean(v) if v else 0 for k, v in results.items()}\n",
    "\n",
    "print('Integration system ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb652e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:05:12,214 INFO df_massspecgym columns after removing duplicates: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-11-19 04:05:12,214 INFO df_external columns after removing duplicates: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-11-19 04:05:19,131 INFO External dataset size: 23111\n",
      "2025-11-19 04:05:19,142 INFO \n",
      "Fold 1/5\n",
      "[I 2025-11-19 04:08:47,983] A new study created in memory with name: no-name-46d1e456-7aae-4c15-be2a-7a8ec35f18b3\n",
      "[I 2025-11-19 04:08:47,984] Trial 0 finished with value: 0.0002792285899920133 and parameters: {'lr': 0.0002792285899920133}. Best is trial 0 with value: 0.0002792285899920133.\n",
      "[I 2025-11-19 04:08:47,985] Trial 1 finished with value: 0.0008331372790301956 and parameters: {'lr': 0.0008331372790301956}. Best is trial 0 with value: 0.0002792285899920133.\n",
      "[I 2025-11-19 04:08:47,986] Trial 2 finished with value: 0.000548350775006724 and parameters: {'lr': 0.000548350775006724}. Best is trial 0 with value: 0.0002792285899920133.\n",
      "[I 2025-11-19 04:08:47,986] Trial 3 finished with value: 3.493682407234493e-05 and parameters: {'lr': 3.493682407234493e-05}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,986] Trial 4 finished with value: 0.0005505893386680396 and parameters: {'lr': 0.0005505893386680396}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,987] Trial 5 finished with value: 0.0004566800086055357 and parameters: {'lr': 0.0004566800086055357}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,987] Trial 6 finished with value: 0.00020638311501178884 and parameters: {'lr': 0.00020638311501178884}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,987] Trial 7 finished with value: 6.825114145185413e-05 and parameters: {'lr': 6.825114145185413e-05}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,988] Trial 8 finished with value: 4.580364757649552e-05 and parameters: {'lr': 4.580364757649552e-05}. Best is trial 3 with value: 3.493682407234493e-05.\n",
      "[I 2025-11-19 04:08:47,988] Trial 9 finished with value: 1.131228343143913e-05 and parameters: {'lr': 1.131228343143913e-05}. Best is trial 9 with value: 1.131228343143913e-05.\n",
      "2025-11-19 04:08:47,988 INFO Best learning rate for fold 1: 0.000011\n",
      "2025-11-19 04:08:48,079 INFO Starting SSL pretraining for fold 1 with train_bs=8, ssl_bs=16...\n",
      "/tmp/ipykernel_566802/2266101679.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a30d29bf7b04245a86f60a482074f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 1:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_566802/2266101679.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1 Loss: 0.8453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9859fa33427042c7811e42196650a92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 2:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 2 Loss: 0.6898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987e5d64bf854f24b7036512a9f71f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 3:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:48:04,518 INFO Starting supervised training for fold 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 3 Loss: 0.5377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_566802/2266101679.py:57: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aacac8e37f4bcb98526fc2ba2cc81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_566802/2266101679.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_566802/2266101679.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3470, Val Loss: 1.8403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73144ec4b0f64e308b72cf4aaaaa16c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2281, Val Loss: 2.3627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0e916002ec4a71a3f0b9f1001d93bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1886, Val Loss: 2.8418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fda189dacb245e78d19a96e4e1de1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1665, Val Loss: 2.5303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6432d161c5ca494fa665430c6392b500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1519, Val Loss: 2.0756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585e7486f5f4f8090ba3b90c8ece51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1415, Val Loss: 1.9566\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='best_msms_hybrid_fold_1.pt' target='_blank'>best_msms_hybrid_fold_1.pt</a><br>"
      ],
      "text/plain": [
       "/home/onepaw/naturems-onepaw/best_msms_hybrid_fold_1.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 11:11:02,703 INFO \n",
      "Fold 2/5\n",
      "[I 2025-11-19 11:14:31,674] A new study created in memory with name: no-name-ec7f8262-2608-4a76-89eb-8b16e19d51c1\n",
      "[I 2025-11-19 11:14:31,674] Trial 0 finished with value: 6.194087750668737e-05 and parameters: {'lr': 6.194087750668737e-05}. Best is trial 0 with value: 6.194087750668737e-05.\n",
      "[I 2025-11-19 11:14:31,675] Trial 1 finished with value: 0.00021306179025333818 and parameters: {'lr': 0.00021306179025333818}. Best is trial 0 with value: 6.194087750668737e-05.\n",
      "[I 2025-11-19 11:14:31,675] Trial 2 finished with value: 1.5359791733001554e-05 and parameters: {'lr': 1.5359791733001554e-05}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,676] Trial 3 finished with value: 0.0007827674805337467 and parameters: {'lr': 0.0007827674805337467}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,676] Trial 4 finished with value: 0.0003959116260564962 and parameters: {'lr': 0.0003959116260564962}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,677] Trial 5 finished with value: 3.855851199603004e-05 and parameters: {'lr': 3.855851199603004e-05}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,677] Trial 6 finished with value: 5.556356530247457e-05 and parameters: {'lr': 5.556356530247457e-05}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,677] Trial 7 finished with value: 0.0003912656758663789 and parameters: {'lr': 0.0003912656758663789}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,678] Trial 8 finished with value: 0.00016025292297343116 and parameters: {'lr': 0.00016025292297343116}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "[I 2025-11-19 11:14:31,679] Trial 9 finished with value: 0.00018155825895655307 and parameters: {'lr': 0.00018155825895655307}. Best is trial 2 with value: 1.5359791733001554e-05.\n",
      "2025-11-19 11:14:31,679 INFO Best learning rate for fold 2: 0.000015\n",
      "2025-11-19 11:14:31,775 INFO Starting SSL pretraining for fold 2 with train_bs=8, ssl_bs=16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da3697d5ef04778a19494c47f8eb5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 1:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1 Loss: 0.8225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fd731e00554f92b54a36e438796203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 2:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 2 Loss: 0.6171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a40a0a82e546f39fbd1d5bf73076dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 3:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 11:53:56,408 INFO Starting supervised training for fold 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 3 Loss: 0.4077\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5111cdda0528442381871d26d41c9a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3164, Val Loss: 1.6130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898ee4affe16402281a32f0d3810bdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2032, Val Loss: 1.9953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a545cb82174aa8a3675fe05653c7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1686, Val Loss: 2.2239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5b0318b6714d86a5f3480037028f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1496, Val Loss: 1.9556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7478db00d3ef447d9d30e2e26f6ed517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1372, Val Loss: 2.2566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6cca933aa2465a8fe9e1a79c36290e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1283, Val Loss: 2.1675\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='best_msms_hybrid_fold_2.pt' target='_blank'>best_msms_hybrid_fold_2.pt</a><br>"
      ],
      "text/plain": [
       "/home/onepaw/naturems-onepaw/best_msms_hybrid_fold_2.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 18:18:58,154 INFO \n",
      "Fold 3/5\n",
      "[I 2025-11-19 18:22:27,001] A new study created in memory with name: no-name-72521e91-9284-4ec9-9762-91314a402c07\n",
      "[I 2025-11-19 18:22:27,002] Trial 0 finished with value: 0.0001032367669556582 and parameters: {'lr': 0.0001032367669556582}. Best is trial 0 with value: 0.0001032367669556582.\n",
      "[I 2025-11-19 18:22:27,003] Trial 1 finished with value: 5.966805509951177e-05 and parameters: {'lr': 5.966805509951177e-05}. Best is trial 1 with value: 5.966805509951177e-05.\n",
      "[I 2025-11-19 18:22:27,003] Trial 2 finished with value: 3.213029237082538e-05 and parameters: {'lr': 3.213029237082538e-05}. Best is trial 2 with value: 3.213029237082538e-05.\n",
      "[I 2025-11-19 18:22:27,003] Trial 3 finished with value: 2.586835394494521e-05 and parameters: {'lr': 2.586835394494521e-05}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,004] Trial 4 finished with value: 6.295667178538748e-05 and parameters: {'lr': 6.295667178538748e-05}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,004] Trial 5 finished with value: 0.0001304043244469699 and parameters: {'lr': 0.0001304043244469699}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,005] Trial 6 finished with value: 3.670069313816484e-05 and parameters: {'lr': 3.670069313816484e-05}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,005] Trial 7 finished with value: 6.868575431347838e-05 and parameters: {'lr': 6.868575431347838e-05}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,005] Trial 8 finished with value: 0.00033216014382201746 and parameters: {'lr': 0.00033216014382201746}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "[I 2025-11-19 18:22:27,006] Trial 9 finished with value: 6.417293100581841e-05 and parameters: {'lr': 6.417293100581841e-05}. Best is trial 3 with value: 2.586835394494521e-05.\n",
      "2025-11-19 18:22:27,007 INFO Best learning rate for fold 3: 0.000026\n",
      "2025-11-19 18:22:27,094 INFO Starting SSL pretraining for fold 3 with train_bs=8, ssl_bs=16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e5e159206048679f49fb5628844550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 1:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1 Loss: 0.7675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1130c8bd8c4cbe89f447397f3ab756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 2:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 2 Loss: 0.4179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5150ee3280a49388236669867b8c3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 3:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 19:01:52,299 INFO Starting supervised training for fold 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 3 Loss: 0.2390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4373c7246064112849031dc0c21ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.2722, Val Loss: 1.6226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522040a4ce3946e2a2a752505bcc0fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.1717, Val Loss: 1.6672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4a5fd541954351af67fb63f36ee32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1440, Val Loss: 1.5602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e4d53ded8447881d0bf9c443c6f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1292, Val Loss: 1.8181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af37e77d9944ec9e229faf0024f524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1197, Val Loss: 1.7263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386a7bf9ccc14fc0af2136d087a52953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1130, Val Loss: 1.8249\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dc0d8f43ec47e2b4dfa808fc324eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 7:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.1077, Val Loss: 2.2292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789f5ffb442b4e36b0913d429c71084c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 8:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.1035, Val Loss: 2.2150\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='best_msms_hybrid_fold_3.pt' target='_blank'>best_msms_hybrid_fold_3.pt</a><br>"
      ],
      "text/plain": [
       "/home/onepaw/naturems-onepaw/best_msms_hybrid_fold_3.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 03:33:59,431 INFO \n",
      "Fold 4/5\n",
      "[I 2025-11-20 03:37:33,627] A new study created in memory with name: no-name-45c52553-aafd-4ceb-87e2-cf656ea0f5bf\n",
      "[I 2025-11-20 03:37:33,633] Trial 0 finished with value: 0.0009244023700960408 and parameters: {'lr': 0.0009244023700960408}. Best is trial 0 with value: 0.0009244023700960408.\n",
      "[I 2025-11-20 03:37:33,633] Trial 1 finished with value: 3.964538104980318e-05 and parameters: {'lr': 3.964538104980318e-05}. Best is trial 1 with value: 3.964538104980318e-05.\n",
      "[I 2025-11-20 03:37:33,634] Trial 2 finished with value: 1.0294526142437247e-05 and parameters: {'lr': 1.0294526142437247e-05}. Best is trial 2 with value: 1.0294526142437247e-05.\n",
      "[I 2025-11-20 03:37:33,634] Trial 3 finished with value: 7.527120538990596e-05 and parameters: {'lr': 7.527120538990596e-05}. Best is trial 2 with value: 1.0294526142437247e-05.\n",
      "[I 2025-11-20 03:37:33,635] Trial 4 finished with value: 1.1238236740539683e-05 and parameters: {'lr': 1.1238236740539683e-05}. Best is trial 2 with value: 1.0294526142437247e-05.\n",
      "[I 2025-11-20 03:37:33,635] Trial 5 finished with value: 0.0005274082223947236 and parameters: {'lr': 0.0005274082223947236}. Best is trial 2 with value: 1.0294526142437247e-05.\n",
      "[I 2025-11-20 03:37:33,636] Trial 6 finished with value: 1.02926254143419e-05 and parameters: {'lr': 1.02926254143419e-05}. Best is trial 6 with value: 1.02926254143419e-05.\n",
      "[I 2025-11-20 03:37:33,636] Trial 7 finished with value: 2.2477824229839567e-05 and parameters: {'lr': 2.2477824229839567e-05}. Best is trial 6 with value: 1.02926254143419e-05.\n",
      "[I 2025-11-20 03:37:33,637] Trial 8 finished with value: 0.0001948340692914176 and parameters: {'lr': 0.0001948340692914176}. Best is trial 6 with value: 1.02926254143419e-05.\n",
      "[I 2025-11-20 03:37:33,637] Trial 9 finished with value: 9.876560287135799e-05 and parameters: {'lr': 9.876560287135799e-05}. Best is trial 6 with value: 1.02926254143419e-05.\n",
      "2025-11-20 03:37:33,638 INFO Best learning rate for fold 4: 0.000010\n",
      "2025-11-20 03:37:33,727 INFO Starting SSL pretraining for fold 4 with train_bs=8, ssl_bs=16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4623c0efdbf04066a10bd1b556656a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 1:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1 Loss: 0.8571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da949cd77204f149be974e9a816b67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 2:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 2 Loss: 0.7143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becc7b2722074a608c46f03adfe0a826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 3:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 04:17:00,880 INFO Starting supervised training for fold 4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 3 Loss: 0.5702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39952b7609d54a5e8863319e24b5e04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3572, Val Loss: 1.6442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c85190b136b434dafcee71e9dceb803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2352, Val Loss: 1.6139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6192a2e97344bc8098ea463a0a385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1944, Val Loss: 1.6089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca196f91fddb46c8996f2a26e9b67292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1714, Val Loss: 1.5430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa8884b013c44a486032221b3004c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1564, Val Loss: 1.6183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca14e59e9204de2b6d057f26408a382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1454, Val Loss: 1.5469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03452e0b66424ef19067926ad58d51ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 7:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.1372, Val Loss: 1.5390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d87c5e325fd49c3bb91c30ddd3bd02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 8:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.1307, Val Loss: 1.5371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a8589c585d444184761639e42330b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 9:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.1255, Val Loss: 1.4400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1205d591e6634b21a076ee70d52159bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 10:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.1211, Val Loss: 1.6763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee9de5675e49ed87fb59f4407dbece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 11:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.1174, Val Loss: 1.4413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c772483da148388921d9fa893eb1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 12:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1142, Val Loss: 1.4727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f08b44487b04857810a3a4b0a792a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 13:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.1115, Val Loss: 1.3508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362cf9bf412740e0bdfa2cf10abd2232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 14:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.1091, Val Loss: 1.4924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e74478246c40a29c9b5460ded724e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 15:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.1070, Val Loss: 1.4519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497a834245124c4e95f584ce072ff4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 16:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.1052, Val Loss: 1.4772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2645ebec629c4b7a9e91692b77de38e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 17:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.1035, Val Loss: 1.4576\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05a97331a934ab8a0a25d14f308b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 18:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.1021, Val Loss: 1.7320\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='best_msms_hybrid_fold_4.pt' target='_blank'>best_msms_hybrid_fold_4.pt</a><br>"
      ],
      "text/plain": [
       "/home/onepaw/naturems-onepaw/best_msms_hybrid_fold_4.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 23:32:06,361 INFO \n",
      "Fold 5/5\n",
      "[I 2025-11-20 23:35:35,163] A new study created in memory with name: no-name-eac48373-4469-4e18-aba9-2856d7c877b9\n",
      "[I 2025-11-20 23:35:35,164] Trial 0 finished with value: 0.00013692890054654416 and parameters: {'lr': 0.00013692890054654416}. Best is trial 0 with value: 0.00013692890054654416.\n",
      "[I 2025-11-20 23:35:35,164] Trial 1 finished with value: 5.131721849901983e-05 and parameters: {'lr': 5.131721849901983e-05}. Best is trial 1 with value: 5.131721849901983e-05.\n",
      "[I 2025-11-20 23:35:35,164] Trial 2 finished with value: 1.0814978119940094e-05 and parameters: {'lr': 1.0814978119940094e-05}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,165] Trial 3 finished with value: 1.3455377067737503e-05 and parameters: {'lr': 1.3455377067737503e-05}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,165] Trial 4 finished with value: 9.961255485368533e-05 and parameters: {'lr': 9.961255485368533e-05}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,165] Trial 5 finished with value: 0.00015183155018963474 and parameters: {'lr': 0.00015183155018963474}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,166] Trial 6 finished with value: 0.0009867227525570696 and parameters: {'lr': 0.0009867227525570696}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,166] Trial 7 finished with value: 6.363668789288728e-05 and parameters: {'lr': 6.363668789288728e-05}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,166] Trial 8 finished with value: 0.0002304494973777336 and parameters: {'lr': 0.0002304494973777336}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "[I 2025-11-20 23:35:35,167] Trial 9 finished with value: 0.0004955486040876001 and parameters: {'lr': 0.0004955486040876001}. Best is trial 2 with value: 1.0814978119940094e-05.\n",
      "2025-11-20 23:35:35,167 INFO Best learning rate for fold 5: 0.000011\n",
      "2025-11-20 23:35:35,257 INFO Starting SSL pretraining for fold 5 with train_bs=8, ssl_bs=16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7534a3ca92d847a1ad6947ad286d4f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 1:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1 Loss: 0.8478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d9f60f63b54c4abdc69133ca8dad0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 2:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 2 Loss: 0.7081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1dca53ec0248e99e27463de4613b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SSL Epoch 3:   0%|          | 0/8438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 00:15:02,410 INFO Starting supervised training for fold 5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Epoch 3 Loss: 0.5749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459f34fe9cac42879dfe734bf2b4d90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3542, Val Loss: 2.6217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13449cde1fbb4b0db3e161a5416fe12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2327, Val Loss: 2.1373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb51fa45ed14c20b24f766f30e1640f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1923, Val Loss: 2.9632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e16f15cdcaa4cca8406d8f8d0aab90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1699, Val Loss: 3.6648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d2cc70360f4f7dbe7ef7bffb00a6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1550, Val Loss: 3.3032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3654e7e9b09f476b9443c2483f0cc381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1444, Val Loss: 3.5895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a0652475e4470583032416281d5690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 7:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.1362, Val Loss: 3.4068\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='best_msms_hybrid_fold_5.pt' target='_blank'>best_msms_hybrid_fold_5.pt</a><br>"
      ],
      "text/plain": [
       "/home/onepaw/naturems-onepaw/best_msms_hybrid_fold_5.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 07:44:25,403 INFO Cross-validation results: [1.8402759010932144, 1.6129989732794285, 1.5601578291889124, 1.3507649380442464, 2.137286630874464]\n",
      "2025-11-21 07:44:25,404 INFO Average validation loss: 1.7003\n",
      "2025-11-21 07:44:25,404 INFO Estimated training time: 1611.4 hours (67.1 days)\n",
      "2025-11-21 07:44:25,405 INFO With RTX 3080 Ti optimizations, expect 8-12 hours total.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90109c8d40954f2c9f910833849a14e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/56254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell 22\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Verify required variables\n",
    "required_vars = ['MSMSDataset', 'config', 'objective', 'ssl_pretrain', 'supervised_train', \n",
    "                'token_to_idx', 'idx_to_token', 'vocab_size', 'PRETRAIN_MAX_LEN', 'SUPERVISED_MAX_LEN', 'device']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        raise NameError(f\"{var} is not defined. Ensure it is defined in previous cells.\")\n",
    "\n",
    "# Verify datasets\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "if 'df_external' not in globals():\n",
    "    raise NameError(\"df_external is not defined. Please load the dataset.\")\n",
    "\n",
    "# Remove duplicate columns\n",
    "df_massspecgym = df_massspecgym.loc[:, ~df_massspecgym.columns.duplicated(keep='first')]\n",
    "df_external = df_external.loc[:, ~df_external.columns.duplicated(keep='first')]\n",
    "logging.info(f\"df_massspecgym columns after removing duplicates: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"df_external columns after removing duplicates: {df_external.columns.tolist()}\")\n",
    "\n",
    "# Verify 'smiles' column exists\n",
    "if 'smiles' not in df_massspecgym.columns:\n",
    "    raise ValueError(\"No 'smiles' column in df_massspecgym after removing duplicates.\")\n",
    "if 'smiles' not in df_external.columns:\n",
    "    raise ValueError(\"No 'smiles' column in df_external after removing duplicates.\")\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "# Debug/interactive safety\n",
    "FAST_DEBUG = globals().get('FAST_DEBUG', False)\n",
    "if FAST_DEBUG:\n",
    "    logging.info('FAST_DEBUG mode enabled: using small subsets, fewer epochs and trials, single-worker loaders')\n",
    "\n",
    "# Create external dataset\n",
    "external_dataset = MSMSDataset(df_external, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "logging.info(f\"External dataset size: {len(external_dataset)}\")\n",
    "external_loader = None  # Set to None as in original code; update if needed\n",
    "\n",
    "# Prefer torch_geometric DataLoader\n",
    "use_geo_loader = False\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "    use_geo_loader = True\n",
    "except Exception:\n",
    "    from torch.utils.data import DataLoader as TorchDataLoader\n",
    "    GeoDataLoader = None\n",
    "    logging.info(\"Using torch.utils.data.DataLoader as fallback.\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_massspecgym)):\n",
    "    logging.info(f\"\\nFold {fold+1}/5\")\n",
    "    train_data = df_massspecgym.iloc[train_idx].copy()\n",
    "    val_data = df_massspecgym.iloc[val_idx].copy()\n",
    "    ssl_data = train_data.sample(frac=0.3, random_state=42).copy()\n",
    "\n",
    "    # FAST_DEBUG sampling\n",
    "    if FAST_DEBUG:\n",
    "        train_data = train_data.sample(n=min(512, len(train_data)), random_state=42)\n",
    "        val_data = val_data.sample(n=min(128, len(val_data)), random_state=42)\n",
    "        ssl_data = ssl_data.sample(n=min(256, len(ssl_data)), random_state=42)\n",
    "        optuna_trials = 2\n",
    "        ssl_epochs = 1\n",
    "        supervised_epochs = 1\n",
    "    else:\n",
    "        optuna_trials = 10\n",
    "        ssl_epochs = config.SSL_EPOCHS\n",
    "        supervised_epochs = config.SUPERVISED_EPOCHS\n",
    "\n",
    "    # DataLoader parameters\n",
    "    workers = 0  # Single worker for Jupyter stability\n",
    "    pin_memory = False  # Disable for notebook safety\n",
    "\n",
    "    # Batch sizes\n",
    "    if torch.cuda.is_available():\n",
    "        train_bs = max(4, int(config.BATCH_SIZE // 8))\n",
    "        val_bs = max(4, int(config.BATCH_SIZE // 8))\n",
    "        ssl_bs = max(8, int(config.BATCH_SIZE // 4))\n",
    "    else:\n",
    "        train_bs = max(8, config.BATCH_SIZE)\n",
    "        val_bs = max(8, config.BATCH_SIZE)\n",
    "        ssl_bs = max(32, config.BATCH_SIZE)\n",
    "\n",
    "    # Build datasets and loaders\n",
    "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    ssl_dataset = MSMSDataset(ssl_data, max_len=PRETRAIN_MAX_LEN, is_ssl=True)\n",
    "\n",
    "    if use_geo_loader and GeoDataLoader is not None:\n",
    "        train_loader = GeoDataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "        val_loader = GeoDataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n",
    "        ssl_loader = GeoDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True)\n",
    "    else:\n",
    "        train_loader = TorchDataLoader(train_dataset, batch_size=train_bs, shuffle=True, num_workers=workers, pin_memory=pin_memory)\n",
    "        val_loader = TorchDataLoader(val_dataset, batch_size=val_bs, shuffle=False, num_workers=workers, pin_memory=pin_memory)\n",
    "        ssl_loader = TorchDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True, num_workers=workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_data, val_data), n_trials=optuna_trials)\n",
    "    best_lr = study.best_params.get('lr', config.LEARNING_RATE)\n",
    "    logging.info(f\"Best learning rate for fold {fold+1}: {best_lr:.6f}\")\n",
    "\n",
    "    # Initialize and train model with multiple OOM retries\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=config.D_MODEL, nhead=config.NHEAD, num_layers=config.NUM_LAYERS).to(device)\n",
    "            logging.info(f\"Starting SSL pretraining for fold {fold+1} with train_bs={train_bs}, ssl_bs={ssl_bs}...\")\n",
    "            ssl_pretrain(model, ssl_loader, epochs=ssl_epochs, lr=best_lr)\n",
    "            logging.info(f\"Starting supervised training for fold {fold+1}...\")\n",
    "            best_val_loss = supervised_train(model, train_loader, val_loader, epochs=supervised_epochs, lr=best_lr, patience=config.PATIENCE)\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower() and retry_count < max_retries - 1 and torch.cuda.is_available():\n",
    "                logging.warning(f\"CUDA OOM detected (retry {retry_count+1}/{max_retries}). Reducing batch sizes...\")\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                train_bs = max(2, train_bs // 2)\n",
    "                val_bs = max(2, val_bs // 2)\n",
    "                ssl_bs = max(4, ssl_bs // 2)\n",
    "                if use_geo_loader and GeoDataLoader is not None:\n",
    "                    train_loader = GeoDataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "                    val_loader = GeoDataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n",
    "                    ssl_loader = GeoDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True)\n",
    "                else:\n",
    "                    train_loader = TorchDataLoader(train_dataset, batch_size=train_bs, shuffle=True, num_workers=0, pin_memory=False)\n",
    "                    val_loader = TorchDataLoader(val_dataset, batch_size=val_bs, shuffle=False, num_workers=0, pin_memory=False)\n",
    "                    ssl_loader = TorchDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True, num_workers=0, pin_memory=False)\n",
    "                logging.info(f\"New batch sizes -> train: {train_bs}, val: {val_bs}, ssl: {ssl_bs}. Retrying fold...\")\n",
    "                retry_count += 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    fold_results.append(best_val_loss)\n",
    "    model_path = f'best_msms_hybrid_fold_{fold+1}.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, model_path)\n",
    "    \n",
    "    try:\n",
    "        display(FileLink(model_path))\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to create FileLink for {model_path}: {e}\")\n",
    "\n",
    "logging.info(f\"Cross-validation results: {fold_results}\")\n",
    "logging.info(f\"Average validation loss: {np.mean(fold_results):.4f}\")\n",
    "\n",
    "# Training time estimation\n",
    "total_samples = len(df_massspecgym)\n",
    "samples_per_epoch = total_samples // max(1, train_bs)\n",
    "total_epochs = config.N_FOLDS * (ssl_epochs + supervised_epochs)\n",
    "estimated_hours = (samples_per_epoch * total_epochs * 0.5) / 3600\n",
    "logging.info(f\"Estimated training time: {estimated_hours:.1f} hours ({estimated_hours/24:.1f} days)\")\n",
    "logging.info(\"With RTX 3080 Ti optimizations, expect 8-12 hours total.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#cell 23\n",
    "# Load the best trained model\n",
    "model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=config.D_MODEL, nhead=config.NHEAD, num_layers=config.NUM_LAYERS).to(device)\n",
    "checkpoint = torch.load('best_msms_hybrid_fold_1.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Model loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d63385c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'< SOS >'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m sample_adduct_idx \u001b[38;5;241m=\u001b[39m external_dataset[sample_idx][\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m     16\u001b[0m true_smiles \u001b[38;5;241m=\u001b[39m external_dataset[sample_idx][\u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m predicted_results \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_spectrum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ion_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_precursor_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_adduct_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_smiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSUPERVISED_MAX_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m pred_smiles_list\u001b[38;5;241m.\u001b[39mextend([smiles \u001b[38;5;28;01mfor\u001b[39;00m smiles, _ \u001b[38;5;129;01min\u001b[39;00m predicted_results])\n\u001b[1;32m     20\u001b[0m true_smiles_list\u001b[38;5;241m.\u001b[39mextend([true_smiles] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(predicted_results))\n",
      "Cell \u001b[0;32mIn[20], line 131\u001b[0m, in \u001b[0;36mbeam_search\u001b[0;34m(model, spectrum, graph_data, ion_mode, precursor_bin, adduct_idx, true_smiles, beam_width, max_len, device)\u001b[0m\n\u001b[1;32m    128\u001b[0m graph_batch \u001b[38;5;241m=\u001b[39m make_graph_batch(graph_data, device)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Start with SOS token\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [[\u001b[43mtoken_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOS_TOKEN\u001b[49m\u001b[43m]\u001b[49m]]\n\u001b[1;32m    132\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n",
      "\u001b[0;31mKeyError\u001b[0m: '< SOS >'"
     ]
    }
   ],
   "source": [
    "#cell 24\n",
    "# External dataset evaluation and visualization\n",
    "model.eval()\n",
    "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
    "pred_smiles_list = []\n",
    "true_smiles_list = []\n",
    "adducts_list = []\n",
    "num_samples = min(5, len(external_dataset))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample_spectrum = external_dataset[sample_idx][0]\n",
    "    sample_graph = external_dataset[sample_idx][1]\n",
    "    sample_ion_mode = external_dataset[sample_idx][3]\n",
    "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
    "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
    "    true_smiles = external_dataset[sample_idx][6]\n",
    "\n",
    "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
    "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
    "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
    "\n",
    "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
    "    print(\"Top Predicted SMILES:\")\n",
    "    for smiles, confidence in predicted_results[:3]:\n",
    "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
    "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
    "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
    "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
    "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
    "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
    "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
    "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "        if smiles != \"Invalid SMILES\":\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
    "\n",
    "    # Visualize molecules\n",
    "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
    "        if pred_mol and true_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            img_array = np.array(img.convert('RGB'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(img_array)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Visualize attention and GNN weights for first sample\n",
    "    if sample_idx == 0:\n",
    "        with torch.no_grad():\n",
    "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
    "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
    "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
    "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
    "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
    "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
    "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
    "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
    "print(\"External Metrics Summary:\")\n",
    "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
    "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
    "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
    "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
    "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a97e84",
   "metadata": {},
   "source": [
    "backup code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3aa58-2334-49ca-8465-c29868f452b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helpers merged from notebook-for-pc.ipynb - added without removing any existing features\n",
    "# 1) SMILES/SELFIES validators and plausibility checks\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdFMCS\n",
    "from rdkit import DataStructs\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Define a conservative set of valid atom symbols used for basic syntactic checks\n",
    "valid_atoms = set([\n",
    "    'H','B','C','N','O','F','P','S','Cl','Br','I',\n",
    "    'c','n','o','s','p'  # aromatic/lowercase tokens sometimes used in SMILES-like checks\n",
    "])\n",
    "\n",
    "def is_valid_smiles_syntax(smiles):\n",
    "    \"Basic SMILES syntax validator: bracket and paren matching + simple token checks.\"\n",
    "    if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return False\n",
    "    stack = []\n",
    "    for c in smiles:\n",
    "        if c in '([':\n",
    "            stack.append(c)\n",
    "        elif c == ')':\n",
    "            if not stack or stack[-1] != '(':\n",
    "                return False\n",
    "            stack.pop()\n",
    "        elif c == ']':\n",
    "            if not stack or stack[-1] != '[':\n",
    "                return False\n",
    "            stack.pop()\n",
    "    if stack:\n",
    "        return False\n",
    "    i = 0\n",
    "    while i < len(smiles):\n",
    "        if smiles[i] == '[':\n",
    "            j = smiles.find(']', i)\n",
    "            if j == -1:\n",
    "                return False\n",
    "            atom = smiles[i+1:j]\n",
    "            # simple check: at least one valid atom symbol is present\n",
    "            if not any(a in atom for a in valid_atoms):\n",
    "                return False\n",
    "            i = j + 1\n",
    "        else:\n",
    "            if smiles[i] in valid_atoms or smiles[i] in '()=#/\\\\@.:+-0123456789%[]':\n",
    "                i += 1\n",
    "            else:\n",
    "                # allow SELFIES tokens and other chars; fallback to RDKit for final check\n",
    "                i += 1\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        return mol is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_plausible_molecule(smiles, true_mol=None, max_mw=1500, min_logp=-7, max_logp=7):\n",
    "    \"Basic RDKit plausibility check used for filtering beam search outputs.\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if not mol or not is_valid_smiles_syntax(smiles):\n",
    "            return False\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        true_mw = Descriptors.MolWt(true_mol) if true_mol else None\n",
    "        if mw > max_mw:\n",
    "            return False\n",
    "        if not (min_logp <= logp <= max_logp):\n",
    "            return False\n",
    "        if true_mw is not None and abs(mw - true_mw) > 300:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.debug(f'is_plausible_molecule error: {e}')\n",
    "        return False\n",
    "\n",
    "# 2) Safe adduct mapping helper (builds mapping if missing and fills NaNs)\n",
    "def ensure_adduct_mapping(df_massspecgym, df_external=None):\n",
    "    \"Builds adduct_types and adduct_to_idx if not present, fills NaNs in adduct_idx.\"\n",
    "    global adduct_types, adduct_to_idx\n",
    "    try:\n",
    "        if 'adduct_to_idx' in globals() and 'adduct_types' in globals():\n",
    "            return adduct_to_idx\n",
    "    except Exception:\n",
    "        pass\n",
    "    adduct_types = df_massspecgym['adduct'].dropna().unique().tolist()\n",
    "    adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "    df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx).fillna(0).astype(int)\n",
    "    if df_external is not None:\n",
    "        df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx).fillna(0).astype(int)\n",
    "    return adduct_to_idx\n",
    "\n",
    "# attempt to ensure mapping if dataframes exist in notebook globals\n",
    "try:\n",
    "    if 'df_massspecgym' in globals():\n",
    "        ensure_adduct_mapping(df_massspecgym, df_external if 'df_external' in globals() else None)\n",
    "except Exception as e:\n",
    "    logging.debug(f'ensure_adduct_mapping failed: {e}')\n",
    "\n",
    "# 3) Enhanced beam search variant (keeps original beam_search intact)\n",
    "def beam_search_enhanced(model, spectrum, graph_data, ion_mode_idx, precursor_idx, adduct_idx, true_smiles=None, beam_width=8, max_len=150, nucleus_p=0.9, device='cpu'):\n",
    "    \"Beam-search that applies SMILES/SELFIES syntax checks, stereochemistry boosting, and plausibility filters.\"\n",
    "    model.eval()\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles) if true_smiles else None\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device) if isinstance(spectrum, torch.Tensor) else torch.tensor(spectrum, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        try:\n",
    "            graph_batch = Batch.from_data_list([graph_data]).to(device)\n",
    "        except Exception:\n",
    "            graph_batch = graph_data if hasattr(graph_data, 'batch') else graph_data\n",
    "        ion_mode_idx = torch.tensor([int(ion_mode_idx)], dtype=torch.long).to(device)\n",
    "        precursor_idx = torch.tensor([int(precursor_idx)], dtype=torch.long).to(device)\n",
    "        adduct_idx = torch.tensor([int(adduct_idx)], dtype=torch.long).to(device)\n",
    "        sequences = [([token_to_idx.get(config.SOS_TOKEN, token_to_idx.get(PAD_TOKEN, 0))], 0.0)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == token_to_idx.get(config.EOS_TOKEN, token_to_idx.get(EOS_TOKEN, 0)):\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                tgt_input = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "                # generate mask if model provides helper, otherwise create causal mask\n",
    "                tgt_mask = None\n",
    "                try:\n",
    "                    outputs = model.decoder(tgt_input, model.combine_layer(torch.cat([model.transformer_encoder(spectrum), model.gnn_encoder(graph_batch)], dim=-1)).unsqueeze(1), None)\n",
    "                    logits = outputs[0, -1]\n",
    "                except Exception:\n",
    "                    # Fallback to calling model forward if decoder signature differs\n",
    "                    outputs = model(spectrum, graph_batch, tgt_input)\n",
    "                    logits = outputs[0][0, -1] if isinstance(outputs, tuple) else outputs[0, -1]\n",
    "                log_probs = F.log_softmax(logits, dim=-1).cpu().numpy()\n",
    "                # boost stereochemistry tokens if present\n",
    "                for tok in ['@','/','\\\\']:\n",
    "                    if tok in token_to_idx:\n",
    "                        log_probs[token_to_idx[tok]] += 0.25\n",
    "                topk = np.argsort(log_probs)[-min(len(log_probs), beam_width*4):][::-1]\n",
    "                for tok in topk[:beam_width]:\n",
    "                    new_seq = seq + [int(tok)]\n",
    "                    new_score = score + float(log_probs[tok])\n",
    "                    # quick syntax check on partial reconstruction\n",
    "                    partial = ''.join([idx_to_token.get(i, '') for i in new_seq[1:]])\n",
    "                    if not is_valid_smiles_syntax(partial):\n",
    "                        # allow but penalize\n",
    "                        new_score -= 1.0\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if all(seq[-1] == token_to_idx.get(config.EOS_TOKEN, token_to_idx.get(EOS_TOKEN, 0)) for seq, _ in sequences):\n",
    "                break\n",
    "\n",
    "        results = []\n",
    "        for seq, score in sequences:\n",
    "            toks = [idx_to_token.get(i, '') for i in seq[1:]]\n",
    "            cand_str = ''.join([t for t in toks if t not in {config.PAD_TOKEN, config.SOS_TOKEN, config.EOS_TOKEN}])\n",
    "            # attempt to decode either as SELFIES or SMILES depending on available decoders\n",
    "            smiles = None\n",
    "            try:\n",
    "                # prefer SELFIES decode if token set is SELFIES-like\n",
    "                if 'sf' in globals():\n",
    "                    s = ''.join(toks)\n",
    "                    smiles = sf.decoder(s) if s else ''\n",
    "            except Exception:\n",
    "                smiles = None\n",
    "            if not smiles:\n",
    "                smiles = cand_str\n",
    "            if smiles and is_plausible_molecule(smiles, true_mol):\n",
    "                results.append((smiles, math.exp(score / max(1, len(seq)))))\n",
    "        return results if results else [(, 0.0)]\n",
    "\n",
    "# Small runtime check to show new helpers are present\n",
    "print('Merged helpers: is_valid_smiles_syntax, is_plausible_molecule, ensure_adduct_mapping, beam_search_enhanced available')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (naturems)",
   "language": "python",
   "name": "naturems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
