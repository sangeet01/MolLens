{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd0Hc_ItxPrz"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio rdkit datasets tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#final_version\n",
    "# stereochemistry_fixed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import optuna\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance\n",
    "from pandarallel import pandarallel #Added by Pawan\n",
    "import time  #Added by Pawan\n",
    "import os #Added by Pawan\n",
    "import glob #Added by Pawan\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define token variables early\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "# - Uses Hugging Face's `load_dataset` function (from `datasets` library) to fetch the \"MassSpecGym\" dataset \n",
    "#   uploaded by user 'roman-bushuiev' on the Hugging Face Hub.\n",
    "# - The `split='val'` argument tells Hugging Face to load only the validation split, \n",
    "#   which is predefined by the dataset creator and stored in the dataset's metadata/split files.\n",
    "# - The split boundaries (which examples belong to 'val') are not determined here; \n",
    "#   they come from the dataset repository's configuration on the Hugging Face Hub.\n",
    "# - The returned object is a `Dataset` object, which we then convert to a pandas DataFrame for easier manipulation.\n",
    "dataset = load_dataset('roman-bushuiev/MassSpecGym', split='val')\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an external dataset (e.g., NIST-like) by doing a manual 90/10 positional split\n",
    "# NOTE: The original DataFrame (loaded from Hugging Face \"val\" split) contains a `fold` column\n",
    "#       with the dataset creator‚Äôs own 'train', 'val', and 'test' labels, but these are NOT used here.\n",
    "#       This manual split ignores those fold labels and simply splits by row position:\n",
    "#         - df_massspecgym: first 90% of rows (position 0 up to position 0.9 * total number of rows)\n",
    "#         - df_external: last 10% of rows (position 0.9 * total number of rows to the end)\n",
    "#       As a result, both subsets may contain a mix of original fold labels.\n",
    "df_massspecgym, df_external = df.iloc[:int(0.9*len(df))], df.iloc[int(0.9*len(df)):]\n",
    "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
    "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
    "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalize SMILES and augment\n",
    "# Takes a SMILES string as input and attempts to return a canonicalized version.\n",
    "# \n",
    "# Purpose:\n",
    "# --------\n",
    "# - Standardize SMILES representations so that different notations for the same molecule\n",
    "#   become identical (e.g., \"OC\" and \"CO\" both ‚Üí \"CO\").\n",
    "# - Remove invalid SMILES from the dataset by returning None when they cannot be parsed.\n",
    "#\n",
    "# Steps:\n",
    "# ------\n",
    "# 1. Parse SMILES into an RDKit Mol object:\n",
    "#    mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "#    - `Chem.MolFromSmiles` converts the SMILES string into RDKit's internal molecule object.\n",
    "#    - `sanitize=True` means RDKit will run chemical sanitization:\n",
    "#         * Assign aromaticity\n",
    "#         * Set atom hybridizations\n",
    "#         * Verify valences\n",
    "#         * Detect and store stereochemistry\n",
    "#      If the SMILES is chemically invalid or violates valence/aromaticity rules, \n",
    "#      this will fail (return None or raise an exception).\n",
    "#\n",
    "# 2. Check if parsing succeeded:\n",
    "#    if mol:\n",
    "#    - In Python, an RDKit Mol object is truthy, while None is falsy.\n",
    "#    - If mol is None (invalid SMILES), skip canonicalization and return None.\n",
    "#\n",
    "# 3. Convert Mol object back to SMILES in canonical form:\n",
    "#    return Chem.MolToSmiles(mol, canonical=True)\n",
    "#    - `Chem.MolToSmiles` generates a SMILES string from the Mol object.\n",
    "#    - `canonical=True` ensures that RDKit outputs a unique, standardized SMILES\n",
    "#      for each molecule regardless of atom order in the original input.\n",
    "#      This is the step that enables deduplication based on chemical structure.\n",
    "#\n",
    "# 4. Handle failures:\n",
    "#    - If any error occurs in parsing or canonicalization (caught by `except:`),\n",
    "#      return None to mark the SMILES as invalid.\n",
    "#\n",
    "# Return values:\n",
    "# --------------\n",
    "# - Valid SMILES ‚Üí canonicalized SMILES string.\n",
    "# - Invalid SMILES or errors ‚Üí None.\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Data augmentation: SMILES enumeration and spectral noise\n",
    "# This function attempts to augment a SMILES string by enumerating all possible stereoisomers\n",
    "# and returning randomized canonical SMILES for each.\n",
    "# - Step 1: Convert SMILES string into an RDKit Mol object using Chem.MolFromSmiles().\n",
    "#           * If the SMILES is valid ‚Üí returns a Mol object (truthy in Python).\n",
    "#           * If the SMILES is invalid (e.g., \"C1CC\" ‚Äî ring not closed properly) ‚Üí returns None (falsy).\n",
    "# - Step 2: If `mol` is valid (not None), enumerate stereoisomers with RDKit's EnumerateStereoisomers function\n",
    "#           and convert each stereoisomer back to SMILES (canonical form, randomized atom order).\n",
    "# - Step 3: If `mol` is None (invalid SMILES), skip augmentation and just return the original SMILES in a list.\n",
    "# - Step 4: If any exception occurs anywhere in the try block (e.g., during stereoisomer enumeration),\n",
    "#           the except block will also return the original SMILES unchanged.\n",
    "# In short:\n",
    "#   * Valid SMILES ‚Üí converted to Mol ‚Üí augmented list of SMILES.\n",
    "#   * Invalid SMILES ‚Üí returned unchanged.\n",
    "#   * Any error ‚Üí returned unchanged.\n",
    "def augment_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol)\n",
    "            return [Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers]\n",
    "        return [smiles]\n",
    "    except:\n",
    "        return [smiles]\n",
    "\n",
    "\n",
    "# Spectrum ‚Üí graph featurization with simple binning + chain edges\n",
    "# This function converts a (m/z, intensity) spectrum into:\n",
    "#   1) A fixed-length binned vector (length = n_bins) normalized to [0, 1]\n",
    "#   2) A PyTorch Geometric graph (Data) whose nodes are bins and edges connect adjacent bins\n",
    "#\n",
    "# Inputs:\n",
    "#   - mzs, intensities: iterables of equal length containing peak positions (m/z) and their intensities\n",
    "#   - ion_mode: numeric encoding of polarity (e.g., +1 for positive, -1 for negative); passed as a scalar feature\n",
    "#   - precursor_mz: the precursor mass/charge; passed as a scalar feature\n",
    "#   - adduct: string adduct label (e.g., \"[M+H]+\"); mapped to an index via `adduct_to_idx`\n",
    "#   - n_bins: number of equal-width bins spanning [0, max_mz)\n",
    "#   - max_mz: upper m/z bound for binning; peaks with m/z >= max_mz are ignored\n",
    "#   - noise_level: std-dev of zero-mean Gaussian noise added to the spectrum AFTER normalization\n",
    "#\n",
    "# Processing overview:\n",
    "#   1) Initialize zero vector `spectrum` of length n_bins\n",
    "#   2) For each (mz, intensity):\n",
    "#        * cast to float (skip if invalid)\n",
    "#        * if mz < max_mz, compute bin index as floor((mz / max_mz) * n_bins) and accumulate intensity\n",
    "#   3) Normalize `spectrum` by its max value (if any nonzero content exists)\n",
    "#   4) Add Gaussian noise ~ N(0, noise_level) per bin; the noise itself is clipped to [0, 1] before addition\n",
    "#        NOTE: This clips the NOISE array, not the final spectrum. If you want the final spectrum in [0,1],\n",
    "#              you may also clip `spectrum` afterward (not done here to preserve original behavior).\n",
    "#   5) Build PyG node features `x`: shape [n_bins, 1], one scalar per bin (the binned intensity)\n",
    "#   6) Build `edge_index` as a bidirectional chain: 0‚Üî1‚Üî2‚Üî...‚Üî(n_bins-1)\n",
    "#   7) Package auxiliary scalar features (ion_mode, precursor_mz) as 1D tensors and look up adduct index\n",
    "#\n",
    "# Returns:\n",
    "#   - spectrum: the final NumPy array of length n_bins (after normalization + noise)\n",
    "#   - Data(...): PyG graph with:\n",
    "#        * x: [n_bins, 1] float tensor of node features\n",
    "#        * edge_index: [2, 2*(n_bins-1)] long tensor of undirected chain edges\n",
    "#        * ion_mode: [1] float tensor (aux scalar)\n",
    "#        * precursor_mz: [1] float tensor (aux scalar)\n",
    "#        * adduct_idx: Python int (category id from `adduct_to_idx`)\n",
    "#\n",
    "# Prereqs in your environment:\n",
    "#   - `from torch_geometric.data import Data`\n",
    "#   - a dict `adduct_to_idx` mapping adduct strings to integer indices\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct,\n",
    "                          n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    # 1) Initialize empty spectrum\n",
    "    spectrum = np.zeros(n_bins)\n",
    "\n",
    "    # 2) Bin peaks by m/z (accumulating intensities into bins)\n",
    "    for mz, intensity in zip(mzs, intensities):\n",
    "        try:\n",
    "            mz = float(mz)\n",
    "            intensity = float(intensity)\n",
    "            if mz < max_mz:\n",
    "                # Map m/z ‚àà [0, max_mz) to integer bin ‚àà [0, n_bins)\n",
    "                bin_idx = int((mz / max_mz) * n_bins)\n",
    "                spectrum[bin_idx] += intensity\n",
    "        except (ValueError, TypeError):\n",
    "            # Skip malformed values\n",
    "            continue\n",
    "\n",
    "    # 3) Normalize to [0, 1] by the maximum nonzero intensity (if any)\n",
    "    if spectrum.max() > 0:\n",
    "        spectrum = spectrum / spectrum.max()\n",
    "\n",
    "    # 4) Add clipped Gaussian noise to the spectrum\n",
    "    #    NOTE: np.random.normal(...).clip(0, 1) clips the NOISE itself to [0, 1] before adding.\n",
    "    #          This preserves shape but allows upward-only perturbations when noise is positive.\n",
    "    spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "\n",
    "    # 5) Node features: one scalar per bin ‚Üí shape [n_bins, 1]\n",
    "    x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "    # 6) Chain graph connectivity: bidirectional edges between adjacent bins\n",
    "    edge_index = []\n",
    "    for i in range(n_bins - 1):\n",
    "        edge_index.append([i, i + 1])\n",
    "        edge_index.append([i + 1, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()  # shape [2, 2*(n_bins-1)]\n",
    "\n",
    "    # 7) Auxiliary scalar features + adduct category index\n",
    "    ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "    precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "    adduct_idx = adduct_to_idx.get(adduct, 0)  # default to 0 if unseen adduct\n",
    "\n",
    "    return spectrum, Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        ion_mode=ion_mode,\n",
    "        precursor_mz=precursor_mz,\n",
    "        adduct_idx=adduct_idx\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=True) #Added by Pawan\n",
    "start_time = time.time()\n",
    "\n",
    "df_massspecgym['smiles'] = df_massspecgym['smiles'].parallel_apply(canonicalize_smiles) # Added by Pawan\n",
    "df_external['smiles'] = df_external['smiles'].parallel_apply(canonicalize_smiles) #Added by Pawan\n",
    "df_massspecgym = df_massspecgym.dropna(subset=['smiles']) \n",
    "df_external = df_external.dropna(subset=['smiles'])\n",
    "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].parallel_apply(augment_smiles)\n",
    "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list'])\n",
    "df_massspecgym = df_massspecgym.drop(columns=['smiles']) # Drop original 'smiles' to prevent duplicates; added by Pawan\n",
    "df_massspecgym = df_massspecgym.rename(columns={'smiles_list': 'smiles'}) # Rename exploded list column to 'smiles'; added by Pawan\n",
    "df_massspecgym.to_parquet(\"df_massspecgym.parquet\")\n",
    "df_external.to_parquet(\"df_external.parquet\")\n",
    "print(\"Completed in {:.2f} seconds\".format(time.time() - start_time)) #Added by Pawan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_massspecgym.shape) #Added by Pawan\n",
    "print(df_massspecgym.index.nunique()) #Added by Pawan\n",
    "print(df_external.shape) #Added by Pawan\n",
    "print(df_external.index.nunique()) #Added by Pawan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_massspecgym.reset_index(drop=True, inplace=True) #Added by Pawan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_massspecgym.shape) #Added by Pawan\n",
    "print(df_massspecgym.index.nunique()) #Added by Pawan\n",
    "print(df_external.shape) #Added by Pawan\n",
    "print(df_external.index.nunique()) #Added by Pawan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_massspecgym.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess ion mode, precursor m/z, and adducts\n",
    "#Setup and Load\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import pyarrow as pa\n",
    "\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=True)\n",
    "\n",
    "# Load datasets\n",
    "df_massspecgym = pd.read_parquet(\"df_massspecgym.parquet\")\n",
    "df_external = pd.read_parquet(\"df_external.parquet\")\n",
    "\n",
    "# Build adduct mapping from df_massspecgym\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"processed_chunks\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess ion mode, precursor m/z, and adducts\n",
    "#Define Processing Function\n",
    "def preprocess_chunk(df_chunk, chunk_idx):\n",
    "    start_time = time.time()\n",
    "\n",
    "    df_chunk['ion_mode'] = df_chunk['adduct'].parallel_apply(\n",
    "        lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0\n",
    "    ).fillna(0)\n",
    "\n",
    "    df_chunk['precursor_bin'] = pd.qcut(\n",
    "        df_chunk['precursor_mz'], q=100, labels=False, duplicates='drop'\n",
    "    )\n",
    "\n",
    "    df_chunk['adduct_idx'] = df_chunk['adduct'].map(adduct_to_idx)\n",
    "\n",
    "    df_chunk[['binned', 'graph_data']] = df_chunk.parallel_apply(\n",
    "        lambda row: pd.Series(bin_spectrum_to_graph(\n",
    "            row['mzs'], row['intensities'], row['ion_mode'],\n",
    "            row['precursor_mz'], row['adduct']\n",
    "        )),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop graph_data column before saving to avoid pyarrow error\n",
    "    df_chunk.drop(columns=['graph_data'], inplace=True)\n",
    "\n",
    "    df_chunk.to_parquet(f\"processed_chunks/df_massspecgym_chunk_{chunk_idx:03}.parquet\")\n",
    "    print(f\"‚úÖ Saved chunk {chunk_idx} | Rows: {len(df_chunk)} | Time: {time.time() - start_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess ion mode, precursor m/z, and adducts\n",
    "#Chunk df_massspecgym\n",
    "chunk_size = 100_000\n",
    "n_chunks = (len(df_massspecgym) + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    output_file = f\"processed_chunks/df_massspecgym_chunk_{i:03}.parquet\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"‚è© Skipping chunk {i} (already exists)\")\n",
    "        continue\n",
    "\n",
    "    df_chunk = df_massspecgym.iloc[i * chunk_size : (i + 1) * chunk_size].copy()\n",
    "    preprocess_chunk(df_chunk, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Preprocess ion mode, precursor m/z, and adducts\n",
    "# ‚úÖ Process each chunk efficiently ‚Üí write to SSD ‚Üí move to external HDD\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pandarallel.initialize(nb_workers=8, progress_bar=False)\n",
    "\n",
    "# ‚úÖ External HDD target directory (Seagate)\n",
    "external_dir = \"/media/onepaw/seagate_manual/graph_data_chunks\"\n",
    "os.makedirs(external_dir, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Temporary SSD write directory\n",
    "temp_dir = \"graph_data_tmp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Load adduct mapping\n",
    "df_massspecgym = pd.read_parquet(\"df_massspecgym.parquet\", columns=[\"adduct\"])\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "del df_massspecgym\n",
    "\n",
    "# ‚úÖ Process each chunk from SSD\n",
    "chunk_files = sorted(glob.glob(\"processed_chunks/df_massspecgym_chunk_*.parquet\"))\n",
    "\n",
    "for i, chunk_file in enumerate(tqdm(chunk_files, desc=\"Processing chunks\")):\n",
    "    df = pd.read_parquet(chunk_file)\n",
    "\n",
    "    graph_data = df.parallel_apply(\n",
    "        lambda row: bin_spectrum_to_graph(\n",
    "            row['mzs'], row['intensities'], row['ion_mode'],\n",
    "            row['precursor_mz'], row['adduct']\n",
    "        )[1],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Save to SSD first (fast write)\n",
    "    temp_path = os.path.join(temp_dir, f\"graph_data_chunk_{i:03}.pkl\")\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        pickle.dump(graph_data.tolist(), f)\n",
    "\n",
    "    # ‚úÖ Then move to external HDD to free SSD space\n",
    "    final_path = os.path.join(external_dir, f\"graph_data_chunk_{i:03}.pkl\")\n",
    "    shutil.move(temp_path, final_path)\n",
    "\n",
    "    del df\n",
    "    del graph_data\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Processed and moved: graph_data_chunk_{i:03}.pkl\")\n",
    "\n",
    "print(\"üéâ All chunks processed and saved to external drive.\")\n",
    "print(\"üïí Completed in {:.2f} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream-load each chunk and write them one by one to HDD to avoid RAM issues\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚úÖ Directory where pickled graph_data chunks are stored (on external HDD)\n",
    "external_dir = \"/media/onepaw/seagate_manual/graph_data_chunks\"\n",
    "chunk_files = sorted(glob.glob(os.path.join(external_dir, \"graph_data_chunk_*.pkl\")))\n",
    "\n",
    "# ‚úÖ Output path on external HDD\n",
    "merged_path = \"/media/onepaw/seagate_manual/df_massspecgym_graph_data_streamed.pkl\"\n",
    "\n",
    "# ‚úÖ Open final output in append-binary mode\n",
    "with open(merged_path, \"wb\") as out_f:\n",
    "    for chunk_file in tqdm(chunk_files, desc=\"Merging (streamed)\"):\n",
    "        with open(chunk_file, \"rb\") as in_f:\n",
    "            data = pickle.load(in_f)\n",
    "            # Stream-write this chunk to output\n",
    "            pickle.dump(data, out_f)\n",
    "            del data  # ensure chunk gets garbage collected\n",
    "\n",
    "print(f\"‚úÖ Streamed merge completed to: {merged_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess ion mode, precursor m/z, and adducts\n",
    "#Full Processing of df_external\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute ion_mode\n",
    "df_external['ion_mode'] = df_external['adduct'].parallel_apply(\n",
    "    lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0\n",
    ").fillna(0)\n",
    "\n",
    "# Compute precursor_bin\n",
    "df_external['precursor_bin'] = pd.qcut(\n",
    "    df_external['precursor_mz'], q=100, labels=False, duplicates='drop'\n",
    ")\n",
    "\n",
    "# Map adduct to index\n",
    "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
    "\n",
    "# Generate binned and graph_data columns\n",
    "df_external[['binned', 'graph_data']] = df_external.parallel_apply(\n",
    "    lambda row: pd.Series(bin_spectrum_to_graph(\n",
    "        row['mzs'], row['intensities'], row['ion_mode'],\n",
    "        row['precursor_mz'], row['adduct']\n",
    "    )),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# üîí Save graph_data separately (optional)\n",
    "with open(\"df_external_graph_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_external['graph_data'].tolist(), f)\n",
    "\n",
    "# ‚ùå Drop graph_data column before saving to parquet\n",
    "df_external.drop(columns=['graph_data'], inplace=True)\n",
    "\n",
    "# ‚úÖ Save remaining data to Parquet\n",
    "df_external.to_parquet(\"df_external_processed.parquet\")\n",
    "\n",
    "print(\"‚úÖ df_external processed and saved in {:.2f} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preprocess ion mode, precursor m/z, and adducts\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "\n",
    "output_path = \"df_massspecgym_processed_full.parquet\"\n",
    "chunk_files = sorted(glob.glob(\"processed_chunks/df_massspecgym_chunk_*.parquet\"))\n",
    "\n",
    "first_df = pd.read_parquet(chunk_files[0])\n",
    "first_df['precursor_bin'] = first_df['precursor_bin'].fillna(-1).astype('int64')\n",
    "\n",
    "table = pa.Table.from_pandas(first_df)\n",
    "writer = pq.ParquetWriter(output_path, table.schema)\n",
    "\n",
    "writer.write_table(table)\n",
    "print(f\"‚úÖ Wrote chunk 0 / {len(chunk_files)}\")\n",
    "\n",
    "for i, file in enumerate(chunk_files[1:], start=1):\n",
    "    start = time.time()\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    df['precursor_bin'] = df['precursor_bin'].fillna(-1).astype('int64')\n",
    "    df = df[first_df.columns]\n",
    "\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    writer.write_table(table)\n",
    "    print(f\"‚úÖ Wrote chunk {i} / {len(chunk_files)} in {time.time() - start:.2f}s\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"üéâ Done merging {len(chunk_files)} chunks ‚ûú {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES Tokenization with Stereochemistry (Final Corrected Version)\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "\n",
    "# ‚úÖ FIX: Define a proper RegEx tokenizer for SMILES that handles multi-character elements\n",
    "SMILES_TOKENIZER_PATTERN =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "smiles_regex = re.compile(SMILES_TOKENIZER_PATTERN)\n",
    "\n",
    "def smiles_tokenizer(smiles):\n",
    "    \"\"\"Tokenize a SMILES string using the regular expression.\"\"\"\n",
    "    return [token for token in smiles_regex.findall(smiles)]\n",
    "\n",
    "# Open the large Parquet file\n",
    "parquet_file = pq.ParquetFile(\"df_massspecgym_processed_full.parquet\")\n",
    "\n",
    "start = time.time()\n",
    "# Step 1: Build vocabulary from all SMILES *tokens*\n",
    "all_tokens = set()\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i, columns=[\"smiles\"])\n",
    "    df = table.to_pandas()\n",
    "    for smiles in df['smiles'].dropna().astype(str):\n",
    "        all_tokens.update(smiles_tokenizer(smiles))\n",
    "\n",
    "# ‚úÖ FIX: Create the vocabulary from the tokenized list\n",
    "special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, MASK_TOKEN]\n",
    "tokens = special_tokens + sorted(list(all_tokens))\n",
    "\n",
    "# This now creates a correct, contiguous mapping from 0 to vocab_size-1\n",
    "token_to_idx = {tok: i for i, tok in enumerate(tokens)}\n",
    "idx_to_token = {i: tok for tok, i in token_to_idx.items()}\n",
    "vocab_size = len(tokens)\n",
    "\n",
    "# Determine supervised max length (based on number of TOKENS, not characters)\n",
    "SUPERVISED_MAX_LEN = 0\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i, columns=[\"smiles\"])\n",
    "    df = table.to_pandas()\n",
    "    if not df.empty and 'smiles' in df.columns and not df['smiles'].dropna().empty:\n",
    "        max_len = max(len(smiles_tokenizer(s)) for s in df['smiles'].dropna().astype(str))\n",
    "        SUPERVISED_MAX_LEN = max(SUPERVISED_MAX_LEN, max_len + 2) # +2 for SOS and EOS\n",
    "\n",
    "PRETRAIN_MAX_LEN = 100\n",
    "\n",
    "print(f\"‚úÖ Vocabulary size: {vocab_size}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}, Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}\")\n",
    "print(\"Sample of token_to_idx to verify 'Cl' exists:\", {k: v for k, v in token_to_idx.items() if 'Cl' in k or 'Br' in k})\n",
    "print(f\"Completed in {time.time() - start:.2f}s\")\n",
    "\n",
    "\n",
    "# Step 2: Define the NEW encoder function using the tokenizer\n",
    "def encode_smiles(smiles, max_len=PRETRAIN_MAX_LEN):\n",
    "    tokenized_smiles = smiles_tokenizer(smiles)\n",
    "    tokens_with_specials = [SOS_TOKEN] + tokenized_smiles[:max_len - 2] + [EOS_TOKEN]\n",
    "    \n",
    "    # Use .get() with a default for any unknown tokens (though unlikely with regex)\n",
    "    token_ids = [token_to_idx.get(tok, token_to_idx[PAD_TOKEN]) for tok in tokens_with_specials]\n",
    "    \n",
    "    # Padding\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [token_to_idx[PAD_TOKEN]] * (max_len - len(token_ids))\n",
    "    \n",
    "    return token_ids[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PREPROCESSING STEP: Pre-tokenize all SMILES strings\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Make sure pandarallel is initialized for this heavy task\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=True)\n",
    "\n",
    "print(\"--- Starting SMILES pre-tokenization ---\")\n",
    "processed_chunk_dir = \"processed_chunks\"\n",
    "chunk_files = sorted(glob.glob(os.path.join(processed_chunk_dir, \"*.parquet\")))\n",
    "\n",
    "if not chunk_files:\n",
    "    raise FileNotFoundError(\"No processed chunk files found. Please run the earlier preprocessing steps.\")\n",
    "\n",
    "for chunk_path in tqdm(chunk_files, desc=\"Pre-tokenizing SMILES chunks\"):\n",
    "    df = pd.read_parquet(chunk_path)\n",
    "    \n",
    "    # Check if this step has already been done to avoid re-running\n",
    "    if 'token_ids' in df.columns:\n",
    "        print(f\"Skipping {os.path.basename(chunk_path)}, already tokenized.\")\n",
    "        continue\n",
    "\n",
    "    # Apply the encode_smiles function to create the new column\n",
    "    df['token_ids'] = df['smiles'].parallel_apply(lambda s: encode_smiles(s, max_len=SUPERVISED_MAX_LEN))\n",
    "    \n",
    "    # Overwrite the chunk file with the new tokenized version\n",
    "    df.to_parquet(chunk_path)\n",
    "\n",
    "print(\"‚úÖ All chunks have been pre-tokenized and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute Morgan fingerprints\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load df_massspecgym from large Parquet file (only SMILES column)\n",
    "massspec_parquet = pq.ParquetFile(\"df_massspecgym_processed_full.parquet\")\n",
    "df_massspecgym = pd.concat([\n",
    "    massspec_parquet.read_row_group(i, columns=[\"smiles\"]).to_pandas()\n",
    "    for i in range(massspec_parquet.num_row_groups)\n",
    "], ignore_index=True)\n",
    "\n",
    "# Load df_external from smaller Parquet file\n",
    "df_external = pd.read_parquet(\"df_external_processed.parquet\")\n",
    "\n",
    "# Combine and deduplicate SMILES\n",
    "all_smiles = list(set(df_massspecgym['smiles'].dropna().tolist() + df_external['smiles'].dropna().tolist()))\n",
    "\n",
    "# Function that avoids unpicklable generator\n",
    "def fingerprint_one(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            generator = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "            return smiles, generator.GetFingerprint(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {smiles} ‚Üí {e}\")\n",
    "    return smiles, None\n",
    "\n",
    "# Run parallel computation with 12 CPUs\n",
    "results = Parallel(n_jobs=12, verbose=5)(\n",
    "    delayed(fingerprint_one)(s) for s in all_smiles\n",
    ")\n",
    "\n",
    "# Collect into dictionary\n",
    "all_fingerprints = {s: fp for s, fp in results if fp is not None}\n",
    "\n",
    "# Save to file\n",
    "with open(\"all_morgan_fingerprints.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_fingerprints, f)\n",
    "\n",
    "print(f\"‚úÖ Done in {time.time() - start:.2f}s ‚Äî {len(all_fingerprints)} fingerprints saved to all_morgan_fingerprints.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSMSDataset Class (Final Optimized Version)\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe, graph_data_list):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.graph_data_list = graph_data_list\n",
    "\n",
    "        if len(self.df) != len(self.graph_data_list):\n",
    "            raise ValueError(f\"DataFrame length ({len(self.df)}) and graph_data_list length ({len(self.graph_data_list)}) must match.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        spectrum = torch.tensor(row[\"binned\"], dtype=torch.float)\n",
    "        graph = self.graph_data_list[idx]\n",
    "\n",
    "        if not isinstance(graph, Data):\n",
    "            if isinstance(graph, tuple) and len(graph) > 0 and isinstance(graph[0], Data):\n",
    "                graph = graph[0]\n",
    "            else:\n",
    "                # Create a placeholder for corrupted data to prevent crashes\n",
    "                graph = Data(x=torch.zeros((1, 1), dtype=torch.float), edge_index=torch.empty((2, 0), dtype=torch.long))\n",
    "\n",
    "        # Directly fetch the pre-tokenized list of integers. This is extremely fast.\n",
    "        smiles_tensor = torch.tensor(row[\"token_ids\"], dtype=torch.long)\n",
    "        \n",
    "        # Handle other potential NaN values safely\n",
    "        precursor_bin_val = 0 if pd.isna(row[\"precursor_bin\"]) else int(row[\"precursor_bin\"])\n",
    "        adduct_idx_val = 0 if pd.isna(row[\"adduct_idx\"]) else int(row[\"adduct_idx\"])\n",
    "        \n",
    "        ion_mode = torch.tensor(row[\"ion_mode\"], dtype=torch.long)\n",
    "        precursor_bin = torch.tensor(precursor_bin_val, dtype=torch.long)\n",
    "        adduct_idx = torch.tensor(adduct_idx_val, dtype=torch.long)\n",
    "        raw_smiles = row[\"smiles\"]\n",
    "            \n",
    "        return (spectrum, graph, smiles_tensor, ion_mode, precursor_bin, adduct_idx, raw_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transformer Encoder\n",
    "class SpectrumTransformerEncoder(nn.Module):\n",
    "    # ‚úÖ FIX: Added num_adducts to the constructor\n",
    "    def __init__(self, num_adducts, input_dim=1000, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.metadata_emb = nn.Linear(2 + 32, 64)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model + 64, d_model // 2)\n",
    "        # ‚úÖ FIX: Use the num_adducts argument\n",
    "        self.adduct_emb = nn.Embedding(num_adducts, 32)\n",
    "\n",
    "    def forward(self, src, ion_mode_idx, precursor_idx, adduct_idx):\n",
    "        src = self.input_proj(src).unsqueeze(1)\n",
    "        adduct_embed = self.adduct_emb(adduct_idx)\n",
    "        metadata = self.metadata_emb(torch.cat([ion_mode_idx.unsqueeze(-1).float(), precursor_idx.unsqueeze(-1).float(), adduct_embed], dim=-1))\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src).squeeze(1)\n",
    "        output = self.norm(output)\n",
    "        output = torch.cat([output, metadata], dim=-1)\n",
    "        output = self.fc(output)\n",
    "        # Return None for attn_weights to simplify\n",
    "        return output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GNN Encoder (Corrected for Missing Attribute)\n",
    "class SpectrumGNNEncoder(MessagePassing):\n",
    "    def __init__(self, num_adducts, d_model=768, hidden_dim=256, num_layers=3, dropout=0.2):\n",
    "        super().__init__(aggr='mean')\n",
    "        # ‚úÖ FIX: Added the missing line to save num_layers as a class attribute\n",
    "        self.num_layers = num_layers \n",
    "        \n",
    "        self.input_proj = nn.Linear(1, hidden_dim)\n",
    "        self.message_nets = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.update_nets = nn.ModuleList([nn.GRUCell(hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.metadata_emb = nn.Linear(2 + 32, hidden_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, d_model // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.adduct_emb = nn.Embedding(num_adducts, 32)\n",
    "\n",
    "    def forward(self, batch_data, ion_mode_idx, precursor_idx, adduct_idx):\n",
    "        x, edge_index, batch = batch_data.x, batch_data.edge_index, batch_data.batch\n",
    "        adduct_embed = self.adduct_emb(adduct_idx)\n",
    "        metadata_per_graph = self.metadata_emb(torch.cat([\n",
    "            ion_mode_idx.unsqueeze(-1).float(), precursor_idx.unsqueeze(-1).float(), adduct_embed\n",
    "        ], dim=-1))\n",
    "        metadata = metadata_per_graph[batch]\n",
    "        x = self.input_proj(x)\n",
    "        h = F.relu(x)\n",
    "        \n",
    "        # This loop will now work correctly\n",
    "        for i in range(self.num_layers):\n",
    "            self._propagate_layer = i\n",
    "            m = self.propagate(edge_index, x=h)\n",
    "            m = m + metadata \n",
    "            h = self.update_nets[i](m, h)\n",
    "            h = self.dropout(h)\n",
    "            \n",
    "        pooled_x = global_mean_pool(h, batch)\n",
    "        pooled_x = self.norm(pooled_x)\n",
    "        return self.output_layer(pooled_x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        layer_idx = getattr(self, '_propagate_layer', 0)\n",
    "        return self.message_nets[layer_idx](x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transformer Decoder\n",
    "class SmilesTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        valence_rules = {'C': 4, 'N': 3, 'O': 2, 'S': 2, 'P': 3, 'F': 1, 'Cl': 1, 'Br': 1, 'I': 1, 'H': 1}\n",
    "        valence_map = torch.zeros(vocab_size)\n",
    "        bond_map = torch.zeros(vocab_size)\n",
    "        for token, idx in token_to_idx.items():\n",
    "            if token in valence_rules: valence_map[idx] = valence_rules[token]\n",
    "            elif token == '=': bond_map[idx] = 2\n",
    "            elif token == '#': bond_map[idx] = 3\n",
    "            elif token in ['-', '(', ')', '[', ']', '.', ':', '@', '/', '\\\\']: bond_map[idx] = 1\n",
    "        self.register_buffer('valence_map', valence_map)\n",
    "        self.register_buffer('bond_map', bond_map)\n",
    "\n",
    "    def compute_valence(self, smiles_token_ids):\n",
    "        atom_valences = self.valence_map[smiles_token_ids]\n",
    "        bond_connections = self.bond_map[smiles_token_ids]\n",
    "        total_valence_provided = torch.sum(atom_valences, dim=1)\n",
    "        total_bonds_formed = torch.sum(bond_connections, dim=1)\n",
    "        return torch.relu(total_bonds_formed - total_valence_provided)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None):\n",
    "        embedded = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        output = self.transformer_decoder(embedded, memory, tgt_mask, memory_key_padding_mask)\n",
    "        output = self.norm(output)\n",
    "        logits = self.output_layer(output)\n",
    "        valence_penalty = self.compute_valence(tgt)\n",
    "        return logits, valence_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Full Hybrid Model\n",
    "class MSMS2SmilesHybrid(nn.Module):\n",
    "    # ‚úÖ FIX: Added num_adducts to the constructor\n",
    "    def __init__(self, vocab_size, num_adducts, d_model=768, nhead=12, num_layers=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # ‚úÖ FIX: Pass num_adducts to the child modules\n",
    "        self.transformer_encoder = SpectrumTransformerEncoder(num_adducts=num_adducts, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout=dropout)\n",
    "        self.gnn_encoder = SpectrumGNNEncoder(num_adducts=num_adducts, d_model=d_model, num_layers=3, dropout=dropout)\n",
    "        self.decoder = SmilesTransformerDecoder(vocab_size, d_model, nhead, num_layers, dropout=dropout)\n",
    "        self.combine_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, tgt_len):\n",
    "        mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "\n",
    "    def forward(self, spectrum, graph_data, tgt, ion_mode_idx, precursor_idx, adduct_idx, tgt_mask=None):\n",
    "        trans_output, _ = self.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "        gnn_output = self.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "        memory = self.combine_layer(torch.cat([trans_output, gnn_output], dim=-1)).unsqueeze(1)\n",
    "        smiles_output, valence_penalty = self.decoder(tgt, memory, tgt_mask)\n",
    "        return smiles_output, valence_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSL Pretraining\n",
    "def ssl_pretrain(model, dataloader, epochs=3, lr=1e-4):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for spectra, graph_data, smiles_tokens, masked_tokens, ion_modes, precursor_bins, adduct_indices, _ in tqdm(dataloader, desc=f\"SSL Epoch {epoch+1}/{epochs}\"):\n",
    "            spectra = spectra.to(device)\n",
    "            ion_modes = ion_modes.to(device)\n",
    "            precursor_bins = precursor_bins.to(device)\n",
    "            adduct_indices = adduct_indices.to(device)\n",
    "            smiles_tokens = smiles_tokens.to(device)\n",
    "            masked_tokens = masked_tokens.to(device)\n",
    "            tgt_input = masked_tokens[:, :-1]\n",
    "            tgt_output = smiles_tokens[:, 1:]\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                smiles_output, _, valence_penalty, _, _, _ = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
    "                loss = criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1)) + 0.1 * valence_penalty.mean()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"SSL Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, f'ssl_checkpoint_epoch_{epoch+1}.pt')\n",
    "        print(f\"Saved SSL checkpoint: ssl_checkpoint_epoch_{epoch+1}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Training with RL\n",
    "def supervised_train(model, train_loader, val_loader, epochs=30, lr=1e-4, patience=5):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    smiles_criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
    "    fp_criterion = nn.BCEWithLogitsLoss()\n",
    "    mw_criterion = nn.MSELoss()\n",
    "    sub_criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, raw_smiles in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            spectra = spectra.to(device)\n",
    "            ion_modes = ion_modes.to(device)\n",
    "            precursor_bins = precursor_bins.to(device)\n",
    "            adduct_indices = adduct_indices.to(device)\n",
    "            smiles_tokens = smiles_tokens.to(device)\n",
    "            tgt_input = smiles_tokens[:, :-1]\n",
    "            tgt_output = smiles_tokens[:, 1:]\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                smiles_output, fp_output, valence_penalty, _, _, substructure_pred = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
    "                smiles_loss = smiles_criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "                fp_loss = 0\n",
    "                mw_loss = 0\n",
    "                sub_loss = 0\n",
    "                valid_count = 0\n",
    "                substructure_targets = torch.zeros(len(raw_smiles), 30, dtype=torch.float, device=device)\n",
    "                for i, (smiles, fp) in enumerate(zip(raw_smiles, fp_output)):\n",
    "                    mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "                    if mol:\n",
    "                        true_fp = morgan_gen.GetFingerprint(mol)\n",
    "                        fp_loss += fp_criterion(fp, torch.tensor([int(b) for b in true_fp.ToBitString()], dtype=torch.float, device=device))\n",
    "                        mw_loss += mw_criterion(torch.tensor(Descriptors.MolWt(mol), dtype=torch.float, device=device), torch.tensor(500.0, dtype=torch.float, device=device))\n",
    "                        for j, smarts in enumerate(model.gnn_encoder.substructures):\n",
    "                            if mol.HasSubstructMatch(Chem.MolFromSmarts(smarts)):\n",
    "                                substructure_targets[i, j] = 1\n",
    "                        valid_count += 1\n",
    "                fp_loss = fp_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
    "                mw_loss = mw_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
    "                sub_loss = sub_criterion(substructure_pred, substructure_targets)\n",
    "                sigma_smiles = torch.clamp(torch.exp(model.log_sigma_smiles), 0.1, 10.0)\n",
    "                sigma_fp = torch.clamp(torch.exp(model.log_sigma_fp), 0.1, 10.0)\n",
    "                sigma_sub = torch.clamp(torch.exp(model.log_sigma_sub), 0.1, 10.0)\n",
    "                supervised_loss = (smiles_loss / (2 * sigma_smiles**2) + model.log_sigma_smiles) + \\\n",
    "                                 (0.1 * fp_loss / (2 * sigma_fp**2) + model.log_sigma_fp) + \\\n",
    "                                 (0.1 * sub_loss / (2 * sigma_sub**2) + model.log_sigma_sub) + \\\n",
    "                                 0.1 * valence_penalty.mean() + 0.1 * mw_loss\n",
    "                # RL component: Tanimoto reward\n",
    "                rl_loss = 0\n",
    "                if epoch >= 5:  # Start RL after initial training\n",
    "                    pred_smiles = beam_search(model, spectra[0], graph_data[0], ion_modes[0], precursor_bins[0], adduct_indices[0], raw_smiles[0], beam_width=5, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "                    if pred_smiles[0][0] != \"Invalid SMILES\":\n",
    "                        tanimoto = tanimoto_similarity(pred_smiles[0][0], raw_smiles[0], all_fingerprints)\n",
    "                        rl_loss = -torch.log(torch.tensor(tanimoto + 1e-6, device=device))\n",
    "                loss = supervised_loss + 0.1 * rl_loss\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, raw_smiles in val_loader:\n",
    "                spectra = spectra.to(device)\n",
    "                ion_modes = ion_modes.to(device)\n",
    "                precursor_bins = precursor_bins.to(device)\n",
    "                adduct_indices = adduct_indices.to(device)\n",
    "                smiles_tokens = smiles_tokens.to(device)\n",
    "                tgt_input = smiles_tokens[:, :-1]\n",
    "                tgt_output = smiles_tokens[:, 1:]\n",
    "                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "                with autocast():\n",
    "                    smiles_output, fp_output, valence_penalty, _, _, substructure_pred = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
    "                    smiles_loss = smiles_criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "                    fp_loss = 0\n",
    "                    mw_loss = 0\n",
    "                    sub_loss = 0\n",
    "                    valid_count = 0\n",
    "                    substructure_targets = torch.zeros(len(raw_smiles), 30, dtype=torch.float, device=device)\n",
    "                    for i, (smiles, fp) in enumerate(zip(raw_smiles, fp_output)):\n",
    "                        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "                        if mol:\n",
    "                            true_fp = morgan_gen.GetFingerprint(mol)\n",
    "                            fp_loss += fp_criterion(fp, torch.tensor([int(b) for b in true_fp.ToBitString()], dtype=torch.float, device=device))\n",
    "                            mw_loss += mw_criterion(torch.tensor(Descriptors.MolWt(mol), dtype=torch.float, device=device), torch.tensor(500.0, dtype=torch.float, device=device))\n",
    "                            for j, smarts in enumerate(model.gnn_encoder.substructures):\n",
    "                                if mol.HasSubstructMatch(Chem.MolFromSmarts(smarts)):\n",
    "                                    substructure_targets[i, j] = 1\n",
    "                            valid_count += 1\n",
    "                    fp_loss = fp_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
    "                    mw_loss = mw_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
    "                    sub_loss = sub_criterion(substructure_pred, substructure_targets)\n",
    "                    sigma_smiles = torch.clamp(torch.exp(model.log_sigma_smiles), 0.1, 10.0)\n",
    "                    sigma_fp = torch.clamp(torch.exp(model.log_sigma_fp), 0.1, 10.0)\n",
    "                    sigma_sub = torch.clamp(torch.exp(model.log_sigma_sub), 0.1, 10.0)\n",
    "                    loss = (smiles_loss / (2 * sigma_smiles**2) + model.log_sigma_smiles) + \\\n",
    "                           (0.1 * fp_loss / (2 * sigma_fp**2) + model.log_sigma_fp) + \\\n",
    "                           (0.1 * sub_loss / (2 * sigma_sub**2) + model.log_sigma_sub) + \\\n",
    "                           0.1 * valence_penalty.mean() + 0.1 * mw_loss\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'token_to_idx': token_to_idx,\n",
    "                'idx_to_token': idx_to_token\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'token_to_idx': token_to_idx,\n",
    "                'idx_to_token': idx_to_token\n",
    "            }, 'best_msms_hybrid.pt')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES Syntax Validator\n",
    "def is_valid_smiles_syntax(smiles):\n",
    "    stack = []\n",
    "    for c in smiles:\n",
    "        if c in '([':\n",
    "            stack.append(c)\n",
    "        elif c == ')':\n",
    "            if not stack or stack[-1] != '(':\n",
    "                return False\n",
    "            stack.pop()\n",
    "        elif c == ']':\n",
    "            if not stack or stack[-1] != '[':\n",
    "                return False\n",
    "            stack.pop()\n",
    "    if stack:\n",
    "        return False\n",
    "    i = 0\n",
    "    while i < len(smiles):\n",
    "        if smiles[i] == '[':\n",
    "            j = smiles.find(']', i)\n",
    "            if j == -1:\n",
    "                return False\n",
    "            atom = smiles[i+1:j]\n",
    "            if not any(a in atom for a in valid_atoms):\n",
    "                return False\n",
    "            i = j + 1\n",
    "        else:\n",
    "            if smiles[i] in valid_atoms or smiles[i] in '()=#/\\\\@.:':\n",
    "                i += 1\n",
    "            else:\n",
    "                return False\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        return mol is not None\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDKit-based Molecular Property Filter\n",
    "def is_plausible_molecule(smiles, true_mol, max_mw=1500, min_logp=-7, max_logp=7):\n",
    "    mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "    if not mol or not is_valid_smiles_syntax(smiles):\n",
    "        return False\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    true_mw = Descriptors.MolWt(true_mol) if true_mol else 500\n",
    "    return mw <= max_mw and min_logp <= logp <= max_logp and abs(mw - true_mw) < 300\n",
    "\n",
    "# Evaluation Metrics\n",
    "def dice_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    if mol1 and mol2:\n",
    "        fp1 = morgan_gen.GetFingerprint(mol1)\n",
    "        fp2 = morgan_gen.GetFingerprint(mol2)\n",
    "        return DataStructs.DiceSimilarity(fp1, fp2)\n",
    "    return 0.0\n",
    "\n",
    "def mcs_similarity(true_smiles, pred_smiles):\n",
    "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
    "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
    "    if mol1 and mol2:\n",
    "        mcs = rdFMCS.FindMCS([mol1, mol2], timeout=30)\n",
    "        return mcs.numAtoms / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    return 0.0\n",
    "\n",
    "def mw_difference(true_smiles, pred_smiles):\n",
    "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
    "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
    "    if mol1 and mol2:\n",
    "        return abs(Descriptors.MolWt(mol1) - Descriptors.MolWt(mol2))\n",
    "    return float('inf')\n",
    "\n",
    "def logp_difference(true_smiles, pred_smiles):\n",
    "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
    "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
    "    if mol1 and mol2:\n",
    "        return abs(Descriptors.MolLogP(mol1) - Descriptors.MolLogP(mol2))\n",
    "    return float('inf')\n",
    "\n",
    "def substructure_match(true_smiles, pred_smiles, substructures):\n",
    "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
    "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
    "    if not mol1 or not mol2:\n",
    "        return 0\n",
    "    matches = 0\n",
    "    for smarts in substructures:\n",
    "        pattern = Chem.MolFromSmarts(smarts)\n",
    "        if mol1.HasSubstructMatch(pattern) and mol2.HasSubstructMatch(pattern):\n",
    "            matches += 1\n",
    "    return matches / len(substructures)\n",
    "\n",
    "def validity_rate(pred_smiles_list):\n",
    "    valid = sum(1 for smiles in pred_smiles_list if Chem.MolFromSmiles(smiles, sanitize=True) is not None)\n",
    "    return valid / len(pred_smiles_list) * 100\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2, precomputed_fps=None):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1, sanitize=True)\n",
    "    if not mol1:\n",
    "        return 0.0\n",
    "    fp1 = morgan_gen.GetFingerprint(mol1)\n",
    "    if precomputed_fps and smiles2 in precomputed_fps:\n",
    "        fp2 = precomputed_fps[smiles2]\n",
    "    else:\n",
    "        mol2 = Chem.MolFromSmiles(smiles2, sanitize=True)\n",
    "        if not mol2:\n",
    "            return 0.0\n",
    "        fp2 = morgan_gen.GetFingerprint(mol2)\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "def prediction_diversity(pred_smiles_list):\n",
    "    if len(pred_smiles_list) < 2:\n",
    "        return 0.0\n",
    "    total_tanimoto = 0\n",
    "    count = 0\n",
    "    for i in range(len(pred_smiles_list)):\n",
    "        for j in range(i+1, len(pred_smiles_list)):\n",
    "            total_tanimoto += tanimoto_similarity(pred_smiles_list[i], pred_smiles_list[j])\n",
    "            count += 1\n",
    "    return 1 - (total_tanimoto / count) if count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search with Stereochemistry\n",
    "def beam_search(model, spectrum, graph_data, ion_mode_idx, precursor_idx, adduct_idx, true_smiles, beam_width=10, max_len=150, nucleus_p=0.9, device='cpu'):\n",
    "    model.eval()\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles) if true_smiles else None\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device)\n",
    "        graph_data = Batch.from_data_list([graph_data]).to(device)\n",
    "        ion_mode_idx = torch.tensor([ion_mode_idx], dtype=torch.long).to(device)\n",
    "        precursor_idx = torch.tensor([precursor_idx], dtype=torch.long).to(device)\n",
    "        adduct_idx = torch.tensor([adduct_idx], dtype=torch.long).to(device)\n",
    "        memory = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)[0]\n",
    "        gnn_output, substructure_pred, _ = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "        memory = model.combine_layer(torch.cat([memory, gnn_output], dim=-1)).unsqueeze(1)\n",
    "        sequences = [([token_to_idx[SOS_TOKEN]], 0.0)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == token_to_idx[EOS_TOKEN]:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                partial_smiles = ''.join([idx_to_token.get(idx, '') for idx in seq[1:]])\n",
    "                if not is_valid_smiles_syntax(partial_smiles):\n",
    "                    continue\n",
    "                tgt_input = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "                tgt_mask = model.generate_square_subsequent_mask(len(seq)).to(device)\n",
    "                outputs, valence_penalty = model.decoder(tgt_input, memory, substructure_pred, tgt_mask)\n",
    "                log_probs = F.log_softmax(outputs[0, -1], dim=-1).cpu().numpy() - 0.1 * valence_penalty.cpu().numpy()\n",
    "                # Boost stereochemistry tokens\n",
    "                for tok in ['@', '/']:\n",
    "                    if tok in token_to_idx:\n",
    "                        log_probs[token_to_idx[tok]] += 0.5\n",
    "                sorted_probs = np.sort(np.exp(log_probs))[::-1]\n",
    "                cumulative_probs = np.cumsum(sorted_probs)\n",
    "                cutoff_idx = np.searchsorted(cumulative_probs, nucleus_p)\n",
    "                top_tokens = np.argsort(log_probs)[-cutoff_idx:] if cutoff_idx > 0 else np.argsort(log_probs)[-1:]\n",
    "                top_probs = np.exp(log_probs[top_tokens]) / np.sum(np.exp(log_probs[top_tokens]))\n",
    "                for tok in np.random.choice(top_tokens, size=min(beam_width, len(top_tokens)), p=top_probs):\n",
    "                    new_smiles = partial_smiles + idx_to_token.get(int(tok), '')\n",
    "                    if is_valid_smiles_syntax(new_smiles):\n",
    "                        diversity_penalty = 0.2 * sum(1 for s, _ in sequences if tok in s[1:-1])\n",
    "                        all_candidates.append((seq + [int(tok)], score + log_probs[tok] - diversity_penalty))\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if all(seq[-1] == token_to_idx[EOS_TOKEN] for seq, _ in sequences):\n",
    "                break\n",
    "\n",
    "        results = []\n",
    "        for seq, score in sequences:\n",
    "            smiles = ''.join([idx_to_token.get(idx, '') for idx in seq[1:-1]])\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "                if mol and is_plausible_molecule(smiles, true_mol):\n",
    "                    smiles = Chem.MolToSmiles(mol, canonical=True, doRandom=True)\n",
    "                    confidence = np.exp(score / len(seq))\n",
    "                    results.append((smiles, confidence))\n",
    "            except:\n",
    "                continue\n",
    "        return results if results else [(\"Invalid SMILES\", 0.0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def plot_attention_weights(attn_weights, title=\"Transformer Attention Weights\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attn_weights.squeeze().cpu().numpy(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_gnn_edge_weights(edge_weights, edge_index, title=\"GNN Edge Importance\"):\n",
    "    edge_scores = edge_weights[-1].cpu().numpy()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.hist(edge_scores, bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Edge Weight Magnitude\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Error Analysis\n",
    "def error_analysis(pred_smiles_list, true_smiles_list, adducts, precomputed_fps):\n",
    "    errors = {'small': 0, 'large': 0, 'aromatic': 0, 'aliphatic': 0}\n",
    "    adduct_errors = {adduct: [] for adduct in adduct_types}\n",
    "    for pred_smiles, true_smiles, adduct in zip(pred_smiles_list, true_smiles_list, adducts):\n",
    "        tanimoto = tanimoto_similarity(pred_smiles, true_smiles, precomputed_fps)\n",
    "        if tanimoto < 0.3:\n",
    "            mol = Chem.MolFromSmiles(true_smiles)\n",
    "            if mol:\n",
    "                mw = Descriptors.MolWt(mol)\n",
    "                is_aromatic = any(atom.GetIsAromatic() for atom in mol.GetAtoms())\n",
    "                errors['small' if mw < 300 else 'large'] += 1\n",
    "                errors['aromatic' if is_aromatic else 'aliphatic'] += 1\n",
    "                adduct_errors[adduct].append(tanimoto)\n",
    "    print(\"Error Analysis:\")\n",
    "    print(f\"Small molecules (<300 Da) errors: {errors['small']}\")\n",
    "    print(f\"Large molecules (‚â•300 Da) errors: {errors['large']}\")\n",
    "    print(f\"Aromatic molecule errors: {errors['aromatic']}\")\n",
    "    print(f\"Aliphatic molecule errors: {errors['aliphatic']}\")\n",
    "    for adduct, scores in adduct_errors.items():\n",
    "        if scores:\n",
    "            print(f\"Adduct {adduct} - Avg Tanimoto: {np.mean(scores):.4f}, Count: {len(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "def objective(trial, train_data, val_data):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
    "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2, fp_size=2048).to(device)\n",
    "    return supervised_train(model, train_loader, val_loader, epochs=10, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optimized Cross-Validation and Training ===\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Setup ---\n",
    "print(\"--- Initializing Setup ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "df_massspecgym_sample = pd.read_parquet(\"df_massspecgym.parquet\", columns=[\"adduct\"])\n",
    "adduct_types = df_massspecgym_sample['adduct'].unique()\n",
    "del df_massspecgym_sample\n",
    "print(f\"Adduct types loaded successfully. Count: {len(adduct_types)}\")\n",
    "\n",
    "processed_chunk_dir = \"processed_chunks\"\n",
    "graph_chunk_dir = \"/media/onepaw/seagate_manual/graph_data_chunks\"\n",
    "processed_files = np.array(sorted(glob.glob(os.path.join(processed_chunk_dir, \"*.parquet\"))))\n",
    "graph_files = np.array(sorted(glob.glob(os.path.join(graph_chunk_dir, \"*.pkl\"))))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "print(f\"Found {len(processed_files)} chunk file pairs for 5-fold cross-validation.\")\n",
    "\n",
    "# === Training Parameters ===\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 4\n",
    "PATIENCE = 5\n",
    "early_break = True  # Set to False to run all folds\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(processed_files)):\n",
    "    print(f\"\\n{'='*20} FOLD {fold_idx + 1}/5 {'='*20}\")\n",
    "    train_proc_files, train_graph_files = processed_files[train_indices], graph_files[train_indices]\n",
    "    val_proc_files, val_graph_files = processed_files[val_indices], graph_files[val_indices]\n",
    "\n",
    "    print(f\"Initializing model for Fold {fold_idx + 1}...\")\n",
    "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, num_adducts=len(adduct_types)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    scaler = torch.cuda.amp.GradScaler()  # ‚úÖ fixed: no device_type here\n",
    "    smiles_criterion = torch.nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
    "    best_val_loss, epochs_no_improve = float('inf'), 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss, train_batches = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for proc_file, graph_file in tqdm(zip(train_proc_files, train_graph_files), total=len(train_proc_files), desc=\"Training\"):\n",
    "            try:\n",
    "                df_chunk = pd.read_parquet(proc_file)\n",
    "                with open(graph_file, 'rb') as f:\n",
    "                    graph_data_chunk = pickle.load(f)\n",
    "\n",
    "                dataset = MSMSDataset(df_chunk, graph_data_chunk)\n",
    "                loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "                for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, _ in loader:\n",
    "                    graph_data = graph_data.to(device)\n",
    "                    spectra, smiles_tokens = spectra.to(device), smiles_tokens.to(device)\n",
    "                    ion_modes, precursor_bins, adduct_indices = ion_modes.to(device), precursor_bins.to(device), adduct_indices.to(device)\n",
    "                    tgt_input, tgt_output = smiles_tokens[:, :-1], smiles_tokens[:, 1:]\n",
    "                    tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1))\n",
    "\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    with torch.cuda.amp.autocast():  # ‚úÖ fixed\n",
    "                        out, val_penalty = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
    "                        loss = smiles_criterion(out.reshape(-1, vocab_size), tgt_output.reshape(-1)) + 0.1 * val_penalty.mean()\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    total_train_loss += loss.item()\n",
    "                    train_batches += 1\n",
    "\n",
    "                del df_chunk, graph_data_chunk, dataset, loader\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in {os.path.basename(proc_file)}: {e}\")\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = total_train_loss / max(train_batches, 1)\n",
    "        print(f\"‚úÖ Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Time: {(time.time()-start_time)/60:.2f} mins\")\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        total_val_loss, val_batches = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for proc_file, graph_file in tqdm(zip(val_proc_files, val_graph_files), total=len(val_proc_files), desc=\"Validating\"):\n",
    "                try:\n",
    "                    df_chunk = pd.read_parquet(proc_file)\n",
    "                    with open(graph_file, 'rb') as f:\n",
    "                        graph_data_chunk = pickle.load(f)\n",
    "\n",
    "                    dataset = MSMSDataset(df_chunk, graph_data_chunk)\n",
    "                    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "                    for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, _ in loader:\n",
    "                        graph_data = graph_data.to(device)\n",
    "                        spectra = spectra.to(device)\n",
    "                        smiles_tokens = smiles_tokens.to(device)\n",
    "                        ion_modes = ion_modes.to(device)\n",
    "                        precursor_bins = precursor_bins.to(device)\n",
    "                        adduct_indices = adduct_indices.to(device)\n",
    "                        tgt_input, tgt_output = smiles_tokens[:, :-1], smiles_tokens[:, 1:]\n",
    "                        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1))\n",
    "\n",
    "                        with torch.cuda.amp.autocast():  # ‚úÖ fixed\n",
    "                            out, val_penalty = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
    "                            loss = smiles_criterion(out.reshape(-1, vocab_size), tgt_output.reshape(-1)) + 0.1 * val_penalty.mean()\n",
    "\n",
    "                        total_val_loss += loss.item()\n",
    "                        val_batches += 1\n",
    "\n",
    "                    del df_chunk, graph_data_chunk, dataset, loader\n",
    "                    gc.collect()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Validation error in {os.path.basename(proc_file)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        avg_val_loss = total_val_loss / max(val_batches, 1)\n",
    "        print(f\"üìä Val Loss: {avg_val_loss:.4f}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"üì• Best model updated (Fold {fold_idx+1})\")\n",
    "            torch.save({'model_state_dict': model.state_dict(), 'token_to_idx': token_to_idx, 'idx_to_token': idx_to_token},\n",
    "                       f\"best_model_fold_{fold_idx+1}.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"üõë Early stopping\")\n",
    "            break\n",
    "\n",
    "    fold_results.append(best_val_loss)\n",
    "    print(f\"‚úÖ Fold {fold_idx+1} done | Best Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    if early_break:\n",
    "        break\n",
    "\n",
    "print(\"\\nüéâ Training Complete\")\n",
    "print(f\"Fold results: {fold_results}\")\n",
    "print(f\"Avg best val loss: {np.mean(fold_results):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIAnnRuExQUs"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# External Dataset Evaluation\n",
    "model.eval()\n",
    "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
    "pred_smiles_list = []\n",
    "true_smiles_list = []\n",
    "adducts_list = []\n",
    "num_samples = min(5, len(external_dataset))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample_spectrum = external_dataset[sample_idx][0]\n",
    "    sample_graph = external_dataset[sample_idx][1]\n",
    "    sample_ion_mode = external_dataset[sample_idx][3]\n",
    "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
    "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
    "    true_smiles = external_dataset[sample_idx][6]\n",
    "\n",
    "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
    "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
    "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
    "\n",
    "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
    "    print(\"Top Predicted SMILES:\")\n",
    "    for smiles, confidence in predicted_results[:3]:\n",
    "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
    "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
    "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
    "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
    "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
    "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
    "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
    "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "        if smiles != \"Invalid SMILES\":\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
    "\n",
    "    # Visualize molecules\n",
    "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
    "        if pred_mol and true_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            img_array = np.array(img.convert('RGB'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(img_array)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Visualize attention and GNN weights for first sample\n",
    "    if sample_idx == 0:\n",
    "        with torch.no_grad():\n",
    "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
    "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
    "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
    "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
    "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
    "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
    "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
    "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
    "print(\"External Metrics Summary:\")\n",
    "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
    "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
    "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
    "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
    "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"AMP GradScaler:\", torch.amp.GradScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (naturems)",
   "language": "python",
   "name": "naturems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
