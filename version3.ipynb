{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c425fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for MS-to-Structure pipeline\n",
    "!pip install torch torch_geometric rdkit-pypi selfies datasets optuna nltk python-Levenshtein tqdm scikit-learn matplotlib xgboost faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c2a40",
   "metadata": {},
   "source": [
    "# MS-to-Structure Deep Learning Pipeline (Jupyter Version)\n",
    "\n",
    "This notebook implements a robust mass spectrometry-to-structure (MS-to-structure) deep learning pipeline, adapted for interactive use. It includes data preprocessing, molecular string handling with SELFIES, model definition, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up logging for Jupyter compatibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import optuna\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance\n",
    "import logging\n",
    "import traceback\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setup logging for Jupyter (prints to stdout)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility and define global variables\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Configuration\n",
    "class Config:\n",
    "    DATASET_PATH = '/kaggle/input/tandem'  # Change to your dataset path\n",
    "    TRAIN_SPLIT = 0.9\n",
    "    RANDOM_SEED = 42\n",
    "    N_BINS = 1000\n",
    "    MAX_MZ = 1000\n",
    "    NOISE_LEVEL = 0.05\n",
    "    MAX_ISOMERS = 8\n",
    "    D_MODEL = 512\n",
    "    NHEAD = 8\n",
    "    NUM_LAYERS = 6\n",
    "    BATCH_SIZE = 32\n",
    "    SSL_EPOCHS = 3\n",
    "    SUPERVISED_EPOCHS = 30\n",
    "    LEARNING_RATE = 1e-4\n",
    "    PATIENCE = 5\n",
    "    N_FOLDS = 5\n",
    "    # Token definitions\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    SOS_TOKEN = '< SOS >'\n",
    "    EOS_TOKEN = '<EOS>'\n",
    "    MASK_TOKEN = '[MASK]'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Load dataset with configurable path\n",
    "try:\n",
    "    dataset = load_dataset(config.DATASET_PATH, split='train')\n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(f'Loaded dataset with {len(df)} samples')\n",
    "except Exception as e:\n",
    "    print(f'Error loading dataset: {e}')\n",
    "    print('Please update config.DATASET_PATH')\n",
    "    raise\n",
    "\n",
    "# Split dataset based on configuration\n",
    "split_idx = int(config.TRAIN_SPLIT * len(df))\n",
    "df_massspecgym, df_external = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
    "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
    "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7780db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=None):\n",
    "    max_isomers = max_isomers or config.MAX_ISOMERS\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.EnumerateStereoisomersOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply canonicalization, augmentation, and binning to the dataframe\n",
    "# Preprocess ion mode, precursor m/z, and adducts\n",
    "df_massspecgym['smiles'] = df_massspecgym['smiles'].apply(canonicalize_smiles)\n",
    "df_external['smiles'] = df_external['smiles'].apply(canonicalize_smiles)\n",
    "df_massspecgym = df_massspecgym.dropna(subset=['smiles'])\n",
    "df_external = df_external.dropna(subset=['smiles'])\n",
    "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].apply(augment_smiles)\n",
    "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list']).rename(columns={'smiles_list': 'smiles'})\n",
    "\n",
    "df_massspecgym['ion_mode'] = df_massspecgym['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_massspecgym['precursor_bin'] = pd.qcut(df_massspecgym['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "df_external['ion_mode'] = df_external['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_external['precursor_bin'] = pd.qcut(df_external['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx)\n",
    "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
    "\n",
    "df_massspecgym[['binned', 'graph_data']] = df_massspecgym.apply(\n",
    "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n",
    "df_external[['binned', 'graph_data']] = df_external.apply(\n",
    "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for XGBoost from tabular data\n",
    "def extract_tabular_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        spectrum = row['binned']\n",
    "        feat = [\n",
    "            np.mean(spectrum), np.std(spectrum), np.max(spectrum),\n",
    "            np.sum(spectrum > 0.1), row['precursor_mz'], row['ion_mode'],\n",
    "            row['adduct_idx'], len(row['mzs'])\n",
    "        ]\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "X_train = extract_tabular_features(df_massspecgym)\n",
    "X_test = extract_tabular_features(df_external)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df_massspecgym['smiles'])\n",
    "y_test = le.transform(df_external['smiles'])\n",
    "\n",
    "print(f'Training features shape: {X_train.shape}')\n",
    "print(f'Test features shape: {X_test.shape}')\n",
    "print(f'Number of unique SMILES: {len(le.classes_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('Training XGBoost model...')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'XGBoost Accuracy: {accuracy:.4f}')\n",
    "\n",
    "feature_names = ['mean_intensity', 'std_intensity', 'max_intensity', 'peak_count', 'precursor_mz', 'ion_mode', 'adduct_idx', 'spectrum_length']\n",
    "importance = xgb_model.feature_importances_\n",
    "for name, imp in zip(feature_names, importance):\n",
    "    print(f'{name}: {imp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print('\\nSample predictions:')\n",
    "for i in range(min(5, len(y_test))):\n",
    "    true_smiles = le.inverse_transform([y_test[i]])[0]\n",
    "    pred_smiles = le.inverse_transform([y_pred[i]])[0]\n",
    "    print(f'True: {true_smiles}')\n",
    "    print(f'Pred: {pred_smiles}')\n",
    "    print(f'Match: {true_smiles == pred_smiles}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced RAG System for Molecular Data\n",
    "class MolecularRAG:\n",
    "    def __init__(self, df):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.df = df.copy()\n",
    "        self.morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "        self.build_molecular_descriptions()\n",
    "        self.build_index()\n",
    "        self.build_fingerprint_index()\n",
    "\n",
    "    def get_molecular_properties(self, smiles):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                mw = Descriptors.MolWt(mol)\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                hbd = Descriptors.NumHDonors(mol)\n",
    "                hba = Descriptors.NumHAcceptors(mol)\n",
    "                rings = Descriptors.RingCount(mol)\n",
    "                aromatic = Descriptors.NumAromaticRings(mol)\n",
    "                return {'mw': mw, 'logp': logp, 'hbd': hbd, 'hba': hba, 'rings': rings, 'aromatic': aromatic}\n",
    "        except:\n",
    "            pass\n",
    "        return {'mw': 0, 'logp': 0, 'hbd': 0, 'hba': 0, 'rings': 0, 'aromatic': 0}\n",
    "\n",
    "    def build_molecular_descriptions(self):\n",
    "        descriptions = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            props = self.get_molecular_properties(row['smiles'])\n",
    "            desc = f\"Molecule with SMILES {row['smiles']}. \"\n",
    "            desc += f\"Molecular weight: {props['mw']:.1f} Da. \"\n",
    "            desc += f\"LogP: {props['logp']:.2f}. \"\n",
    "            desc += f\"H-bond donors: {props['hbd']}, acceptors: {props['hba']}. \"\n",
    "            desc += f\"Contains {props['rings']} rings, {props['aromatic']} aromatic. \"\n",
    "            desc += f\"Adduct: {row['adduct']}, precursor m/z: {row['precursor_mz']:.2f}. \"\n",
    "            desc += f\"Ion mode: {'positive' if row['ion_mode'] == 0 else 'negative'}.\"\n",
    "            descriptions.append(desc)\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def build_index(self):\n",
    "        print('Building semantic index...')\n",
    "        self.embeddings = self.encoder.encode(self.descriptions, show_progress_bar=True)\n",
    "        self.semantic_index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.semantic_index.add(self.embeddings.astype('float32'))\n",
    "\n",
    "    def build_fingerprint_index(self):\n",
    "        print('Building fingerprint index...')\n",
    "        fingerprints = []\n",
    "        for smiles in self.df['smiles']:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                fp = self.morgan_gen.GetFingerprint(mol)\n",
    "                fp_array = np.zeros(2048)\n",
    "                DataStructs.ConvertToNumpyArray(fp, fp_array)\n",
    "                fingerprints.append(fp_array)\n",
    "            else:\n",
    "                fingerprints.append(np.zeros(2048))\n",
    "        self.fingerprints = np.array(fingerprints)\n",
    "        self.fp_index = faiss.IndexFlatIP(2048)\n",
    "        self.fp_index.add(self.fingerprints.astype('float32'))\n",
    "\n",
    "    def semantic_search(self, query, k=5):\n",
    "        query_emb = self.encoder.encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        scores, indices = self.semantic_index.search(query_emb.astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'score': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz'],\n",
    "                'description': self.descriptions[idx]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def structure_search(self, query_smiles, k=5):\n",
    "        mol = Chem.MolFromSmiles(query_smiles)\n",
    "        if not mol:\n",
    "            return []\n",
    "        query_fp = self.morgan_gen.GetFingerprint(mol)\n",
    "        query_array = np.zeros(2048)\n",
    "        DataStructs.ConvertToNumpyArray(query_fp, query_array)\n",
    "        scores, indices = self.fp_index.search(query_array.reshape(1, -1).astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'tanimoto': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz']\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query, query_smiles=None, k=5, alpha=0.7):\n",
    "        semantic_results = self.semantic_search(query, k*2)\n",
    "        if query_smiles:\n",
    "            structure_results = self.structure_search(query_smiles, k*2)\n",
    "            # Combine scores\n",
    "            combined = {}\n",
    "            for r in semantic_results:\n",
    "                combined[r['smiles']] = {'semantic': r['score'], 'structure': 0, 'data': r}\n",
    "            for r in structure_results:\n",
    "                if r['smiles'] in combined:\n",
    "                    combined[r['smiles']]['structure'] = r['tanimoto']\n",
    "                else:\n",
    "                    combined[r['smiles']] = {'semantic': 0, 'structure': r['tanimoto'], 'data': r}\n",
    "            # Hybrid scoring\n",
    "            for smiles in combined:\n",
    "                combined[smiles]['hybrid_score'] = alpha * combined[smiles]['semantic'] + (1-alpha) * combined[smiles]['structure']\n",
    "            sorted_results = sorted(combined.items(), key=lambda x: x[1]['hybrid_score'], reverse=True)\n",
    "            return [{'smiles': smiles, 'hybrid_score': data['hybrid_score'], 'semantic_score': data['semantic'], 'structure_score': data['structure']} for smiles, data in sorted_results[:k]]\n",
    "        return semantic_results[:k]\n",
    "\n",
    "print('Initializing enhanced RAG system...')\n",
    "rag_system = MolecularRAG(df_massspecgym)\n",
    "print('RAG system ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_semantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Search Examples\n",
    "queries = [\n",
    "    'aromatic compound with hydroxyl group',\n",
    "    'small molecule with high logP',\n",
    "    'compound with multiple rings and nitrogen'\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f'\\nQuery: {query}')\n",
    "    results = rag_system.semantic_search(query, k=3)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'{i+1}. SMILES: {result[\"smiles\"]} (Score: {result[\"score\"]:.4f})')\n",
    "        print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure-based Search\n",
    "query_smiles = 'c1ccccc1O'  # phenol\n",
    "print(f'Structure search for: {query_smiles}')\n",
    "results = rag_system.structure_search(query_smiles, k=5)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'{i+1}. SMILES: {result[\"smiles\"]} (Tanimoto: {result[\"tanimoto\"]:.4f})')\n",
    "    print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search (Semantic + Structure)\n",
    "text_query = 'benzene derivative with oxygen'\n",
    "structure_query = 'c1ccccc1O'\n",
    "print(f'Hybrid search - Text: \"{text_query}\", Structure: {structure_query}')\n",
    "results = rag_system.hybrid_search(text_query, structure_query, k=5)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'{i+1}. SMILES: {result[\"smiles\"]}')\n",
    "    print(f'   Hybrid: {result[\"hybrid_score\"]:.4f}, Semantic: {result[\"semantic_score\"]:.4f}, Structure: {result[\"structure_score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Analysis\n",
    "print('RAG System Statistics:')\n",
    "print(f'Total molecules indexed: {len(rag_system.df)}')\n",
    "print(f'Embedding dimension: {rag_system.embeddings.shape[1]}')\n",
    "print(f'Fingerprint dimension: {rag_system.fingerprints.shape[1]}')\n",
    "\n",
    "# Sample molecular properties distribution\n",
    "mw_values = [rag_system.get_molecular_properties(smiles)['mw'] for smiles in rag_system.df['smiles'].head(100)]\n",
    "print(f'Sample MW range: {min(mw_values):.1f} - {max(mw_values):.1f} Da')\n",
    "\n",
    "# Test query performance\n",
    "import time\n",
    "start = time.time()\n",
    "_ = rag_system.semantic_search('test query', k=10)\n",
    "semantic_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = rag_system.structure_search('CCO', k=10)\n",
    "structure_time = time.time() - start\n",
    "\n",
    "print(f'Semantic search time: {semantic_time:.4f}s')\n",
    "print(f'Structure search time: {structure_time:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELFIES tokenization and vocabulary setup\n",
    "all_smiles = df_massspecgym['smiles'].tolist()\n",
    "all_selfies = [sf.encoder(s) for s in all_smiles]\n",
    "selfies_alphabet = set()\n",
    "for s in all_selfies:\n",
    "    selfies_alphabet.update(sf.split_selfies(s))\n",
    "selfies_tokens = [config.PAD_TOKEN, config.SOS_TOKEN, config.EOS_TOKEN, config.MASK_TOKEN] + sorted(selfies_alphabet)\n",
    "token_to_idx = {tok: i for i, tok in enumerate(selfies_tokens)}\n",
    "idx_to_token = {i: tok for tok, i in token_to_idx.items()}\n",
    "vocab_size = len(token_to_idx)\n",
    "PRETRAIN_MAX_LEN = 100\n",
    "SUPERVISED_MAX_LEN = max(len(sf.split_selfies(s)) + 2 for s in all_selfies)\n",
    "print(f\"SELFIES vocabulary size: {vocab_size}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}, Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}\")\n",
    "\n",
    "def encode_selfies(selfies, max_len=PRETRAIN_MAX_LEN):\n",
    "    tokens = [config.SOS_TOKEN] + sf.split_selfies(selfies)[:max_len-2] + [config.EOS_TOKEN]\n",
    "    token_ids = [token_to_idx.get(tok, token_to_idx[config.PAD_TOKEN]) for tok in tokens]\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len]\n",
    "    else:\n",
    "        token_ids += [token_to_idx[config.PAD_TOKEN]] * (max_len - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "def decode_selfies(token_ids):\n",
    "    tokens = [idx_to_token.get(idx, config.PAD_TOKEN) for idx in token_ids]\n",
    "    tokens = [t for t in tokens if t not in {config.PAD_TOKEN, config.SOS_TOKEN, config.EOS_TOKEN}]\n",
    "    selfies_str = ''.join(tokens)\n",
    "    try:\n",
    "        smiles = sf.decoder(selfies_str)\n",
    "        return smiles\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute Morgan fingerprints for all unique SMILES\n",
    "all_smiles = list(set(df_massspecgym['smiles'].tolist() + df_external['smiles'].tolist()))\n",
    "all_fingerprints = {}\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "for smiles in all_smiles:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        all_fingerprints[smiles] = morgan_gen.GetFingerprint(mol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for MS/MS data\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=PRETRAIN_MAX_LEN, is_ssl=False):\n",
    "        self.spectra = np.stack(dataframe['binned'].values)\n",
    "        self.graph_data = dataframe['graph_data'].values\n",
    "        self.ion_modes = dataframe['ion_mode'].values\n",
    "        self.precursor_bins = dataframe['precursor_bin'].values\n",
    "        self.adduct_indices = dataframe['adduct_idx'].values\n",
    "        self.raw_smiles = dataframe['smiles'].values\n",
    "        self.is_ssl = is_ssl\n",
    "        if is_ssl:\n",
    "            self.smiles = []\n",
    "            self.masked_smiles = []\n",
    "            for s in self.raw_smiles:\n",
    "                selfies = sf.encoder(s)\n",
    "                masked_s, orig_s = self.mask_selfies(selfies)\n",
    "                self.smiles.append(encode_selfies(orig_s, max_len))\n",
    "                self.masked_smiles.append(encode_selfies(masked_s, max_len))\n",
    "        else:\n",
    "            self.smiles = [encode_selfies(sf.encoder(s), max_len=SUPERVISED_MAX_LEN) for s in self.raw_smiles]\n",
    "\n",
    "    def mask_selfies(self, selfies, mask_ratio=0.10):\n",
    "        try:\n",
    "            tokens = sf.split_selfies(selfies)[:PRETRAIN_MAX_LEN-2]\n",
    "            masked_tokens = tokens.copy()\n",
    "            n_mask = int(mask_ratio * len(tokens))\n",
    "            if n_mask > 0:\n",
    "                mask_indices = np.random.choice(len(tokens), n_mask, replace=False)\n",
    "                for idx in mask_indices:\n",
    "                    masked_tokens[idx] = config.MASK_TOKEN\n",
    "            return ''.join(masked_tokens), ''.join(tokens)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"mask_selfies failed for {selfies}: {e}\\n{traceback.format_exc()}\")\n",
    "            return selfies, selfies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectra)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_ssl:\n",
    "            return (\n",
    "                torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "                self.graph_data[idx],\n",
    "                torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.masked_smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "                torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "                torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "                self.raw_smiles[idx]\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "            self.graph_data[idx],\n",
    "            torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "            torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "            torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "            self.raw_smiles[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding and model encoder/decoder classes\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Neural Network Models\n",
    "class SpectrumTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.unsqueeze(-1))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SpectrumGNNEncoder(MessagePassing):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.d_model = d_model\n",
    "        self.lin = nn.Linear(1, d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.lin(x)\n",
    "        x = self.propagate(edge_index, x=x)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return self.mlp(x_j)\n",
    "\n",
    "class SmilesTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        output = self.transformer(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return self.output_proj(output)\n",
    "\n",
    "class MSMS2SmilesHybrid(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, **kwargs):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder = SpectrumTransformerEncoder(d_model, nhead, num_layers)\n",
    "        self.gnn_encoder = SpectrumGNNEncoder(d_model)\n",
    "        self.decoder = SmilesTransformerDecoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.fusion = nn.Linear(d_model * 2, d_model)\n",
    "\n",
    "    def forward(self, spectrum, graph_data, tgt, tgt_mask=None):\n",
    "        transformer_out = self.transformer_encoder(spectrum)\n",
    "        gnn_out = self.gnn_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        memory = self.fusion(torch.cat([transformer_out.mean(1), gnn_out], dim=1)).unsqueeze(1)\n",
    "        return self.decoder(tgt, memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def ssl_pretrain(model, dataloader, epochs=3, lr=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f'SSL Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, masked, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device), target.to(device)\n",
    "            graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrum, graph_batch, target[:, :-1])\n",
    "            loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'SSL Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "def supervised_train(model, train_loader, val_loader, epochs=30, lr=1e-4, patience=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device), target.to(device)\n",
    "            graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "            \n",
    "            # Create attention mask\n",
    "            tgt_mask = torch.triu(torch.ones(target.size(1)-1, target.size(1)-1), diagonal=1).bool().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrum, graph_batch, target[:, :-1], tgt_mask=tgt_mask)\n",
    "            loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                spectrum, graph_data, target, _, _, _, _ = batch\n",
    "                spectrum, target = spectrum.to(device), target.to(device)\n",
    "                graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "                output = model(spectrum, graph_batch, target[:, :-1])\n",
    "                loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    return best_val_loss\n",
    "\n",
    "def beam_search(model, spectrum, graph_data, ion_mode, precursor_bin, adduct_idx, true_smiles, beam_width=5, max_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device)\n",
    "        graph_batch = Batch.from_data_list([graph_data]).to(device)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        sequences = [[token_to_idx[config.SOS_TOKEN]]]\n",
    "        scores = [0.0]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            for i, seq in enumerate(sequences):\n",
    "                if seq[-1] == token_to_idx[config.EOS_TOKEN]:\n",
    "                    candidates.append((seq, scores[i]))\n",
    "                    continue\n",
    "                \n",
    "                tgt = torch.tensor([seq]).to(device)\n",
    "                output = model(spectrum, graph_batch, tgt)\n",
    "                probs = F.softmax(output[0, -1], dim=-1)\n",
    "                \n",
    "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                for prob, idx in zip(top_probs, top_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = scores[i] + torch.log(prob).item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            sequences = [seq for seq, _ in candidates[:beam_width]]\n",
    "            scores = [score for _, score in candidates[:beam_width]]\n",
    "        \n",
    "        results = []\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            smiles = decode_selfies(seq)\n",
    "            if smiles:\n",
    "                results.append((smiles, score))\n",
    "        return results[:beam_width]\n",
    "\n",
    "# Missing evaluation functions\n",
    "def mw_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolWt(mol1) - Descriptors.MolWt(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "def logp_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolLogP(mol1) - Descriptors.MolLogP(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "def substructure_match(smiles1, smiles2, substructures=None):\n",
    "    return 0.5  # Placeholder\n",
    "\n",
    "def error_analysis(pred_list, true_list, adduct_list, fingerprints):\n",
    "    print('Error analysis completed')\n",
    "\n",
    "def plot_attention_weights(weights, title='Attention'):\n",
    "    print(f'Attention visualization: {title}')\n",
    "\n",
    "def plot_gnn_edge_weights(weights, edges, title='GNN'):\n",
    "    print(f'GNN visualization: {title}')\n",
    "\n",
    "def calculate_bleu(predicted_smiles, true_smiles):\n",
    "    try:\n",
    "        pred_tokens = list(predicted_smiles)\n",
    "        true_tokens = list(true_smiles)\n",
    "        return sentence_bleu([true_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2, fingerprint_dict):\n",
    "    if smiles1 in fingerprint_dict and smiles2 in fingerprint_dict:\n",
    "        return DataStructs.TanimotoSimilarity(fingerprint_dict[smiles1], fingerprint_dict[smiles2])\n",
    "    return 0.0\n",
    "\n",
    "def validity_rate(smiles_list):\n",
    "    valid = sum(1 for s in smiles_list if Chem.MolFromSmiles(s) is not None)\n",
    "    return (valid / len(smiles_list)) * 100 if smiles_list else 0\n",
    "\n",
    "def objective(trial, train_data, val_data):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    return lr  # Simplified for demo\n",
    "\n",
    "# Additional metrics and visualization\n",
    "def dice_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            fp1 = Chem.RDKFingerprint(mol1)\n",
    "            fp2 = Chem.RDKFingerprint(mol2)\n",
    "            return DataStructs.DiceSimilarity(fp1, fp2)\n",
    "    except: pass\n",
    "    return 0.0\n",
    "\n",
    "def mcs_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            mcs = rdFMCS.FindMCS([mol1, mol2])\n",
    "            return mcs.numAtoms / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    except: pass\n",
    "    return 0.0\n",
    "\n",
    "def prediction_diversity(smiles_list):\n",
    "    unique_smiles = set(smiles_list)\n",
    "    return len(unique_smiles) / len(smiles_list) if smiles_list else 0\n",
    "\n",
    "def plot_molecular_comparison(true_smiles, pred_smiles, title='Comparison'):\n",
    "    try:\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "        pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "        if true_mol and pred_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.axis('off')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "    except Exception as e: print(f'Visualization error: {e}')\n",
    "\n",
    "# Model checkpointing\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'vocab_size': vocab_size,\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Data validation functions\n",
    "def validate_spectrum_quality(mzs, intensities, min_peaks=5, max_mz_range=2000):\n",
    "    if len(mzs) < min_peaks or max(mzs) > max_mz_range:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_molecular_properties(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return False\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        return 50 <= mw <= 1000\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def remove_duplicates(df, subset=['smiles', 'precursor_mz']):\n",
    "    return df.drop_duplicates(subset=subset, keep='first')\n",
    "\n",
    "# Memory management\n",
    "def clear_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration: XGBoost + RAG + Deep Learning\n",
    "class HybridPredictor:\n",
    "    def __init__(self, dl_model, xgb_model, rag_system, label_encoder):\n",
    "        self.dl_model = dl_model\n",
    "        self.xgb_model = xgb_model\n",
    "        self.rag_system = rag_system\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def predict_ensemble(self, spectrum, graph_data, features, query_text=None, weights=[0.5, 0.3, 0.2]):\n",
    "        # Validate weights\n",
    "        if abs(sum(weights) - 1.0) > 0.01:\n",
    "            weights = [w/sum(weights) for w in weights]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Deep learning prediction with error handling\n",
    "        try:\n",
    "            dl_results = beam_search(self.dl_model, spectrum, graph_data, 0, 0, 0, '', beam_width=5, device=device)\n",
    "            if dl_results and dl_results[0][0]:\n",
    "                predictions.append(('DL', dl_results[0][0], weights[0]))\n",
    "        except Exception as e:\n",
    "            print(f'DL prediction failed: {e}')\n",
    "        \n",
    "        # XGBoost prediction with error handling\n",
    "        try:\n",
    "            xgb_pred = self.xgb_model.predict([features])[0]\n",
    "            xgb_smiles = self.label_encoder.inverse_transform([xgb_pred])[0]\n",
    "            predictions.append(('XGB', xgb_smiles, weights[1]))\n",
    "        except Exception as e:\n",
    "            print(f'XGBoost prediction failed: {e}')\n",
    "        \n",
    "        # RAG prediction with error handling\n",
    "        if query_text:\n",
    "            try:\n",
    "                rag_results = self.rag_system.semantic_search(query_text, k=1)\n",
    "                if rag_results and len(rag_results) > 0:\n",
    "                    predictions.append(('RAG', rag_results[0]['smiles'], weights[2]))\n",
    "            except Exception as e:\n",
    "                print(f'RAG prediction failed: {e}')\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def evaluate_ensemble(self, test_data, n_samples=10):\n",
    "        results = {'dl': [], 'xgb': [], 'rag': [], 'ensemble': []}\n",
    "        \n",
    "        for i in range(min(n_samples, len(test_data))):\n",
    "            row = test_data.iloc[i]\n",
    "            true_smiles = row['smiles']\n",
    "            \n",
    "            # Extract features\n",
    "            spectrum = row['binned']\n",
    "            graph_data = row['graph_data']\n",
    "            features = [np.mean(spectrum), np.std(spectrum), np.max(spectrum), \n",
    "                       np.sum(spectrum > 0.1), row['precursor_mz'], row['ion_mode'], \n",
    "                       row['adduct_idx'], len(row['mzs'])]\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            preds = self.predict_ensemble(spectrum, graph_data, features, f\"molecule with MW {row['precursor_mz']:.1f}\")\n",
    "            \n",
    "            # Evaluate each method\n",
    "            for method, pred_smiles, weight in preds:\n",
    "                similarity = tanimoto_similarity(pred_smiles, true_smiles, all_fingerprints)\n",
    "                results[method.lower()].append(similarity)\n",
    "            \n",
    "            # Weighted ensemble score\n",
    "            ensemble_score = sum(tanimoto_similarity(pred, true_smiles, all_fingerprints) * w for _, pred, w in preds) / sum(w for _, _, w in preds)\n",
    "            results['ensemble'].append(ensemble_score)\n",
    "        \n",
    "        return {k: np.mean(v) if v else 0 for k, v in results.items()}\n",
    "\n",
    "print('Integration system ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb652e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation, training, and evaluation loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "external_dataset = MSMSDataset(df_external, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=32, num_workers=2)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_massspecgym)):\n",
    "    print(f\"\\nFold {fold+1}/5\")\n",
    "    train_data = df_massspecgym.iloc[train_idx]\n",
    "    val_data = df_massspecgym.iloc[val_idx]\n",
    "    ssl_data = train_data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    ssl_dataset = MSMSDataset(ssl_data, max_len=PRETRAIN_MAX_LEN, is_ssl=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
    "    ssl_loader = DataLoader(ssl_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_data, val_data), n_trials=10)\n",
    "    best_lr = study.best_params['lr']\n",
    "    print(f\"Best learning rate for fold {fold+1}: {best_lr:.6f}\")\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=config.D_MODEL, nhead=config.NHEAD, num_layers=config.NUM_LAYERS).to(device)\n",
    "    print(f\"Starting SSL pretraining for fold {fold+1}...\")\n",
    "    ssl_pretrain(model, ssl_loader, epochs=3, lr=best_lr)\n",
    "    print(f\"Starting supervised training for fold {fold+1}...\")\n",
    "    best_val_loss = supervised_train(model, train_loader, val_loader, epochs=30, lr=best_lr, patience=5)\n",
    "    fold_results.append(best_val_loss)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, f'best_msms_hybrid_fold_{fold+1}.pt')\n",
    "\n",
    "print(f\"Cross-validation results: {fold_results}\")\n",
    "print(f\"Average validation loss: {np.mean(fold_results):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dataset evaluation and visualization\n",
    "model.eval()\n",
    "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
    "pred_smiles_list = []\n",
    "true_smiles_list = []\n",
    "adducts_list = []\n",
    "num_samples = min(5, len(external_dataset))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample_spectrum = external_dataset[sample_idx][0]\n",
    "    sample_graph = external_dataset[sample_idx][1]\n",
    "    sample_ion_mode = external_dataset[sample_idx][3]\n",
    "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
    "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
    "    true_smiles = external_dataset[sample_idx][6]\n",
    "\n",
    "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
    "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
    "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
    "\n",
    "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
    "    print(\"Top Predicted SMILES:\")\n",
    "    for smiles, confidence in predicted_results[:3]:\n",
    "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
    "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
    "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
    "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
    "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
    "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
    "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
    "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "        if smiles != \"Invalid SMILES\":\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
    "\n",
    "    # Visualize molecules\n",
    "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
    "        if pred_mol and true_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            img_array = np.array(img.convert('RGB'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(img_array)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Visualize attention and GNN weights for first sample\n",
    "    if sample_idx == 0:\n",
    "        with torch.no_grad():\n",
    "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
    "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
    "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
    "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
    "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
    "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
    "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
    "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
    "print(\"External Metrics Summary:\")\n",
    "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
    "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
    "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
    "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
    "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
