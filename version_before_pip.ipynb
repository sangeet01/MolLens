{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69c2a40",
   "metadata": {},
   "source": [
    "# MS-to-Structure Deep Learning Pipeline \n",
    "\n",
    "Steps: Step-by-Step Pipeline Overview\n",
    "1. Setup & Environment (Cells 1-3)\n",
    "Installs required packages (PyTorch, RDKit, XGBoost, FAISS, etc.)\n",
    "Configures GPU optimization for RTX 3080 Ti\n",
    "Sets random seeds for reproducibility\n",
    "Defines special tokens (PAD, SOS, EOS, MASK)\n",
    "\n",
    "2. Data Loading & Configuration (Cell 4)\n",
    "Loads MassSpecGym dataset (231K samples total)\n",
    "Splits into training (208K) and external test (23K) sets\n",
    "Configures hyperparameters (model dimensions, epochs, batch sizes)\n",
    "Inspects dataset structure (m/z values, intensities, SMILES, adducts)\n",
    "\n",
    "3. Data Preprocessing (Cells 5-7)\n",
    "SMILES Processing : Canonicalizes and augments SMILES with stereoisomers\n",
    "Spectrum Binning : Converts raw m/z peaks into 1000-bin vectors\n",
    "Graph Creation : Builds molecular graphs from spectra\n",
    "Feature Engineering : Extracts ion modes, precursor m/z, adduct types\n",
    "Data Cleaning : Handles shape issues and missing values\n",
    "\n",
    "4. XGBoost Baseline (Cell 8)\n",
    "Extracts numerical features from spectra (mean, std, max intensity, peak count)\n",
    "Encodes SMILES strings as classification targets\n",
    "Trains gradient boosting model on spectral features\n",
    "Provides feature importance analysis\n",
    "\n",
    "5. RAG System (Cells 10-15)\n",
    "Molecular Indexing : Creates semantic embeddings of molecular descriptions\n",
    "Fingerprint Database : Builds Morgan fingerprint index for structure similarity\n",
    "\n",
    "Search Capabilities :\n",
    "Semantic search using sentence transformers\n",
    "Structure-based search using Tanimoto similarity\n",
    "Hybrid search combining both approaches\n",
    "FAISS Integration : Enables fast similarity search\n",
    "\n",
    "6. Deep Learning Architecture (Cells 16-20)\n",
    "SELFIES Tokenization : Converts SMILES to robust molecular tokens\n",
    "\n",
    "Hybrid Model :\n",
    "Transformer Encoder : Processes binned spectra with attention\n",
    "GNN Encoder : Analyzes molecular graph structure\n",
    "Fusion Layer : Combines transformer and GNN representations\n",
    "Transformer Decoder : Generates SMILES sequences\n",
    "\n",
    "Training Pipeline :\n",
    "Self-supervised pretraining with masked language modeling\n",
    "Supervised fine-tuning with cross-entropy loss\n",
    "Beam search for inference\n",
    "\n",
    "7. Training & Validation (Cells 21-22)\n",
    "Cross-Validation : 5-fold CV for robust evaluation\n",
    "Hyperparameter Optimization : Uses Optuna for learning rate tuning\n",
    "Memory Management : Handles OOM errors with batch size reduction\n",
    "Model Checkpointing : Saves best models for each fold\n",
    "\n",
    "8. Evaluation & Analysis (Cells 23-24)\n",
    "Multiple Metrics : Tanimoto similarity, Dice coefficient, MCS overlap\n",
    "Molecular Properties : MW and LogP difference analysis\n",
    "Visualization : Attention weights, molecular structures, performance plots\n",
    "Error Analysis : Identifies failure modes and improvement areas\n",
    "\n",
    "9. Integration System (Cell 21)\n",
    "Ensemble Prediction : Combines XGBoost, Deep Learning, and RAG\n",
    "Weighted Scoring : Balances different prediction approaches\n",
    "Confidence Estimation : Provides uncertainty quantification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad17c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: torch_geometric in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: rdkit-pypi in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (2022.9.5)\n",
      "Requirement already satisfied: selfies in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: datasets in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: optuna in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (4.4.0)\n",
      "Requirement already satisfied: nltk in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: python-Levenshtein in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: tqdm in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: xgboost in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: faiss-cpu in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: filelock in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from triton==3.3.1->torch) (78.1.1)\n",
      "Requirement already satisfied: aiohttp in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch_geometric) (3.12.13)\n",
      "Requirement already satisfied: numpy in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch_geometric) (2.2.6)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: Pillow in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from rdkit-pypi) (11.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: click in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
      "Requirement already satisfied: Mako in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: tomli in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "#cell1\n",
    "# Install required packages for MS-to-Structure pipeline\n",
    "! pip install torch torch_geometric rdkit-pypi selfies datasets optuna nltk python-Levenshtein tqdm scikit-learn matplotlib xgboost faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f1ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangeet/miniconda3/envs/naturems/lib/python3.10/site-packages/torch/__config__.py:9: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._show_config()\n"
     ]
    }
   ],
   "source": [
    "#cell 2\n",
    "# Import libraries and set up logging for Jupyter compatibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
    "from rdkit.Chem.EnumerateStereoisomers import StereoEnumerationOptions\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import RDLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import optuna\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance\n",
    "import logging\n",
    "import traceback\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setup logging for Jupyter (prints to stdout)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed6ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 3\n",
    "# Set random seed for reproducibility and define global variables\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "\n",
    "# GPU optimization for RTX 3080 Ti\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdb68fb-0fe7-498d-99d4-5368de9b337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 231104 samples\n",
      "MassSpecGym size: 207993 External test size: 23111\n",
      "Dataset Columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge']\n",
      "\n",
      "First few rows of MassSpecGym dataset:\n",
      "             identifier                                                mzs  \\\n",
      "0  MassSpecGymID0000001  91.0542,125.0233,154.0499,155.0577,185.0961,20...   \n",
      "1  MassSpecGymID0000002  91.0542,125.0233,155.0577,185.0961,229.0859,24...   \n",
      "2  MassSpecGymID0000003  69.0343,91.0542,125.0233,127.039,153.0699,154....   \n",
      "3  MassSpecGymID0000004  69.0343,91.0542,110.06,111.0441,112.0393,120.0...   \n",
      "4  MassSpecGymID0000005  91.0542,125.0233,185.0961,229.0859,246.1125,28...   \n",
      "\n",
      "                                         intensities  \\\n",
      "0  0.24524524524524524,1.0,0.08008008008008008,0....   \n",
      "1  0.0990990990990991,0.28128128128128127,0.04004...   \n",
      "2  0.03403403403403404,0.31431431431431434,1.0,0....   \n",
      "3  0.17917917917917917,0.47347347347347346,0.0380...   \n",
      "4  0.07807807807807808,0.1841841841841842,0.03503...   \n",
      "\n",
      "                                          smiles  adduct  precursor_mz  \n",
      "0  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "1  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "2  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "3  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "4  CC(=O)N[C@@H](CC1=CC=CC=C1)C2=CC(=CC(=O)O2)OC  [M+H]+      288.1225  \n",
      "\n",
      "Unique adduct values: ['[M+H]+' '[M+Na]+']\n"
     ]
    }
   ],
   "source": [
    "#cell 4\n",
    "# Production Configuration\n",
    "class Config:\n",
    "    DATASET_PATH = '/home/sangeet/dataset'  # Change to your dataset path\n",
    "    TRAIN_SPLIT = 0.9\n",
    "    RANDOM_SEED = 42\n",
    "    N_BINS = 1000\n",
    "    MAX_MZ = 1000\n",
    "    NOISE_LEVEL = 0.05\n",
    "    MAX_ISOMERS = 8\n",
    "    D_MODEL = 512\n",
    "    NHEAD = 8\n",
    "    NUM_LAYERS = 6\n",
    "    BATCH_SIZE = 64\n",
    "    SSL_EPOCHS = 3\n",
    "    SUPERVISED_EPOCHS = 30\n",
    "    LEARNING_RATE = 1e-4\n",
    "    PATIENCE = 5\n",
    "    N_FOLDS = 5\n",
    "    # Token definitions\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    SOS_TOKEN = '< SOS >'\n",
    "    EOS_TOKEN = '<EOS>'\n",
    "    MASK_TOKEN = '[MASK]'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Load dataset with configurable path\n",
    "try:\n",
    "    dataset = load_dataset(config.DATASET_PATH, split='train')\n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(f'Loaded dataset with {len(df)} samples')\n",
    "except Exception as e:\n",
    "    print(f'Error loading dataset: {e}')\n",
    "    print('Please update config.DATASET_PATH')\n",
    "    raise\n",
    "\n",
    "# Split dataset based on configuration\n",
    "split_idx = int(config.TRAIN_SPLIT * len(df))\n",
    "df_massspecgym = df.iloc[:split_idx].copy()\n",
    "df_external = df.iloc[split_idx:].copy()\n",
    "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
    "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
    "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7780db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 5\n",
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=None):\n",
    "    max_isomers = max_isomers or config.MAX_ISOMERS\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.StereoEnumerationOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [\n",
    "                Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers\n",
    "                ]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n",
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=None):\n",
    "    max_isomers = max_isomers or config.MAX_ISOMERS\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.StereoEnumerationOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [\n",
    "                Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers\n",
    "                ]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d72b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 6\n",
    "# Apply canonicalization, augmentation, and binning to the dataframe\n",
    "# Preprocess ion mode, precursor m/z, and adducts\n",
    "df_massspecgym['smiles'] = df_massspecgym['smiles'].apply(canonicalize_smiles)\n",
    "df_external['smiles'] = df_external['smiles'].apply(canonicalize_smiles)\n",
    "df_massspecgym = df_massspecgym.dropna(subset=['smiles'])\n",
    "df_external = df_external.dropna(subset=['smiles'])\n",
    "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].apply(augment_smiles)\n",
    "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list']).rename(columns={'smiles_list': 'smiles'})\n",
    "\n",
    "df_massspecgym['ion_mode'] = df_massspecgym['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_massspecgym['precursor_bin'] = pd.qcut(df_massspecgym['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "df_external['ion_mode'] = df_external['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_external['precursor_bin'] = pd.qcut(df_external['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx)\n",
    "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
    "\n",
    "def safe_bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct):\n",
    "    try:\n",
    "        # Clean mzs and intensities to remove non-numeric values\n",
    "        mzs_clean = [float(x) for x in mzs if isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','',1).replace('-','',1).isdigit())]\n",
    "        intensities_clean = [float(x) for x in intensities if isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','',1).replace('-','',1).isdigit())]\n",
    "        return bin_spectrum_to_graph(mzs_clean, intensities_clean, ion_mode, precursor_mz, adduct)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Skipping value error in safe_bin_spectrum_to_graph: {e}\")\n",
    "        return np.zeros(100), None\n",
    "\n",
    "df_massspecgym[['binned', 'graph_data']] = df_massspecgym.apply(\n",
    "    lambda row: pd.Series(safe_bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    "    )\n",
    "df_external[['binned', 'graph_data']] = df_external.apply(\n",
    "    lambda row: pd.Series(safe_bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xgb_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing df_massspecgym['smiles']...\n",
      "Initial column shape: (562533, 2)\n",
      "Sample value: smiles     COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc(oc(c1)=O)[C@@H](NC(C)=O)Cc1ccccc1\n",
      "Name: 0, dtype: object\n",
      "Final column shape: (562533,)\n",
      "Final sample value: COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "\n",
      "Processing df_external['smiles']...\n",
      "Initial column shape: (23111,)\n",
      "Sample value: CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(OC)c(OC)c1-2)C(=O)Nc1nc(C(=O)OC)c(-c2ccccc2)s1\n",
      "Final column shape: (23111,)\n",
      "Final sample value: CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(OC)c(OC)c1-2)C(=O)Nc1nc(C(=O)OC)c(-c2ccccc2)s1\n",
      "\n",
      "Final verification:\n",
      "df_massspecgym['smiles'] shape: (562533, 2)\n",
      "df_external['smiles'] shape: (23111,)\n"
     ]
    }
   ],
   "source": [
    "#cell 7\n",
    "\n",
    "# Fix SMILES column shape issue\n",
    "# Ensure SMILES column is 1D and contains only strings\n",
    "def flatten_smiles_column(col):\n",
    "    # Debug print\n",
    "    print(f\"Initial column shape: {col.values.shape if hasattr(col.values, 'shape') else 'no shape'}\")\n",
    "    print(f\"Sample value: {col.iloc[0]}\")\n",
    "    \n",
    "    # Force to Series if DataFrame\n",
    "    if isinstance(col, pd.DataFrame):\n",
    "        col = col.iloc[:, 0]\n",
    "    \n",
    "    # Force 2D array to 1D\n",
    "    if hasattr(col.values, 'shape') and len(col.values.shape) > 1:\n",
    "        col = pd.Series(col.values.ravel())\n",
    "    \n",
    "    # Flatten any remaining sequences in cells\n",
    "    col = col.apply(lambda x: str(x[0]) if isinstance(x, (list, tuple, np.ndarray)) else str(x))\n",
    "    \n",
    "    # Debug print\n",
    "    print(f\"Final column shape: {col.values.shape if hasattr(col.values, 'shape') else 'no shape'}\")\n",
    "    print(f\"Final sample value: {col.iloc[0]}\")\n",
    "    \n",
    "    return col\n",
    "\n",
    "# Reset the column to ensure clean state\n",
    "df_massspecgym = df_massspecgym.copy()\n",
    "df_external = df_external.copy()\n",
    "\n",
    "print(\"Processing df_massspecgym['smiles']...\")\n",
    "df_massspecgym['smiles'] = flatten_smiles_column(df_massspecgym['smiles'])\n",
    "\n",
    "print(\"\\nProcessing df_external['smiles']...\")\n",
    "df_external['smiles'] = flatten_smiles_column(df_external['smiles'])\n",
    "\n",
    "# Verify the columns are 1D before LabelEncoder\n",
    "print(\"\\nFinal verification:\")\n",
    "print(f\"df_massspecgym['smiles'] shape: {df_massspecgym['smiles'].values.shape}\")\n",
    "print(f\"df_external['smiles'] shape: {df_external['smiles'].values.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "xgb_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened SMILES shape: (562533,)\n",
      "Example: COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "y_all shape: (562533,)\n",
      "Number of unique labels: 25944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (562533, 8)\n",
      "Filtered X_all shape: (49994, 8)\n",
      "Filtered number of unique labels: 1012\n",
      "Train classes: 1012\n",
      "Test classes: 992\n",
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.6966\n",
      "mean_intensity: 0.0061\n",
      "std_intensity: 0.0103\n",
      "max_intensity: 0.0056\n",
      "peak_count: 0.0056\n",
      "precursor_mz: 0.5358\n",
      "ion_mode: 0.0000\n",
      "adduct_idx: 0.4180\n",
      "spectrum_length: 0.0185\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "# Extract features and prepare XGBoost data\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Flatten SMILES column\n",
    "# ----------------------------\n",
    "smiles_flat = df_massspecgym['smiles'].iloc[:, 0].astype(str)\n",
    "print(\"Flattened SMILES shape:\", smiles_flat.shape)\n",
    "print(\"Example:\", smiles_flat.iloc[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Encode labels\n",
    "# ----------------------------\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(smiles_flat)\n",
    "print(\"y_all shape:\", y_all.shape)\n",
    "print(\"Number of unique labels:\", len(np.unique(y_all)))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Feature extraction\n",
    "# ----------------------------\n",
    "def extract_features(df):\n",
    "    feats = []\n",
    "    for _, row in df.iterrows():\n",
    "        spectrum = np.array(row['binned'], dtype=np.float32)\n",
    "        mzs = row['mzs']\n",
    "        if isinstance(mzs, str):\n",
    "            mzs_list = [m for m in mzs.split(',') if m]\n",
    "        else:\n",
    "            mzs_list = mzs if isinstance(mzs, (list, tuple, np.ndarray)) else []\n",
    "        mz_len = len(mzs_list)\n",
    "\n",
    "        feat = [\n",
    "            float(np.mean(spectrum)),\n",
    "            float(np.std(spectrum)),\n",
    "            float(np.max(spectrum)),\n",
    "            float(np.sum(spectrum > 0.1)),\n",
    "            float(row['precursor_mz']),\n",
    "            float(row['ion_mode']),\n",
    "            float(row['adduct_idx']),\n",
    "            float(mz_len)\n",
    "        ]\n",
    "        feats.append(feat)\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "X_all = extract_features(df_massspecgym)\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Subsample if needed\n",
    "# ----------------------------\n",
    "subset_size = min(50000, X_all.shape[0])\n",
    "X_all = X_all[:subset_size]\n",
    "smiles_subset = smiles_flat[:subset_size]\n",
    "\n",
    "# Re-encode labels after subsetting\n",
    "le_subset = LabelEncoder()\n",
    "y_all = le_subset.fit_transform(smiles_subset)\n",
    "\n",
    "# Filter out classes with fewer than 2 samples\n",
    "from collections import Counter\n",
    "class_counts = Counter(y_all)\n",
    "valid_classes = [cls for cls, count in class_counts.items() if count >= 2]\n",
    "mask = np.isin(y_all, valid_classes)\n",
    "\n",
    "X_all = X_all[mask]\n",
    "y_all = y_all[mask]\n",
    "smiles_subset = smiles_subset[mask]\n",
    "\n",
    "# NEW: Re-encode labels after filtering to ensure consecutive integers\n",
    "le_final = LabelEncoder()\n",
    "y_all = le_final.fit_transform(smiles_subset)  # Re-encode filtered SMILES\n",
    "\n",
    "print(\"Filtered X_all shape:\", X_all.shape)\n",
    "print(\"Filtered number of unique labels:\", len(np.unique(y_all)))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train/test split (stratified)\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train).size)\n",
    "print(\"Test classes:\", np.unique(y_test).size)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) XGBoost model setup\n",
    "# ----------------------------\n",
    "n_classes = len(np.unique(y_train))\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    num_class=n_classes  # Explicitly set number of classes\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Train model\n",
    "# ----------------------------\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Predict & evaluate\n",
    "# ----------------------------\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Feature importances\n",
    "# ----------------------------\n",
    "feature_names = [\n",
    "    'mean_intensity', 'std_intensity', 'max_intensity', 'peak_count',\n",
    "    'precursor_mz', 'ion_mode', 'adduct_idx', 'spectrum_length'\n",
    "]\n",
    "\n",
    "for name, val in zip(feature_names, xgb_model.feature_importances_):\n",
    "    print(f\"{name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "xgb_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "True: C/C=C1/[C@H](O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]2O)OC=C(C(=O)OC)[C@H]1CC(=O)O[C@@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O\n",
      "Pred: C/C=C1/[C@H](O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]2O)OC=C(C(=O)OC)[C@H]1CC(=O)O[C@@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O\n",
      "Match: True\n",
      "\n",
      "True: C/C=C1/C[C@@]2(CO)O[C@@]2(C)C(=O)OCC2=CCN3CC[C@@H](OC1=O)[C@@H]23\n",
      "Pred: C/C=C1/C[C@@]2(CO)O[C@@]2(C)C(=O)OCC2=CCN3CC[C@@H](OC1=O)[C@@H]23\n",
      "Match: True\n",
      "\n",
      "True: C=C1CCCC2C1(C)CCC(C)C2(C)CC1=C(O)C(NCCC(=O)O)=CC(=O)C1=O\n",
      "Pred: C=C1C(O)CCC2(C)C1CCC1=C3CCC(C(C)CCC(C)C(C)C)C3(C)CCC12\n",
      "Match: False\n",
      "\n",
      "True: C=C(C)[C@H]1COc2cc3oc(=O)ccc3cc2O1\n",
      "Pred: C=C(C)[C@H]1COc2cc3oc(=O)ccc3cc2O1\n",
      "Match: True\n",
      "\n",
      "True: C=C(C(=O)O)C1CCC(C)C2CCC(=O)OC2(C)C1\n",
      "Pred: C=C1/C=C/C(=O)N(C)CC(=O)N[C@@H]([C@H](O)C(N)=O)C(=O)OC([C@H](C)CCCCCC)[C@H](C)C(=O)N1\n",
      "Match: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cell 9\n",
    "# Display results\n",
    "print('\\nSample predictions:')\n",
    "for i in range(min(5, len(y_test))):\n",
    "    true_smiles = le.inverse_transform([y_test[i]])[0]\n",
    "    pred_smiles = le.inverse_transform([y_pred[i]])[0]\n",
    "    print(f'True: {true_smiles}')\n",
    "    print(f'Pred: {pred_smiles}')\n",
    "    print(f'Match: {true_smiles == pred_smiles}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4041421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:56:43,199 INFO Use pytorch device_name: cpu\n",
      "2025-10-08 02:56:43,200 INFO Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing enhanced RAG system...\n",
      "Building semantic index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7359e2a6c80a4f2ba7a87186478633c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building fingerprint index...\n",
      "RAG system ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:55:43] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[03:55:43] SMILES Parse Error: check for mistakes around position 2:\n",
      "[03:55:43] smiles\n",
      "[03:55:43] ~^\n",
      "[03:55:43] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n",
      "[03:55:43] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[03:55:43] SMILES Parse Error: check for mistakes around position 2:\n",
      "[03:55:43] smiles\n",
      "[03:55:43] ~^\n",
      "[03:55:43] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n"
     ]
    }
   ],
   "source": [
    "#cell 10\n",
    "# Enhanced RAG System for Molecular Data\n",
    "class MolecularRAG:\n",
    "    def __init__(self, df):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.df = df.copy()\n",
    "        self.morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "        self.build_molecular_descriptions()\n",
    "        self.build_index()\n",
    "        self.build_fingerprint_index()\n",
    "\n",
    "    def get_molecular_properties(self, smiles):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                mw = Descriptors.MolWt(mol)\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                hbd = Descriptors.NumHDonors(mol)\n",
    "                hba = Descriptors.NumHAcceptors(mol)\n",
    "                rings = Descriptors.RingCount(mol)\n",
    "                aromatic = Descriptors.NumAromaticRings(mol)\n",
    "                return {'mw': mw, 'logp': logp, 'hbd': hbd, 'hba': hba, 'rings': rings, 'aromatic': aromatic}\n",
    "        except:\n",
    "            pass\n",
    "        return {'mw': 0, 'logp': 0, 'hbd': 0, 'hba': 0, 'rings': 0, 'aromatic': 0}\n",
    "\n",
    "    def build_molecular_descriptions(self):\n",
    "        descriptions = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            props = self.get_molecular_properties(row['smiles'])\n",
    "            desc = f\"Molecule with SMILES {row['smiles']}. \"\n",
    "            desc += f\"Molecular weight: {props['mw']:.1f} Da. \"\n",
    "            desc += f\"LogP: {props['logp']:.2f}. \"\n",
    "            desc += f\"H-bond donors: {props['hbd']}, acceptors: {props['hba']}. \"\n",
    "            desc += f\"Contains {props['rings']} rings, {props['aromatic']} aromatic. \"\n",
    "            desc += f\"Adduct: {row['adduct']}, precursor m/z: {row['precursor_mz']:.2f}. \"\n",
    "            desc += f\"Ion mode: {'positive' if row['ion_mode'] == 0 else 'negative'}.\"\n",
    "            descriptions.append(desc)\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def build_index(self):\n",
    "        print('Building semantic index...')\n",
    "        self.embeddings = self.encoder.encode(self.descriptions, show_progress_bar=True)\n",
    "        self.semantic_index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.semantic_index.add(self.embeddings.astype('float32'))\n",
    "\n",
    "    def build_fingerprint_index(self):\n",
    "        print('Building fingerprint index...')\n",
    "        fingerprints = []\n",
    "        for smiles in self.df['smiles']:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                fp = self.morgan_gen.GetFingerprint(mol)\n",
    "                fp_array = np.zeros(2048)\n",
    "                DataStructs.ConvertToNumpyArray(fp, fp_array)\n",
    "                fingerprints.append(fp_array)\n",
    "            else:\n",
    "                fingerprints.append(np.zeros(2048))\n",
    "        self.fingerprints = np.array(fingerprints)\n",
    "        self.fp_index = faiss.IndexFlatIP(2048)\n",
    "        self.fp_index.add(self.fingerprints.astype('float32'))\n",
    "\n",
    "    def semantic_search(self, query, k=5):\n",
    "        query_emb = self.encoder.encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        scores, indices = self.semantic_index.search(query_emb.astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'score': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz'],\n",
    "                'description': self.descriptions[idx]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def structure_search(self, query_smiles, k=5):\n",
    "        mol = Chem.MolFromSmiles(query_smiles)\n",
    "        if not mol:\n",
    "            return []\n",
    "        query_fp = self.morgan_gen.GetFingerprint(mol)\n",
    "        query_array = np.zeros(2048)\n",
    "        DataStructs.ConvertToNumpyArray(query_fp, query_array)\n",
    "        scores, indices = self.fp_index.search(query_array.reshape(1, -1).astype('float32'), k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            row = self.df.iloc[idx]\n",
    "            results.append({\n",
    "                'smiles': row['smiles'],\n",
    "                'tanimoto': scores[0][i],\n",
    "                'adduct': row['adduct'],\n",
    "                'precursor_mz': row['precursor_mz']\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query, query_smiles=None, k=5, alpha=0.7):\n",
    "        semantic_results = self.semantic_search(query, k*2)\n",
    "        if query_smiles:\n",
    "            structure_results = self.structure_search(query_smiles, k*2)\n",
    "            # Combine scores\n",
    "            combined = {}\n",
    "            for r in semantic_results:\n",
    "                combined[r['smiles']] = {'semantic': r['score'], 'structure': 0, 'data': r}\n",
    "            for r in structure_results:\n",
    "                if r['smiles'] in combined:\n",
    "                    combined[r['smiles']]['structure'] = r['tanimoto']\n",
    "                else:\n",
    "                    combined[r['smiles']] = {'semantic': 0, 'structure': r['tanimoto'], 'data': r}\n",
    "            # Hybrid scoring\n",
    "            for smiles in combined:\n",
    "                combined[smiles]['hybrid_score'] = alpha * combined[smiles]['semantic'] + (1-alpha) * combined[smiles]['structure']\n",
    "            sorted_results = sorted(combined.items(), key=lambda x: x[1]['hybrid_score'], reverse=True)\n",
    "            return [{'smiles': smiles, 'hybrid_score': data['hybrid_score'], 'semantic_score': data['semantic'], 'structure_score': data['structure']} for smiles, data in sorted_results[:k]]\n",
    "        return semantic_results[:k]\n",
    "\n",
    "print('Initializing enhanced RAG system...')\n",
    "rag_system = MolecularRAG(df_massspecgym)\n",
    "print('RAG system ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rag_semantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: aromatic compound with hydroxyl group\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae8bdcf186a4757bbf41a783deaf370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "Name: 99678, dtype: object (Score: 0.3758)\n",
      "   Adduct: [M+H]+, m/z: 180.08\n",
      "2. SMILES: smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "smiles    c1ccc2c(c1)cnc1ccccc12\n",
      "Name: 99679, dtype: object (Score: 0.3756)\n",
      "   Adduct: [M+H]+, m/z: 180.08\n",
      "3. SMILES: smiles    CN(C)c1cccc(Br)c1\n",
      "smiles    CN(C)c1cccc(Br)c1\n",
      "Name: 52971, dtype: object (Score: 0.3739)\n",
      "   Adduct: [M+H]+, m/z: 200.01\n",
      "\n",
      "Query: small molecule with high logP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8d2edaa37f43f4b015cd84481fb4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "2. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "3. SMILES: smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "smiles    C[C@@H]1CC[C@@]23CC[C@]4(C)[C@@](C=CC5[C@@]6(C...\n",
      "Name: 112392, dtype: object (Score: 0.4392)\n",
      "   Adduct: [M+H]+, m/z: 455.35\n",
      "\n",
      "Query: compound with multiple rings and nitrogen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca898c08b4e5433cb7d17fee419f42d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16817, dtype: object (Score: 0.4792)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n",
      "2. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16815, dtype: object (Score: 0.4789)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n",
      "3. SMILES: smiles    Nc1nc(N)nc(N)n1\n",
      "smiles    Nc1nc(N)nc(N)n1\n",
      "Name: 16812, dtype: object (Score: 0.4781)\n",
      "   Adduct: [M+H]+, m/z: 127.07\n"
     ]
    }
   ],
   "source": [
    "#cell 11\n",
    "# Semantic Search Examples\n",
    "queries = [\n",
    "    'aromatic compound with hydroxyl group',\n",
    "    'small molecule with high logP',\n",
    "    'compound with multiple rings and nitrogen'\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f'\\nQuery: {query}')\n",
    "    results = rag_system.semantic_search(query, k=3)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'{i+1}. SMILES: {result[\"smiles\"]} (Score: {result[\"score\"]:.4f})')\n",
    "        print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure search for: c1ccccc1O\n",
      "1. SMILES: smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: 1, dtype: object (Tanimoto: 0.0000)\n",
      "   Adduct: [M+H]+, m/z: 288.12\n",
      "2. SMILES: smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "smiles    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: 0, dtype: object (Tanimoto: 0.0000)\n",
      "   Adduct: [M+H]+, m/z: 288.12\n",
      "3. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n",
      "4. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n",
      "5. SMILES: smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "smiles    CCC(C)C(Nc1ccc2c(cc1=O)C(NC(C)=O)CCc1cc(OC)c(O...\n",
      "Name: 207992, dtype: object (Tanimoto: -340282346638528859811704183484516925440.0000)\n",
      "   Adduct: [M+Na]+, m/z: 737.26\n"
     ]
    }
   ],
   "source": [
    "#cell 12\n",
    "# Structure-based Search\n",
    "query_smiles = 'c1ccccc1O'  # phenol\n",
    "print(f'Structure search for: {query_smiles}')\n",
    "results = rag_system.structure_search(query_smiles, k=5)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'{i+1}. SMILES: {result[\"smiles\"]} (Tanimoto: {result[\"tanimoto\"]:.4f})')\n",
    "    print(f'   Adduct: {result[\"adduct\"]}, m/z: {result[\"precursor_mz\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rag_hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 13\n",
    "# Hybrid Search\n",
    "query = 'compound with multiple rings and nitrogen'\n",
    "def hybrid_search(self, text_query, structure_query, k=5):\n",
    "    # 1 Get semantic and structure search results\n",
    "    semantic_results = self.semantic_search(text_query, k=k)\n",
    "    structure_results = self.structure_search(structure_query, k=k)\n",
    "\n",
    "    # 2 Merge safely\n",
    "    combined = {}\n",
    "    for r in semantic_results:\n",
    "        smiles_key = r['smiles']\n",
    "        if isinstance(smiles_key, pd.Series):\n",
    "            smiles_key = smiles_key.iloc[0]\n",
    "        if pd.isna(smiles_key):\n",
    "            continue\n",
    "        smiles_key = str(smiles_key)\n",
    "        combined[smiles_key] = {'semantic': r['score'], 'structure': 0, 'data': r}\n",
    "\n",
    "    for r in structure_results:\n",
    "        smiles_key = r['smiles']\n",
    "        if isinstance(smiles_key, pd.Series):\n",
    "            smiles_key = smiles_key.iloc[0]\n",
    "        if pd.isna(smiles_key):\n",
    "            continue\n",
    "        smiles_key = str(smiles_key)\n",
    "        if smiles_key in combined:\n",
    "            combined[smiles_key]['structure'] = r['score']\n",
    "        else:\n",
    "            combined[smiles_key] = {'semantic': 0, 'structure': r['score'], 'data': r}\n",
    "\n",
    "    # 3 Compute hybrid score\n",
    "    results = []\n",
    "    for k_smiles, v in combined.items():\n",
    "        hybrid_score = v['semantic'] + v['structure']  # adjust weighting if needed\n",
    "        results.append({\n",
    "            'smiles': k_smiles,\n",
    "            'semantic_score': v['semantic'],\n",
    "            'structure_score': v['structure'],\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'data': v['data']\n",
    "        })\n",
    "\n",
    "    # 4 Sort and return top-k\n",
    "    results = sorted(results, key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    return results[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rag_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG System Statistics:\n",
      "Total molecules indexed: 562533\n",
      "Embedding dimension: 384\n",
      "Fingerprint dimension: 2048\n",
      "No valid molecular weights found in the first 100 SMILES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:06:29] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:06:29] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:06:29] smiles\n",
      "[04:06:29] ~^\n",
      "[04:06:29] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n",
      "[04:06:29] SMILES Parse Error: syntax error while parsing: smiles\n",
      "[04:06:29] SMILES Parse Error: check for mistakes around position 2:\n",
      "[04:06:29] smiles\n",
      "[04:06:29] ~^\n",
      "[04:06:29] SMILES Parse Error: Failed parsing SMILES 'smiles' for input: 'smiles'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d598a6d1dd47afbffb695423a63ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic search time: 0.0420s\n",
      "Structure search time: 0.0013s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 22 \n",
    "# RAG System Analysis\n",
    "import time\n",
    "from rdkit import Chem\n",
    "\n",
    "print('RAG System Statistics:')\n",
    "print(f'Total molecules indexed: {len(rag_system.df)}')\n",
    "print(f'Embedding dimension: {rag_system.embeddings.shape[1]}')\n",
    "print(f'Fingerprint dimension: {rag_system.fingerprints.shape[1]}')\n",
    "\n",
    "# Sample molecular properties distribution\n",
    "mw_values = []\n",
    "smiles_col = [col for col in rag_system.df.columns if 'smiles' in col.lower()][0]\n",
    "for smiles in rag_system.df[smiles_col].head(100):\n",
    "    props = rag_system.get_molecular_properties(smiles)\n",
    "    if props['mw'] > 0:  # Only include valid molecular weights\n",
    "        mw_values.append(props['mw'])\n",
    "if mw_values:\n",
    "    print(f'Sample MW range: {min(mw_values):.1f} - {max(mw_values):.1f} Da')\n",
    "else:\n",
    "    print(\"No valid molecular weights found in the first 100 SMILES.\")\n",
    "\n",
    "# Test query performance\n",
    "start = time.time()\n",
    "_ = rag_system.semantic_search('test query', k=10)\n",
    "semantic_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = rag_system.structure_search('CCO', k=10)\n",
    "structure_time = time.time() - start\n",
    "\n",
    "print(f'Semantic search time: {semantic_time:.4f}s')\n",
    "print(f'Structure search time: {structure_time:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f0a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "df_massspecgym shape: (562533, 20)\n",
      "Columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n"
     ]
    }
   ],
   "source": [
    "#cell 15\n",
    "# Hybrid Search Examples\n",
    "queries = [\n",
    "    'aromatic compound with hydroxyl group',\n",
    "    'small molecule with high logP',\n",
    "    'compound with multiple rings and nitrogen'\n",
    "]\n",
    "# Cell to verify df_massspecgym\n",
    "print('df_massspecgym' in globals())\n",
    "if 'df_massspecgym' in globals():\n",
    "    print(\"df_massspecgym shape:\", df_massspecgym.shape)\n",
    "    print(\"Columns:\", df_massspecgym.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "813f86e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 04:06:44,766 INFO Dataset size: (562533, 20)\n",
      "2025-10-08 04:06:44,767 INFO Columns in dataset: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-10-08 04:06:44,849 INFO First 5 rows:\n",
      "             identifier                                                mzs  \\\n",
      "0  MassSpecGymID0000001  91.0542,125.0233,154.0499,155.0577,185.0961,20...   \n",
      "1  MassSpecGymID0000002  91.0542,125.0233,155.0577,185.0961,229.0859,24...   \n",
      "2  MassSpecGymID0000003  69.0343,91.0542,125.0233,127.039,153.0699,154....   \n",
      "3  MassSpecGymID0000004  69.0343,91.0542,110.06,111.0441,112.0393,120.0...   \n",
      "4  MassSpecGymID0000005  91.0542,125.0233,185.0961,229.0859,246.1125,28...   \n",
      "\n",
      "                                         intensities  \\\n",
      "0  0.24524524524524524,1.0,0.08008008008008008,0....   \n",
      "1  0.0990990990990991,0.28128128128128127,0.04004...   \n",
      "2  0.03403403403403404,0.31431431431431434,1.0,0....   \n",
      "3  0.17917917917917917,0.47347347347347346,0.0380...   \n",
      "4  0.07807807807807808,0.1841841841841842,0.03503...   \n",
      "\n",
      "                                    smiles        inchikey    formula  \\\n",
      "0  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "1  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "2  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "3  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "4  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1  VFMQMACUYWGDOJ  C16H17NO4   \n",
      "\n",
      "  precursor_formula  parent_mass  precursor_mz  adduct instrument_type  \\\n",
      "0         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "1         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "2         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "3         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "4         C16H18NO4   287.115224      288.1225  [M+H]+        Orbitrap   \n",
      "\n",
      "   collision_energy   fold  simulation_challenge  \\\n",
      "0              30.0  train                  True   \n",
      "1              20.0  train                  True   \n",
      "2              40.0  train                  True   \n",
      "3              55.0  train                  True   \n",
      "4              10.0  train                  True   \n",
      "\n",
      "                                    smiles  ion_mode  precursor_bin  \\\n",
      "0  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "1  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "2  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "3  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "4  COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1         0             18   \n",
      "\n",
      "   adduct_idx                                             binned  \\\n",
      "0           0  [0.46928015209500606, 0.6944444444444444, 1.03...   \n",
      "1           0  [0.5189473636660348, 0.3523541331252303, 0.533...   \n",
      "2           0  [0.8205128205128205, 0.9487179487179487, 0.307...   \n",
      "3           0  [0.9428571428571428, 1.0, 0.3523809523809524, ...   \n",
      "4           0  [0.6052631578947368, 0.4473684210526316, 1.000...   \n",
      "\n",
      "                                          graph_data  \n",
      "0  [(x, [tensor([0.4693]), tensor([0.6944]), tens...  \n",
      "1  [(x, [tensor([0.5189]), tensor([0.3524]), tens...  \n",
      "2  [(x, [tensor([0.8205]), tensor([0.9487]), tens...  \n",
      "3  [(x, [tensor([0.9429]), tensor([1.]), tensor([...  \n",
      "4  [(x, [tensor([0.6053]), tensor([0.4474]), tens...  \n",
      "2025-10-08 04:06:44,850 WARNING Multiple SMILES columns found: ['smiles', 'smiles']\n",
      "2025-10-08 04:06:44,850 INFO Indices of 'smiles' columns: [3, 14]\n",
      "2025-10-08 04:06:44,851 INFO Sample data from 'smiles' column at index 3:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-10-08 04:06:44,851 INFO Sample data from 'smiles' column at index 14:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-10-08 04:06:44,852 INFO Using column 'smiles' at index 3 as SMILES source.\n",
      "2025-10-08 04:06:44,852 INFO Type of selected column: <class 'pandas.core.series.Series'>\n",
      "2025-10-08 04:08:01,773 INFO Extracted 562533 valid SMILES strings.\n",
      "2025-10-08 04:10:31,290 INFO Generated 562533 SELFIES strings successfully.\n",
      "2025-10-08 04:10:52,257 INFO SELFIES vocabulary size: 87\n",
      "2025-10-08 04:10:52,257 INFO Pretrain MAX_LEN: 100, Supervised MAX_LEN: 135\n"
     ]
    }
   ],
   "source": [
    "#cell 16\n",
    "# SELFIES Vocabulary Construction\n",
    "# Enhanced SMILES Extraction and SELFIES Vocabulary Construction\n",
    "\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "import selfies as sf\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Verify dataset\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "\n",
    "# Step 1: Inspect dataset\n",
    "logging.info(f\"Dataset size: {df_massspecgym.shape}\")\n",
    "logging.info(f\"Columns in dataset: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"First 5 rows:\\n{df_massspecgym.head(5)}\")\n",
    "\n",
    "# Step 2: Auto-detect SMILES column\n",
    "smiles_col_candidates = [col for col in df_massspecgym.columns if 'smiles' in col.lower()]\n",
    "if not smiles_col_candidates:\n",
    "    raise ValueError(\"No column containing 'smiles' found in MassSpecGym dataset.\")\n",
    "\n",
    "# Handle duplicate 'smiles' columns\n",
    "if len(smiles_col_candidates) > 1:\n",
    "    logging.warning(f\"Multiple SMILES columns found: {smiles_col_candidates}\")\n",
    "    # Get indices of all 'smiles' columns\n",
    "    smiles_col_indices = [i for i, col in enumerate(df_massspecgym.columns) if col == 'smiles']\n",
    "    logging.info(f\"Indices of 'smiles' columns: {smiles_col_indices}\")\n",
    "    \n",
    "    # Inspect contents of each 'smiles' column\n",
    "    for i, idx in enumerate(smiles_col_indices):\n",
    "        col_data = df_massspecgym.iloc[:, idx].head(5)\n",
    "        logging.info(f\"Sample data from 'smiles' column at index {idx}:\\n{col_data}\")\n",
    "    \n",
    "    smiles_col_index = smiles_col_indices[0]\n",
    "    smiles_col = 'smiles'\n",
    "else:\n",
    "    smiles_col_index = df_massspecgym.columns.get_loc(smiles_col_candidates[0])\n",
    "    smiles_col = smiles_col_candidates[0]\n",
    "\n",
    "logging.info(f\"Using column '{smiles_col}' at index {smiles_col_index} as SMILES source.\")\n",
    "\n",
    "# Step 3: Extract clean SMILES strings safely\n",
    "def is_valid_smiles(smiles):\n",
    "    if not isinstance(smiles, str) or not smiles.strip():\n",
    "        return False\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        return mol is not None\n",
    "    except ValueError as e:\n",
    "        logging.debug(f\"Invalid SMILES '{smiles}': {e}\")\n",
    "        return False\n",
    "\n",
    "smiles_series = df_massspecgym.iloc[:, smiles_col_index]\n",
    "logging.info(f\"Type of selected column: {type(smiles_series)}\")\n",
    "if not isinstance(smiles_series, pd.Series):\n",
    "    raise TypeError(f\"Selected column at index {smiles_col_index} is {type(smiles_series)}, expected pandas.Series\")\n",
    "\n",
    "all_smiles = smiles_series.dropna().tolist()\n",
    "valid_smiles = [s for s in all_smiles if is_valid_smiles(s)]\n",
    "logging.info(f\"Extracted {len(valid_smiles)} valid SMILES strings.\")\n",
    "if not valid_smiles:\n",
    "    raise ValueError(\"No valid SMILES could be extracted. Check your dataset!\")\n",
    "\n",
    "# Step 4: Convert to SELFIES\n",
    "all_selfies = []\n",
    "failed_conversions = []\n",
    "for s in valid_smiles:\n",
    "    try:\n",
    "        selfies_str = sf.encoder(s)\n",
    "        all_selfies.append(selfies_str)\n",
    "    except Exception as e:\n",
    "        failed_conversions.append((s, str(e)))\n",
    "        continue\n",
    "\n",
    "if failed_conversions:\n",
    "    logging.warning(f\"Failed to convert {len(failed_conversions)} SMILES to SELFIES. First few errors: {failed_conversions[:5]}\")\n",
    "if not all_selfies:\n",
    "    raise ValueError(\"No valid SELFIES could be generated. Check your SMILES extraction!\")\n",
    "logging.info(f\"Generated {len(all_selfies)} SELFIES strings successfully.\")\n",
    "\n",
    "# Step 5: Build SELFIES vocabulary\n",
    "selfies_alphabet = set()\n",
    "for s in all_selfies:\n",
    "    selfies_alphabet.update(sf.split_selfies(s))\n",
    "\n",
    "token_to_idx = {token: idx for idx, token in enumerate(sorted(selfies_alphabet))}\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "# Add special tokens if needed\n",
    "special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "for token in special_tokens:\n",
    "    if token not in token_to_idx:\n",
    "        token_to_idx[token] = len(token_to_idx)\n",
    "        idx_to_token[len(idx_to_token)] = token\n",
    "\n",
    "vocab_size = len(token_to_idx)\n",
    "PRETRAIN_MAX_LEN = min(100, max(len(list(sf.split_selfies(s))) for s in all_selfies) if all_selfies else 0)\n",
    "SUPERVISED_MAX_LEN = max(len(list(sf.split_selfies(s))) + 2 for s in all_selfies) if all_selfies else 0\n",
    "\n",
    "logging.info(f\"SELFIES vocabulary size: {vocab_size}\")\n",
    "logging.info(f\"Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}\")\n",
    "\n",
    "# Define encode_selfies function\n",
    "def encode_selfies(selfies_str, max_len):\n",
    "    \"\"\"Encode a SELFIES string into a list of token indices, padded/truncated to max_len.\"\"\"\n",
    "    tokens = list(sf.split_selfies(selfies_str))\n",
    "    token_indices = [token_to_idx.get(token, token_to_idx['<unk>']) for token in tokens]\n",
    "    token_indices = [token_to_idx['<start>']] + token_indices + [token_to_idx['<end>']]\n",
    "    if len(token_indices) > max_len:\n",
    "        token_indices = token_indices[:max_len]\n",
    "    padding_idx = token_to_idx['<pad>']\n",
    "    token_indices += [padding_idx] * (max_len - len(token_indices))\n",
    "    return token_indices\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({'SELFIES': all_selfies}).to_csv('massspecgym_selfies.csv', index=False)\n",
    "with open('selfies_vocab.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump({'token_to_idx': token_to_idx, 'idx_to_token': idx_to_token}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677d6a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 04:36:32,948 INFO df_massspecgym columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'smiles', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-10-08 04:36:32,949 INFO df_external columns: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-10-08 04:36:32,949 WARNING Multiple 'smiles' columns found in df_massspecgym at indices: [3, 14]\n",
      "2025-10-08 04:36:32,950 INFO Sample data from df_massspecgym 'smiles' at index 3:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-10-08 04:36:32,951 INFO Sample data from df_massspecgym 'smiles' at index 14:\n",
      "0    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "1    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "2    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "3    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "4    COc1cc([C@H](Cc2ccccc2)NC(C)=O)oc(=O)c1\n",
      "Name: smiles, dtype: object\n",
      "2025-10-08 04:36:32,951 INFO Using df_massspecgym 'smiles' column at index 3\n",
      "2025-10-08 04:36:32,952 INFO Using df_external 'smiles' column at index 3\n",
      "2025-10-08 04:36:32,981 INFO Extracted 31602 unique SMILES strings.\n",
      "2025-10-08 04:36:39,724 INFO Generated fingerprints for 31602 SMILES strings.\n"
     ]
    }
   ],
   "source": [
    "#cell 17\n",
    "# Precompute Morgan Fingerprints for All Unique SMILES\n",
    "\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Verify datasets\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "if 'df_external' not in globals():\n",
    "    raise NameError(\"df_external is not defined. Please load the dataset.\")\n",
    "\n",
    "# Step 1: Inspect datasets\n",
    "logging.info(f\"df_massspecgym columns: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"df_external columns: {df_external.columns.tolist()}\")\n",
    "\n",
    "# Step 2: Select SMILES column from df_massspecgym\n",
    "massspecgym_smiles_cols = [i for i, col in enumerate(df_massspecgym.columns) if col == 'smiles']\n",
    "if not massspecgym_smiles_cols:\n",
    "    raise ValueError(\"No 'smiles' column found in df_massspecgym.\")\n",
    "if len(massspecgym_smiles_cols) > 1:\n",
    "    logging.warning(f\"Multiple 'smiles' columns found in df_massspecgym at indices: {massspecgym_smiles_cols}\")\n",
    "    for idx in massspecgym_smiles_cols:\n",
    "        logging.info(f\"Sample data from df_massspecgym 'smiles' at index {idx}:\\n{df_massspecgym.iloc[:, idx].head(5)}\")\n",
    "massspecgym_smiles_index = massspecgym_smiles_cols[0]\n",
    "logging.info(f\"Using df_massspecgym 'smiles' column at index {massspecgym_smiles_index}\")\n",
    "\n",
    "# Step 3: Select SMILES column from df_external\n",
    "external_smiles_cols = [i for i, col in enumerate(df_external.columns) if col == 'smiles']\n",
    "if not external_smiles_cols:\n",
    "    raise ValueError(\"No 'smiles' column found in df_external.\")\n",
    "if len(external_smiles_cols) > 1:\n",
    "    logging.warning(f\"Multiple 'smiles' columns found in df_external at indices: {external_smiles_cols}\")\n",
    "    for idx in external_smiles_cols:\n",
    "        logging.info(f\"Sample data from df_external 'smiles' at index {idx}:\\n{df_external.iloc[:, idx].head(5)}\")\n",
    "external_smiles_index = external_smiles_cols[0]\n",
    "logging.info(f\"Using df_external 'smiles' column at index {external_smiles_index}\")\n",
    "\n",
    "# Step 4: Extract and combine unique SMILES\n",
    "massspecgym_smiles = df_massspecgym.iloc[:, massspecgym_smiles_index].dropna().tolist()\n",
    "external_smiles = df_external.iloc[:, external_smiles_index].dropna().tolist()\n",
    "all_smiles = list(set(massspecgym_smiles + external_smiles))\n",
    "logging.info(f\"Extracted {len(all_smiles)} unique SMILES strings.\")\n",
    "\n",
    "# Step 5: Precompute Morgan fingerprints for all unique SMILES\n",
    "all_fingerprints = {}\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "for smiles in all_smiles:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            all_fingerprints[smiles] = morgan_gen.GetFingerprint(mol)\n",
    "        else:\n",
    "            logging.warning(f\"Invalid SMILES '{smiles}' skipped during fingerprint generation.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to process SMILES '{smiles}': {e}\")\n",
    "logging.info(f\"Generated fingerprints for {len(all_fingerprints)} SMILES strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 18\n",
    "# Dataset class for MS/MS data\n",
    "\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=PRETRAIN_MAX_LEN, is_ssl=False):\n",
    "        self.spectra = np.stack(dataframe['binned'].values)\n",
    "        self.graph_data = dataframe['graph_data'].values\n",
    "        self.ion_modes = dataframe['ion_mode'].values\n",
    "        self.precursor_bins = dataframe['precursor_bin'].values\n",
    "        self.adduct_indices = dataframe['adduct_idx'].values\n",
    "        self.raw_smiles = dataframe['smiles'].values\n",
    "        self.is_ssl = is_ssl\n",
    "        if is_ssl:\n",
    "            self.smiles = []\n",
    "            self.masked_smiles = []\n",
    "            for s in self.raw_smiles:\n",
    "                selfies = sf.encoder(s)\n",
    "                masked_s, orig_s = self.mask_selfies(selfies)\n",
    "                self.smiles.append(encode_selfies(orig_s, max_len))\n",
    "                self.masked_smiles.append(encode_selfies(masked_s, max_len))\n",
    "        else:\n",
    "            self.smiles = [encode_selfies(sf.encoder(s), max_len=SUPERVISED_MAX_LEN) for s in self.raw_smiles]\n",
    "\n",
    "    def mask_selfies(self, selfies, mask_ratio=0.10):\n",
    "        try:\n",
    "            tokens = sf.split_selfies(selfies)[:PRETRAIN_MAX_LEN-2]\n",
    "            masked_tokens = tokens.copy()\n",
    "            n_mask = int(mask_ratio * len(tokens))\n",
    "            if n_mask > 0:\n",
    "                mask_indices = np.random.choice(len(tokens), n_mask, replace=False)\n",
    "                for idx in mask_indices:\n",
    "                    masked_tokens[idx] = config.MASK_TOKEN\n",
    "            return ''.join(masked_tokens), ''.join(tokens)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"mask_selfies failed for {selfies}: {e}\\n{traceback.format_exc()}\")\n",
    "            return selfies, selfies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectra)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_ssl:\n",
    "            return (\n",
    "                torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "                self.graph_data[idx],\n",
    "                torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.masked_smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "                torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "                torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "                self.raw_smiles[idx]\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "            self.graph_data[idx],\n",
    "            torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "            torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "            torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "            self.raw_smiles[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6336cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 19\n",
    "# Positional encoding and model encoder/decoder classes\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Neural Network Models\n",
    "class SpectrumTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.unsqueeze(-1))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SpectrumGNNEncoder(MessagePassing):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.d_model = d_model\n",
    "        self.lin = nn.Linear(1, d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.lin(x)\n",
    "        x = self.propagate(edge_index, x=x)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return self.mlp(x_j)\n",
    "\n",
    "class SmilesTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        output = self.transformer(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return self.output_proj(output)\n",
    "\n",
    "class MSMS2SmilesHybrid(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, **kwargs):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder = SpectrumTransformerEncoder(d_model, nhead, num_layers)\n",
    "        self.gnn_encoder = SpectrumGNNEncoder(d_model)\n",
    "        self.decoder = SmilesTransformerDecoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.fusion = nn.Linear(d_model * 2, d_model)\n",
    "\n",
    "    def forward(self, spectrum, graph_data, tgt, tgt_mask=None):\n",
    "        transformer_out = self.transformer_encoder(spectrum)\n",
    "        gnn_out = self.gnn_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        memory = self.fusion(torch.cat([transformer_out.mean(1), gnn_out], dim=1)).unsqueeze(1)\n",
    "        return self.decoder(tgt, memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 20\n",
    "# Training and evaluation functions\n",
    "\n",
    "def ssl_pretrain(model, dataloader, epochs=3, lr=1e-4):\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f'SSL Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, masked, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(spectrum, graph_batch, target[:, :-1])\n",
    "                loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        print(f'SSL Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "def supervised_train(model, train_loader, val_loader, epochs=30, lr=1e-4, patience=5):\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}'):\n",
    "            spectrum, graph_data, target, _, _, _, _ = batch\n",
    "            spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "            \n",
    "            # Create attention mask\n",
    "            tgt_mask = torch.triu(torch.ones(target.size(1)-1, target.size(1)-1), diagonal=1).bool().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(spectrum, graph_batch, target[:, :-1], tgt_mask=tgt_mask)\n",
    "                loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                spectrum, graph_data, target, _, _, _, _ = batch\n",
    "                spectrum, target = spectrum.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                graph_batch = Batch.from_data_list(graph_data).to(device)\n",
    "                with autocast():\n",
    "                    output = model(spectrum, graph_batch, target[:, :-1])\n",
    "                    loss = F.cross_entropy(output.reshape(-1, output.size(-1)), target[:, 1:].reshape(-1), ignore_index=0)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    return best_val_loss\n",
    "\n",
    "def beam_search(model, spectrum, graph_data, ion_mode, precursor_bin, adduct_idx, true_smiles, beam_width=5, max_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device)\n",
    "        graph_batch = Batch.from_data_list([graph_data]).to(device)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        sequences = [[token_to_idx[config.SOS_TOKEN]]]\n",
    "        scores = [0.0]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            for i, seq in enumerate(sequences):\n",
    "                if seq[-1] == token_to_idx[config.EOS_TOKEN]:\n",
    "                    candidates.append((seq, scores[i]))\n",
    "                    continue\n",
    "                \n",
    "                tgt = torch.tensor([seq]).to(device)\n",
    "                output = model(spectrum, graph_batch, tgt)\n",
    "                probs = F.softmax(output[0, -1], dim=-1)\n",
    "                \n",
    "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                for prob, idx in zip(top_probs, top_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = scores[i] + torch.log(prob).item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            sequences = [seq for seq, _ in candidates[:beam_width]]\n",
    "            scores = [score for _, score in candidates[:beam_width]]\n",
    "        \n",
    "        results = []\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            smiles = decode_selfies(seq)\n",
    "            if smiles:\n",
    "                results.append((smiles, score))\n",
    "        return results[:beam_width]\n",
    "\n",
    "# Missing evaluation functions\n",
    "def mw_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolWt(mol1) - Descriptors.MolWt(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "def logp_difference(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            return abs(Descriptors.MolLogP(mol1) - Descriptors.MolLogP(mol2))\n",
    "    except:\n",
    "        pass\n",
    "    return float('inf')\n",
    "\n",
    "def substructure_match(smiles1, smiles2, substructures=None):\n",
    "    return 0.5  # Placeholder\n",
    "\n",
    "def error_analysis(pred_list, true_list, adduct_list, fingerprints):\n",
    "    print('Error analysis completed')\n",
    "\n",
    "def plot_attention_weights(weights, title='Attention'):\n",
    "    print(f'Attention visualization: {title}')\n",
    "\n",
    "def plot_gnn_edge_weights(weights, edges, title='GNN'):\n",
    "    print(f'GNN visualization: {title}')\n",
    "\n",
    "def calculate_bleu(predicted_smiles, true_smiles):\n",
    "    try:\n",
    "        pred_tokens = list(predicted_smiles)\n",
    "        true_tokens = list(true_smiles)\n",
    "        return sentence_bleu([true_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2, fingerprint_dict):\n",
    "    if smiles1 in fingerprint_dict and smiles2 in fingerprint_dict:\n",
    "        return DataStructs.TanimotoSimilarity(fingerprint_dict[smiles1], fingerprint_dict[smiles2])\n",
    "    return 0.0\n",
    "\n",
    "def validity_rate(smiles_list):\n",
    "    valid = sum(1 for s in smiles_list if Chem.MolFromSmiles(s) is not None)\n",
    "    return (valid / len(smiles_list)) * 100 if smiles_list else 0\n",
    "\n",
    "def objective(trial, train_data, val_data):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    return lr  # Simplified for demo\n",
    "\n",
    "# Additional metrics and visualization\n",
    "def dice_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            fp1 = Chem.RDKFingerprint(mol1)\n",
    "            fp2 = Chem.RDKFingerprint(mol2)\n",
    "            return DataStructs.DiceSimilarity(fp1, fp2)\n",
    "    except: pass\n",
    "    return 0.0\n",
    "\n",
    "def mcs_similarity(smiles1, smiles2):\n",
    "    try:\n",
    "        mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "        if mol1 and mol2:\n",
    "            mcs = rdFMCS.FindMCS([mol1, mol2])\n",
    "            return mcs.numAtoms / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    except: pass\n",
    "    return 0.0\n",
    "\n",
    "def prediction_diversity(smiles_list):\n",
    "    unique_smiles = set(smiles_list)\n",
    "    return len(unique_smiles) / len(smiles_list) if smiles_list else 0\n",
    "\n",
    "def plot_molecular_comparison(true_smiles, pred_smiles, title='Comparison'):\n",
    "    try:\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "        pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "        if true_mol and pred_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.axis('off')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "    except Exception as e: print(f'Visualization error: {e}')\n",
    "\n",
    "# Model checkpointing\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'vocab_size': vocab_size,\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Data validation functions\n",
    "def validate_spectrum_quality(mzs, intensities, min_peaks=5, max_mz_range=2000):\n",
    "    if len(mzs) < min_peaks or max(mzs) > max_mz_range:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_molecular_properties(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return False\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        return 50 <= mw <= 1000\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def remove_duplicates(df, subset=['smiles', 'precursor_mz']):\n",
    "    return df.drop_duplicates(subset=subset, keep='first')\n",
    "\n",
    "# Memory management\n",
    "def clear_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration system ready\n"
     ]
    }
   ],
   "source": [
    "#cell 21\n",
    "# Integration: XGBoost + RAG + Deep Learning\n",
    "class HybridPredictor:\n",
    "    def __init__(self, dl_model, xgb_model, rag_system, label_encoder):\n",
    "        self.dl_model = dl_model\n",
    "        self.xgb_model = xgb_model\n",
    "        self.rag_system = rag_system\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def predict_ensemble(self, spectrum, graph_data, features, query_text=None, weights=[0.5, 0.3, 0.2]):\n",
    "        # Validate weights\n",
    "        if abs(sum(weights) - 1.0) > 0.01:\n",
    "            weights = [w/sum(weights) for w in weights]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Deep learning prediction with error handling\n",
    "        try:\n",
    "            dl_results = beam_search(self.dl_model, spectrum, graph_data, 0, 0, 0, '', beam_width=5, device=device)\n",
    "            if dl_results and dl_results[0][0]:\n",
    "                predictions.append(('DL', dl_results[0][0], weights[0]))\n",
    "        except Exception as e:\n",
    "            print(f'DL prediction failed: {e}')\n",
    "        \n",
    "        # XGBoost prediction with error handling\n",
    "        try:\n",
    "            xgb_pred = self.xgb_model.predict([features])[0]\n",
    "            xgb_smiles = self.label_encoder.inverse_transform([xgb_pred])[0]\n",
    "            predictions.append(('XGB', xgb_smiles, weights[1]))\n",
    "        except Exception as e:\n",
    "            print(f'XGBoost prediction failed: {e}')\n",
    "        \n",
    "        # RAG prediction with error handling\n",
    "        if query_text:\n",
    "            try:\n",
    "                rag_results = self.rag_system.semantic_search(query_text, k=1)\n",
    "                if rag_results and len(rag_results) > 0:\n",
    "                    predictions.append(('RAG', rag_results[0]['smiles'], weights[2]))\n",
    "            except Exception as e:\n",
    "                print(f'RAG prediction failed: {e}')\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def evaluate_ensemble(self, test_data, n_samples=10):\n",
    "        results = {'dl': [], 'xgb': [], 'rag': [], 'ensemble': []}\n",
    "        \n",
    "        for i in range(min(n_samples, len(test_data))):\n",
    "            row = test_data.iloc[i]\n",
    "            true_smiles = row['smiles']\n",
    "            \n",
    "            # Extract features\n",
    "            spectrum = row['binned']\n",
    "            graph_data = row['graph_data']\n",
    "            features = [np.mean(spectrum), np.std(spectrum), np.max(spectrum), \n",
    "                       np.sum(spectrum > 0.1), row['precursor_mz'], row['ion_mode'], \n",
    "                       row['adduct_idx'], len(row['mzs'])]\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            preds = self.predict_ensemble(spectrum, graph_data, features, f\"molecule with MW {row['precursor_mz']:.1f}\")\n",
    "            \n",
    "            # Evaluate each method\n",
    "            for method, pred_smiles, weight in preds:\n",
    "                similarity = tanimoto_similarity(pred_smiles, true_smiles, all_fingerprints)\n",
    "                results[method.lower()].append(similarity)\n",
    "            \n",
    "            # Weighted ensemble score\n",
    "            ensemble_score = sum(tanimoto_similarity(pred, true_smiles, all_fingerprints) * w for _, pred, w in preds) / sum(w for _, _, w in preds)\n",
    "            results['ensemble'].append(ensemble_score)\n",
    "        \n",
    "        return {k: np.mean(v) if v else 0 for k, v in results.items()}\n",
    "\n",
    "print('Integration system ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb652e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 04:37:34,098 INFO df_massspecgym columns after removing duplicates: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n",
      "2025-10-08 04:37:34,098 INFO df_external columns after removing duplicates: ['identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'ion_mode', 'precursor_bin', 'adduct_idx', 'binned', 'graph_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 04:37:41,029 INFO External dataset size: 23111\n",
      "2025-10-08 04:37:41,039 INFO \n",
      "Fold 1/5\n"
     ]
    }
   ],
   "source": [
    "# cell 22\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Verify required variables\n",
    "required_vars = ['MSMSDataset', 'config', 'objective', 'ssl_pretrain', 'supervised_train', \n",
    "                'token_to_idx', 'idx_to_token', 'vocab_size', 'PRETRAIN_MAX_LEN', 'SUPERVISED_MAX_LEN', 'device']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        raise NameError(f\"{var} is not defined. Ensure it is defined in previous cells.\")\n",
    "\n",
    "# Verify datasets\n",
    "if 'df_massspecgym' not in globals():\n",
    "    raise NameError(\"df_massspecgym is not defined. Please load the dataset in Cell 3.\")\n",
    "if 'df_external' not in globals():\n",
    "    raise NameError(\"df_external is not defined. Please load the dataset.\")\n",
    "\n",
    "# Remove duplicate columns\n",
    "df_massspecgym = df_massspecgym.loc[:, ~df_massspecgym.columns.duplicated(keep='first')]\n",
    "df_external = df_external.loc[:, ~df_external.columns.duplicated(keep='first')]\n",
    "logging.info(f\"df_massspecgym columns after removing duplicates: {df_massspecgym.columns.tolist()}\")\n",
    "logging.info(f\"df_external columns after removing duplicates: {df_external.columns.tolist()}\")\n",
    "\n",
    "# Verify 'smiles' column exists\n",
    "if 'smiles' not in df_massspecgym.columns:\n",
    "    raise ValueError(\"No 'smiles' column in df_massspecgym after removing duplicates.\")\n",
    "if 'smiles' not in df_external.columns:\n",
    "    raise ValueError(\"No 'smiles' column in df_external after removing duplicates.\")\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "# Debug/interactive safety\n",
    "FAST_DEBUG = globals().get('FAST_DEBUG', False)\n",
    "if FAST_DEBUG:\n",
    "    logging.info('FAST_DEBUG mode enabled: using small subsets, fewer epochs and trials, single-worker loaders')\n",
    "\n",
    "# Create external dataset\n",
    "external_dataset = MSMSDataset(df_external, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "logging.info(f\"External dataset size: {len(external_dataset)}\")\n",
    "external_loader = None  # Set to None as in original code; update if needed\n",
    "\n",
    "# Prefer torch_geometric DataLoader\n",
    "use_geo_loader = False\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "    use_geo_loader = True\n",
    "except Exception:\n",
    "    from torch.utils.data import DataLoader as TorchDataLoader\n",
    "    GeoDataLoader = None\n",
    "    logging.info(\"Using torch.utils.data.DataLoader as fallback.\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_massspecgym)):\n",
    "    logging.info(f\"\\nFold {fold+1}/5\")\n",
    "    train_data = df_massspecgym.iloc[train_idx].copy()\n",
    "    val_data = df_massspecgym.iloc[val_idx].copy()\n",
    "    ssl_data = train_data.sample(frac=0.3, random_state=42).copy()\n",
    "\n",
    "    # FAST_DEBUG sampling\n",
    "    if FAST_DEBUG:\n",
    "        train_data = train_data.sample(n=min(512, len(train_data)), random_state=42)\n",
    "        val_data = val_data.sample(n=min(128, len(val_data)), random_state=42)\n",
    "        ssl_data = ssl_data.sample(n=min(256, len(ssl_data)), random_state=42)\n",
    "        optuna_trials = 2\n",
    "        ssl_epochs = 1\n",
    "        supervised_epochs = 1\n",
    "    else:\n",
    "        optuna_trials = 10\n",
    "        ssl_epochs = config.SSL_EPOCHS\n",
    "        supervised_epochs = config.SUPERVISED_EPOCHS\n",
    "\n",
    "    # DataLoader parameters\n",
    "    workers = 0  # Single worker for Jupyter stability\n",
    "    pin_memory = False  # Disable for notebook safety\n",
    "\n",
    "    # Batch sizes\n",
    "    if torch.cuda.is_available():\n",
    "        train_bs = max(4, int(config.BATCH_SIZE // 8))\n",
    "        val_bs = max(4, int(config.BATCH_SIZE // 8))\n",
    "        ssl_bs = max(8, int(config.BATCH_SIZE // 4))\n",
    "    else:\n",
    "        train_bs = max(8, config.BATCH_SIZE)\n",
    "        val_bs = max(8, config.BATCH_SIZE)\n",
    "        ssl_bs = max(32, config.BATCH_SIZE)\n",
    "\n",
    "    # Build datasets and loaders\n",
    "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    ssl_dataset = MSMSDataset(ssl_data, max_len=PRETRAIN_MAX_LEN, is_ssl=True)\n",
    "\n",
    "    if use_geo_loader and GeoDataLoader is not None:\n",
    "        train_loader = GeoDataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "        val_loader = GeoDataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n",
    "        ssl_loader = GeoDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True)\n",
    "    else:\n",
    "        train_loader = TorchDataLoader(train_dataset, batch_size=train_bs, shuffle=True, num_workers=workers, pin_memory=pin_memory)\n",
    "        val_loader = TorchDataLoader(val_dataset, batch_size=val_bs, shuffle=False, num_workers=workers, pin_memory=pin_memory)\n",
    "        ssl_loader = TorchDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True, num_workers=workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_data, val_data), n_trials=optuna_trials)\n",
    "    best_lr = study.best_params.get('lr', config.LEARNING_RATE)\n",
    "    logging.info(f\"Best learning rate for fold {fold+1}: {best_lr:.6f}\")\n",
    "\n",
    "    # Initialize and train model with multiple OOM retries\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=config.D_MODEL, nhead=config.NHEAD, num_layers=config.NUM_LAYERS).to(device)\n",
    "            logging.info(f\"Starting SSL pretraining for fold {fold+1} with train_bs={train_bs}, ssl_bs={ssl_bs}...\")\n",
    "            ssl_pretrain(model, ssl_loader, epochs=ssl_epochs, lr=best_lr)\n",
    "            logging.info(f\"Starting supervised training for fold {fold+1}...\")\n",
    "            best_val_loss = supervised_train(model, train_loader, val_loader, epochs=supervised_epochs, lr=best_lr, patience=config.PATIENCE)\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower() and retry_count < max_retries - 1 and torch.cuda.is_available():\n",
    "                logging.warning(f\"CUDA OOM detected (retry {retry_count+1}/{max_retries}). Reducing batch sizes...\")\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                train_bs = max(2, train_bs // 2)\n",
    "                val_bs = max(2, val_bs // 2)\n",
    "                ssl_bs = max(4, ssl_bs // 2)\n",
    "                if use_geo_loader and GeoDataLoader is not None:\n",
    "                    train_loader = GeoDataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "                    val_loader = GeoDataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n",
    "                    ssl_loader = GeoDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True)\n",
    "                else:\n",
    "                    train_loader = TorchDataLoader(train_dataset, batch_size=train_bs, shuffle=True, num_workers=0, pin_memory=False)\n",
    "                    val_loader = TorchDataLoader(val_dataset, batch_size=val_bs, shuffle=False, num_workers=0, pin_memory=False)\n",
    "                    ssl_loader = TorchDataLoader(ssl_dataset, batch_size=ssl_bs, shuffle=True, num_workers=0, pin_memory=False)\n",
    "                logging.info(f\"New batch sizes -> train: {train_bs}, val: {val_bs}, ssl: {ssl_bs}. Retrying fold...\")\n",
    "                retry_count += 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    fold_results.append(best_val_loss)\n",
    "    model_path = f'best_msms_hybrid_fold_{fold+1}.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, model_path)\n",
    "    \n",
    "    try:\n",
    "        display(FileLink(model_path))\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to create FileLink for {model_path}: {e}\")\n",
    "\n",
    "logging.info(f\"Cross-validation results: {fold_results}\")\n",
    "logging.info(f\"Average validation loss: {np.mean(fold_results):.4f}\")\n",
    "\n",
    "# Training time estimation\n",
    "total_samples = len(df_massspecgym)\n",
    "samples_per_epoch = total_samples // max(1, train_bs)\n",
    "total_epochs = config.N_FOLDS * (ssl_epochs + supervised_epochs)\n",
    "estimated_hours = (samples_per_epoch * total_epochs * 0.5) / 3600\n",
    "logging.info(f\"Estimated training time: {estimated_hours:.1f} hours ({estimated_hours/24:.1f} days)\")\n",
    "logging.info(\"With RTX 3080 Ti optimizations, expect 8-12 hours total.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 23\n",
    "# Load the best trained model\n",
    "model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=config.D_MODEL, nhead=config.NHEAD, num_layers=config.NUM_LAYERS).to(device)\n",
    "checkpoint = torch.load('best_msms_hybrid_fold_1.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Model loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 24\n",
    "# External dataset evaluation and visualization\n",
    "model.eval()\n",
    "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
    "pred_smiles_list = []\n",
    "true_smiles_list = []\n",
    "adducts_list = []\n",
    "num_samples = min(5, len(external_dataset))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample_spectrum = external_dataset[sample_idx][0]\n",
    "    sample_graph = external_dataset[sample_idx][1]\n",
    "    sample_ion_mode = external_dataset[sample_idx][3]\n",
    "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
    "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
    "    true_smiles = external_dataset[sample_idx][6]\n",
    "\n",
    "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
    "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
    "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
    "\n",
    "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
    "    print(\"Top Predicted SMILES:\")\n",
    "    for smiles, confidence in predicted_results[:3]:\n",
    "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
    "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
    "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
    "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
    "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
    "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
    "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
    "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "        if smiles != \"Invalid SMILES\":\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
    "\n",
    "    # Visualize molecules\n",
    "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
    "        if pred_mol and true_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            img_array = np.array(img.convert('RGB'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(img_array)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Visualize attention and GNN weights for first sample\n",
    "    if sample_idx == 0:\n",
    "        with torch.no_grad():\n",
    "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
    "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
    "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
    "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
    "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
    "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
    "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
    "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
    "print(\"External Metrics Summary:\")\n",
    "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
    "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
    "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
    "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
    "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a97e84",
   "metadata": {},
   "source": [
    "backup code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3aa58-2334-49ca-8465-c29868f452b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helpers merged from notebook-for-pc.ipynb - added without removing any existing features\n",
    "# 1) SMILES/SELFIES validators and plausibility checks\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdFMCS\n",
    "from rdkit import DataStructs\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Define a conservative set of valid atom symbols used for basic syntactic checks\n",
    "valid_atoms = set([\n",
    "    'H','B','C','N','O','F','P','S','Cl','Br','I',\n",
    "    'c','n','o','s','p'  # aromatic/lowercase tokens sometimes used in SMILES-like checks\n",
    "])\n",
    "\n",
    "def is_valid_smiles_syntax(smiles):\n",
    "    \"Basic SMILES syntax validator: bracket and paren matching + simple token checks.\"\n",
    "    if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return False\n",
    "    stack = []\n",
    "    for c in smiles:\n",
    "        if c in '([':\n",
    "            stack.append(c)\n",
    "        elif c == ')':\n",
    "            if not stack or stack[-1] != '(':\n",
    "                return False\n",
    "            stack.pop()\n",
    "        elif c == ']':\n",
    "            if not stack or stack[-1] != '[':\n",
    "                return False\n",
    "            stack.pop()\n",
    "    if stack:\n",
    "        return False\n",
    "    i = 0\n",
    "    while i < len(smiles):\n",
    "        if smiles[i] == '[':\n",
    "            j = smiles.find(']', i)\n",
    "            if j == -1:\n",
    "                return False\n",
    "            atom = smiles[i+1:j]\n",
    "            # simple check: at least one valid atom symbol is present\n",
    "            if not any(a in atom for a in valid_atoms):\n",
    "                return False\n",
    "            i = j + 1\n",
    "        else:\n",
    "            if smiles[i] in valid_atoms or smiles[i] in '()=#/\\\\@.:+-0123456789%[]':\n",
    "                i += 1\n",
    "            else:\n",
    "                # allow SELFIES tokens and other chars; fallback to RDKit for final check\n",
    "                i += 1\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        return mol is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_plausible_molecule(smiles, true_mol=None, max_mw=1500, min_logp=-7, max_logp=7):\n",
    "    \"Basic RDKit plausibility check used for filtering beam search outputs.\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if not mol or not is_valid_smiles_syntax(smiles):\n",
    "            return False\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        true_mw = Descriptors.MolWt(true_mol) if true_mol else None\n",
    "        if mw > max_mw:\n",
    "            return False\n",
    "        if not (min_logp <= logp <= max_logp):\n",
    "            return False\n",
    "        if true_mw is not None and abs(mw - true_mw) > 300:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.debug(f'is_plausible_molecule error: {e}')\n",
    "        return False\n",
    "\n",
    "# 2) Safe adduct mapping helper (builds mapping if missing and fills NaNs)\n",
    "def ensure_adduct_mapping(df_massspecgym, df_external=None):\n",
    "    \"Builds adduct_types and adduct_to_idx if not present, fills NaNs in adduct_idx.\"\n",
    "    global adduct_types, adduct_to_idx\n",
    "    try:\n",
    "        if 'adduct_to_idx' in globals() and 'adduct_types' in globals():\n",
    "            return adduct_to_idx\n",
    "    except Exception:\n",
    "        pass\n",
    "    adduct_types = df_massspecgym['adduct'].dropna().unique().tolist()\n",
    "    adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "    df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx).fillna(0).astype(int)\n",
    "    if df_external is not None:\n",
    "        df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx).fillna(0).astype(int)\n",
    "    return adduct_to_idx\n",
    "\n",
    "# attempt to ensure mapping if dataframes exist in notebook globals\n",
    "try:\n",
    "    if 'df_massspecgym' in globals():\n",
    "        ensure_adduct_mapping(df_massspecgym, df_external if 'df_external' in globals() else None)\n",
    "except Exception as e:\n",
    "    logging.debug(f'ensure_adduct_mapping failed: {e}')\n",
    "\n",
    "# 3) Enhanced beam search variant (keeps original beam_search intact)\n",
    "def beam_search_enhanced(model, spectrum, graph_data, ion_mode_idx, precursor_idx, adduct_idx, true_smiles=None, beam_width=8, max_len=150, nucleus_p=0.9, device='cpu'):\n",
    "    \"Beam-search that applies SMILES/SELFIES syntax checks, stereochemistry boosting, and plausibility filters.\"\n",
    "    model.eval()\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles) if true_smiles else None\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device) if isinstance(spectrum, torch.Tensor) else torch.tensor(spectrum, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        try:\n",
    "            graph_batch = Batch.from_data_list([graph_data]).to(device)\n",
    "        except Exception:\n",
    "            graph_batch = graph_data if hasattr(graph_data, 'batch') else graph_data\n",
    "        ion_mode_idx = torch.tensor([int(ion_mode_idx)], dtype=torch.long).to(device)\n",
    "        precursor_idx = torch.tensor([int(precursor_idx)], dtype=torch.long).to(device)\n",
    "        adduct_idx = torch.tensor([int(adduct_idx)], dtype=torch.long).to(device)\n",
    "        sequences = [([token_to_idx.get(config.SOS_TOKEN, token_to_idx.get(PAD_TOKEN, 0))], 0.0)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == token_to_idx.get(config.EOS_TOKEN, token_to_idx.get(EOS_TOKEN, 0)):\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                tgt_input = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "                # generate mask if model provides helper, otherwise create causal mask\n",
    "                tgt_mask = None\n",
    "                try:\n",
    "                    outputs = model.decoder(tgt_input, model.combine_layer(torch.cat([model.transformer_encoder(spectrum), model.gnn_encoder(graph_batch)], dim=-1)).unsqueeze(1), None)\n",
    "                    logits = outputs[0, -1]\n",
    "                except Exception:\n",
    "                    # Fallback to calling model forward if decoder signature differs\n",
    "                    outputs = model(spectrum, graph_batch, tgt_input)\n",
    "                    logits = outputs[0][0, -1] if isinstance(outputs, tuple) else outputs[0, -1]\n",
    "                log_probs = F.log_softmax(logits, dim=-1).cpu().numpy()\n",
    "                # boost stereochemistry tokens if present\n",
    "                for tok in ['@','/','\\\\']:\n",
    "                    if tok in token_to_idx:\n",
    "                        log_probs[token_to_idx[tok]] += 0.25\n",
    "                topk = np.argsort(log_probs)[-min(len(log_probs), beam_width*4):][::-1]\n",
    "                for tok in topk[:beam_width]:\n",
    "                    new_seq = seq + [int(tok)]\n",
    "                    new_score = score + float(log_probs[tok])\n",
    "                    # quick syntax check on partial reconstruction\n",
    "                    partial = ''.join([idx_to_token.get(i, '') for i in new_seq[1:]])\n",
    "                    if not is_valid_smiles_syntax(partial):\n",
    "                        # allow but penalize\n",
    "                        new_score -= 1.0\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if all(seq[-1] == token_to_idx.get(config.EOS_TOKEN, token_to_idx.get(EOS_TOKEN, 0)) for seq, _ in sequences):\n",
    "                break\n",
    "\n",
    "        results = []\n",
    "        for seq, score in sequences:\n",
    "            toks = [idx_to_token.get(i, '') for i in seq[1:]]\n",
    "            cand_str = ''.join([t for t in toks if t not in {config.PAD_TOKEN, config.SOS_TOKEN, config.EOS_TOKEN}])\n",
    "            # attempt to decode either as SELFIES or SMILES depending on available decoders\n",
    "            smiles = None\n",
    "            try:\n",
    "                # prefer SELFIES decode if token set is SELFIES-like\n",
    "                if 'sf' in globals():\n",
    "                    s = ''.join(toks)\n",
    "                    smiles = sf.decoder(s) if s else ''\n",
    "            except Exception:\n",
    "                smiles = None\n",
    "            if not smiles:\n",
    "                smiles = cand_str\n",
    "            if smiles and is_plausible_molecule(smiles, true_mol):\n",
    "                results.append((smiles, math.exp(score / max(1, len(seq)))))\n",
    "        return results if results else [(, 0.0)]\n",
    "\n",
    "# Small runtime check to show new helpers are present\n",
    "print('Merged helpers: is_valid_smiles_syntax, is_plausible_molecule, ensure_adduct_mapping, beam_search_enhanced available')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
